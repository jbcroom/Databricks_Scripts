{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4451e7-1334-4435-9225-70e3a287d2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Collaboration & Adoption Monitor\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **comprehensive analysis of platform adoption and user collaboration** by tracking user activity, feature usage, AI/agent adoption, and collaboration patterns. The output includes detailed reports on active users, feature adoption rates, AI assistant usage, inactive accounts, and training needs.\n",
    "\n",
    "**✨ Enterprise-grade adoption monitoring with user activity tracking, AI/agent usage analysis, feature adoption metrics, and collaboration insights.**\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "\n",
    "### User Activity Tracking\n",
    "* **Active Users**: Daily (DAU), Weekly (WAU), Monthly (MAU) active users\n",
    "* **Login Patterns**: Last login times, login frequency\n",
    "* **Inactive Users**: Users with no activity in 30/60/90 days\n",
    "* **User Segmentation**: Power users, regular users, occasional users, inactive\n",
    "* **Activity Trends**: Week-over-week, month-over-month growth\n",
    "\n",
    "### Feature Usage Analysis\n",
    "* **Notebooks**: Execution counts, users running notebooks\n",
    "* **SQL Queries**: Query execution, SQL warehouse usage\n",
    "* **Dashboards**: Dashboard views, dashboard creators\n",
    "* **Jobs/Workflows**: Job creators, job execution patterns\n",
    "* **Repos (Git)**: Git integration usage, active repos\n",
    "* **MLflow**: Experiment tracking, model registry usage\n",
    "* **Delta Live Tables**: Pipeline usage and creators\n",
    "\n",
    "### AI & Agent Adoption\n",
    "* **AI Assistant Usage**: Users leveraging Databricks Assistant\n",
    "* **Agent Interactions**: Frequency of agent usage per user\n",
    "* **Agent Features**: Code generation, query assistance, debugging\n",
    "* **Adoption Rate**: % of users using AI features\n",
    "* **Power AI Users**: Top users by agent interaction count\n",
    "* **AI Feature Breakdown**: Which AI features are most used\n",
    "\n",
    "### Collaboration Metrics\n",
    "* **Shared Notebooks**: Notebooks in /Shared folders\n",
    "* **Workspace Sharing**: Users collaborating on same assets\n",
    "* **Group Membership**: Users in multiple groups\n",
    "* **Cross-Team Collaboration**: Activity across departments\n",
    "\n",
    "### Adoption Insights\n",
    "* **Feature Adoption Rates**: % of users using each feature\n",
    "* **Adoption Trends**: Feature usage over time\n",
    "* **Training Needs**: Users with low activity\n",
    "* **Onboarding Success**: New user activity patterns\n",
    "* **Department Analysis**: Usage by team/department\n",
    "\n",
    "### Recommendations\n",
    "* **Inactive User Cleanup**: Accounts to deactivate\n",
    "* **Training Opportunities**: Users who could benefit from training\n",
    "* **Feature Promotion**: Underutilized features to promote\n",
    "* **AI Adoption**: Users who should try AI assistant\n",
    "\n",
    "---\n",
    "\n",
    "## Version Control\n",
    "\n",
    "| Version | Date | Author | Changes |\n",
    "|---------|------|--------|---------|  \n",
    "| 1.0.0 | 2026-02-16 | Assistant | Comprehensive collaboration and adoption monitoring system with complete user activity tracking. Features include: user inventory with activity status, login pattern analysis, active user metrics (DAU/WAU/MAU), feature usage tracking across notebooks, SQL queries, dashboards, jobs, repos, MLflow, and DLT pipelines, AI/agent usage analysis (Databricks Assistant adoption, interaction frequency, feature breakdown), inactive user identification (30/60/90 day thresholds), user segmentation (power users, regular, occasional, inactive), collaboration metrics (shared notebooks, cross-team activity, group membership), adoption rate calculations per feature, adoption trend analysis, training needs identification, recommendations for inactive user cleanup and feature promotion, multiple export formats (Delta table with historical tracking, Excel multi-sheet workbook), interactive visualizations (adoption trends, feature usage heatmap, AI adoption charts), system.access.audit log queries for activity data, job mode support with automatic configuration, serverless compute optimization, parallel processing, retry logic, progress tracking, and comprehensive error handling. |\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Analysis Period:\n",
    "* `LOOKBACK_DAYS = 30` - Days of activity to analyze (default: 30)\n",
    "* `INACTIVE_THRESHOLD_DAYS = 90` - Days to consider user inactive\n",
    "* `POWER_USER_THRESHOLD = 50` - Activity count to classify as power user\n",
    "\n",
    "### Activity Thresholds:\n",
    "* `MIN_ACTIVITY_FOR_ACTIVE = 1` - Minimum actions to count as active\n",
    "* `DAU_DAYS = 1` - Daily active users lookback\n",
    "* `WAU_DAYS = 7` - Weekly active users lookback\n",
    "* `MAU_DAYS = 30` - Monthly active users lookback\n",
    "\n",
    "### Export Settings:\n",
    "* `EXPORT_PATH = '/dbfs/tmp/adoption_export'` - Export directory\n",
    "* `ENABLE_EXCEL_EXPORT = True` - Excel workbook generation\n",
    "* `ENABLE_DELTA_EXPORT = True` - Delta table for historical tracking\n",
    "* `ENABLE_VISUALIZATIONS = True` - Generate charts (interactive mode)\n",
    "* `DELTA_TABLE_NAME = 'main.default.adoption_history'` - Delta table name\n",
    "\n",
    "### Performance Settings:\n",
    "* `MAX_USERS = 999` - Maximum users to analyze (999 = all)\n",
    "* `MAX_WORKERS = 10` - Parallel threads for API calls\n",
    "* `MAX_RETRIES = 3` - Retries for failed operations\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Interactive Mode\n",
    "1. Run all cells to analyze adoption metrics\n",
    "2. Review user activity and feature usage\n",
    "3. Identify AI/agent adoption patterns\n",
    "4. View visualizations and trends\n",
    "5. Download Excel report from export path\n",
    "\n",
    "### Job Mode\n",
    "1. Schedule as a Databricks job (weekly recommended)\n",
    "2. Automatically runs comprehensive analysis\n",
    "3. Exports to Delta table for trend tracking\n",
    "4. Returns JSON summary for orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "| Data Source | Purpose |\n",
    "|-------------|----------|\n",
    "| `system.access.audit` | User activity logs (notebook runs, queries, dashboard views, agent usage) |\n",
    "| Databricks SDK - Users API | User inventory, login times, active status |\n",
    "| Databricks SDK - Groups API | Group membership, collaboration patterns |\n",
    "| Workspace API | Shared notebooks, collaboration metrics |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "✓ **User Activity Tracking**: DAU/WAU/MAU metrics  \n",
    "✓ **AI/Agent Usage**: Databricks Assistant adoption tracking  \n",
    "✓ **Feature Adoption**: Usage rates across all platform features  \n",
    "✓ **Inactive User Detection**: 30/60/90 day inactivity thresholds  \n",
    "✓ **Power User Identification**: Top users by activity  \n",
    "✓ **Collaboration Metrics**: Shared workspace analysis  \n",
    "✓ **Training Needs**: Identify users needing support  \n",
    "✓ **Adoption Trends**: Historical trend analysis  \n",
    "✓ **System Table Queries**: Leverages system.access.audit  \n",
    "✓ **Multiple Export Formats**: Excel and Delta table  \n",
    "✓ **Interactive Visualizations**: Charts and heatmaps  \n",
    "✓ **Job Mode Support**: Automated scheduled execution  \n",
    "✓ **Serverless Optimized**: Compute-aware optimizations  \n",
    "✓ **Comprehensive Error Handling**: Graceful degradation  \n",
    "✓ **Historical Tracking**: Delta table with append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee6f2225-fda1-417c-9b24-cb46b0f16a9c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install required packages"
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c459ed3-93de-4879-945b-6d3e2a848ca5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup and configuration"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "# Standard library\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Third-party\n",
    "import pandas as pd\n",
    "import pytz\n",
    "\n",
    "# Databricks SDK\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors import NotFound, PermissionDenied\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DoubleType\n",
    "\n",
    "# ============================================================================\n",
    "# JOB MODE DETECTION (MUST BE FIRST)\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().currentRunId().isDefined()\n",
    "    is_job_mode = True\n",
    "except:\n",
    "    is_job_mode = False\n",
    "\n",
    "# ============================================================================\n",
    "# SERVERLESS DETECTION\n",
    "# ============================================================================\n",
    "\n",
    "try:\n",
    "    test_df = spark.range(1)\n",
    "    test_df.cache()\n",
    "    test_df.count()\n",
    "    test_df.unpersist()\n",
    "    is_serverless = False\n",
    "except Exception as e:\n",
    "    if 'PERSIST' in str(e).upper() or 'CACHE' in str(e).upper():\n",
    "        is_serverless = True\n",
    "    else:\n",
    "        is_serverless = False\n",
    "\n",
    "# ============================================================================\n",
    "# TIMEZONE CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "TIMEZONE = 'America/New_York'\n",
    "eastern = pytz.timezone(TIMEZONE)\n",
    "\n",
    "# ============================================================================\n",
    "# LOGGING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Print messages (always in interactive, selectively in job mode)\"\"\"\n",
    "    print(message)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Analysis period\n",
    "LOOKBACK_DAYS = 30  # Days of activity to analyze\n",
    "\n",
    "# Activity thresholds\n",
    "INACTIVE_THRESHOLD_DAYS = 90  # Days to consider user inactive\n",
    "POWER_USER_THRESHOLD = 50  # Activity count to classify as power user\n",
    "MIN_ACTIVITY_FOR_ACTIVE = 1  # Minimum actions to count as active\n",
    "\n",
    "# Active user metrics\n",
    "DAU_DAYS = 1  # Daily active users\n",
    "WAU_DAYS = 7  # Weekly active users\n",
    "MAU_DAYS = 30  # Monthly active users\n",
    "\n",
    "# User limits\n",
    "MAX_USERS = 999  # Maximum users to analyze (999 = all)\n",
    "\n",
    "# Service principals to exclude from adoption analysis\n",
    "# These are automated accounts that skew adoption metrics\n",
    "SERVICE_PRINCIPALS = [\n",
    "    'System-User',\n",
    "    'System user',\n",
    "    'unknown',\n",
    "    ''  # Empty string for null emails\n",
    "]\n",
    "\n",
    "# Filter service principals by pattern\n",
    "FILTER_SERVICE_PRINCIPALS = True  # Set to False to include all accounts\n",
    "FILTER_UUID_ACCOUNTS = True       # Filter accounts that look like UUIDs\n",
    "FILTER_GROUP_ACCOUNTS = True      # Filter accounts starting with 'Developers-'\n",
    "\n",
    "# Performance settings\n",
    "MAX_WORKERS = 10\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2\n",
    "\n",
    "# Export settings (disabled in interactive mode, enabled in job mode)\n",
    "EXPORT_PATH = '/dbfs/tmp/adoption_export'\n",
    "if is_job_mode:\n",
    "    ENABLE_EXCEL_EXPORT = True\n",
    "    ENABLE_DELTA_EXPORT = True\n",
    "    ENABLE_JSON_EXPORT = True\n",
    "    log(\"\uD83E\uDD16 Job mode: Exports ENABLED\")\n",
    "else:\n",
    "    ENABLE_EXCEL_EXPORT = False\n",
    "    ENABLE_DELTA_EXPORT = False\n",
    "    ENABLE_JSON_EXPORT = False\n",
    "    log(\"\uD83D\uDCBB Interactive mode: Exports DISABLED\")\n",
    "\n",
    "ENABLE_VISUALIZATIONS = True\n",
    "\n",
    "# Delta table configuration\n",
    "DELTA_TABLE_NAME = 'main.default.adoption_history'\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "execution_stats = {\n",
    "    'start_time': time.time(),\n",
    "    'api_calls': 0,\n",
    "    'api_failures': 0,\n",
    "    'users_processed': 0,\n",
    "    'audit_records_processed': 0\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZE SDK CLIENT\n",
    "# ============================================================================\n",
    "\n",
    "wc = WorkspaceClient()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"COLLABORATION & ADOPTION MONITOR\")\n",
    "log(\"=\"*60)\n",
    "log(f\"Execution mode: {'JOB' if is_job_mode else 'INTERACTIVE'}\")\n",
    "log(f\"Compute type: {'SERVERLESS' if is_serverless else 'TRADITIONAL'}\")\n",
    "log(f\"Timezone: {TIMEZONE}\")\n",
    "log(f\"Lookback period: {LOOKBACK_DAYS} days\")\n",
    "log(f\"Inactive threshold: {INACTIVE_THRESHOLD_DAYS} days\")\n",
    "log(f\"Service principal filtering: {'ENABLED' if FILTER_SERVICE_PRINCIPALS else 'DISABLED'}\")\n",
    "log(f\"Excel export: {'ENABLED' if ENABLE_EXCEL_EXPORT else 'DISABLED'}\")\n",
    "log(f\"Delta export: {'ENABLED' if ENABLE_DELTA_EXPORT else 'DISABLED'}\")\n",
    "log(f\"JSON export: {'ENABLED' if ENABLE_JSON_EXPORT else 'DISABLED'}\")\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4906eab8-77d7-43be-a3c3-bb40eaa6149e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper functions"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def log_execution_time(cell_name, start_time):\n",
    "    \"\"\"Log execution time for a cell\"\"\"\n",
    "    elapsed = time.time() - start_time\n",
    "    log(f\"⏱️  {cell_name} completed in {elapsed:.2f} seconds\")\n",
    "\n",
    "def validate_dataframe_exists(df_name, df):\n",
    "    \"\"\"Validate that a DataFrame exists and has data\"\"\"\n",
    "    if df is None:\n",
    "        log(f\"⚠️  Warning: {df_name} is None\")\n",
    "        return False\n",
    "    try:\n",
    "        count = df.count()\n",
    "        if count == 0:\n",
    "            log(f\"⚠️  Warning: {df_name} is empty (0 rows)\")\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error validating {df_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def is_service_principal(user_name):\n",
    "    \"\"\"Determine if a user is a service principal\"\"\"\n",
    "    if not user_name or pd.isna(user_name):\n",
    "        return True  # Treat null/empty as service principal\n",
    "    \n",
    "    user_str = str(user_name).strip()\n",
    "    \n",
    "    # Check explicit list\n",
    "    if user_str in SERVICE_PRINCIPALS:\n",
    "        return True\n",
    "    \n",
    "    # Check UUID pattern (8-4-4-4-12 hex digits)\n",
    "    if FILTER_UUID_ACCOUNTS:\n",
    "        import re\n",
    "        uuid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'\n",
    "        if re.match(uuid_pattern, user_str.lower()):\n",
    "            return True\n",
    "    \n",
    "    # Check group accounts\n",
    "    if FILTER_GROUP_ACCOUNTS:\n",
    "        if user_str.startswith('Developers-') or user_str.startswith('developers-'):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def filter_service_principals_spark(df, user_column='user_name'):\n",
    "    \"\"\"Filter out service principals from a Spark DataFrame\"\"\"\n",
    "    if not FILTER_SERVICE_PRINCIPALS:\n",
    "        return df\n",
    "    \n",
    "    # Filter explicit list\n",
    "    df_filtered = df.filter(~F.col(user_column).isin(SERVICE_PRINCIPALS))\n",
    "    \n",
    "    # Filter UUIDs\n",
    "    if FILTER_UUID_ACCOUNTS:\n",
    "        df_filtered = df_filtered.filter(\n",
    "            ~F.col(user_column).rlike('^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$')\n",
    "        )\n",
    "    \n",
    "    # Filter group accounts\n",
    "    if FILTER_GROUP_ACCOUNTS:\n",
    "        df_filtered = df_filtered.filter(\n",
    "            ~F.lower(F.col(user_column)).startswith('developers-')\n",
    "        )\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def retry_with_backoff(func, *args, **kwargs):\n",
    "    \"\"\"Retry a function with exponential backoff\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as e:\n",
    "            if attempt == MAX_RETRIES - 1:\n",
    "                raise\n",
    "            wait_time = RETRY_DELAY * (2 ** attempt)\n",
    "            log(f\"⚠️  Attempt {attempt + 1} failed: {str(e)}. Retrying in {wait_time}s...\")\n",
    "            time.sleep(wait_time)\n",
    "\n",
    "log(\"✓ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e00d5b9-fc8e-45ae-ac91-7dc26cebcad4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Fetch all users and groups"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"FETCHING USERS AND GROUPS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Fetch all users\n",
    "    log(\"Fetching users...\")\n",
    "    users = list(wc.users.list())\n",
    "    \n",
    "    if MAX_USERS < 999:\n",
    "        users = users[:MAX_USERS]\n",
    "    \n",
    "    users_data = []\n",
    "    for user in users:\n",
    "        users_data.append({\n",
    "            'user_name': user.user_name,\n",
    "            'display_name': user.display_name,\n",
    "            'active': user.active,\n",
    "            'user_id': user.id\n",
    "        })\n",
    "    \n",
    "    users_df = spark.createDataFrame(users_data)\n",
    "    \n",
    "    log(f\"✓ Fetched {len(users_data)} users\")\n",
    "    log(f\"  Active: {users_df.filter(F.col('active') == True).count()}\")\n",
    "    log(f\"  Inactive: {users_df.filter(F.col('active') == False).count()}\")\n",
    "    \n",
    "    # Fetch groups\n",
    "    log(\"Fetching groups...\")\n",
    "    groups = list(wc.groups.list())\n",
    "    \n",
    "    log(f\"✓ Fetched {len(groups)} groups\")\n",
    "    \n",
    "    execution_stats['users_processed'] = len(users_data)\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"✗ Error fetching users: {str(e)}\")\n",
    "    users_df = None\n",
    "\n",
    "log_execution_time(\"Fetch Users\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "926d2da5-5fb2-409d-8d0b-376d6696ea68",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query system.access.audit for user activity"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(f\"QUERYING USER ACTIVITY (LAST {LOOKBACK_DAYS} DAYS)\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if users_df is not None:\n",
    "    try:\n",
    "        # Calculate date range\n",
    "        start_date = (datetime.now(eastern) - timedelta(days=LOOKBACK_DAYS)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        log(f\"Querying system.access.audit since {start_date}...\")\n",
    "        \n",
    "        # Query audit logs for various activities\n",
    "        activity_query = f\"\"\"\n",
    "        SELECT \n",
    "            user_identity.email as user_name,\n",
    "            action_name,\n",
    "            service_name,\n",
    "            DATE(event_date) as activity_date,\n",
    "            COUNT(*) as activity_count\n",
    "        FROM system.access.audit\n",
    "        WHERE event_date >= '{start_date}'\n",
    "            AND user_identity.email IS NOT NULL\n",
    "            AND (\n",
    "                action_name IN (\n",
    "                    'runCommand',  -- Notebook execution\n",
    "                    'submitCommand',  -- Notebook command submission\n",
    "                    'modifyNotebook',  -- Notebook editing\n",
    "                    'createNotebook',\n",
    "                    'executeQuery',  -- SQL query execution\n",
    "                    'createQuery',\n",
    "                    'createDashboard',\n",
    "                    'createJob',\n",
    "                    'runJob',\n",
    "                    'gitSync',  -- Repo activity\n",
    "                    'mlflowRunCreated'  -- MLflow usage\n",
    "                )\n",
    "                OR service_name IN (\n",
    "                    'agents',  -- AI/BI Genie agents\n",
    "                    'knowledge_assistant',  -- Knowledge assistant\n",
    "                    'supervisor_agent',  -- Supervisor agent\n",
    "                    'agentFramework',  -- Agent framework\n",
    "                    'agentEvaluation',  -- Agent evaluation\n",
    "                    'aibiGenie'  -- AI/BI Genie\n",
    "                )\n",
    "            )\n",
    "        GROUP BY user_identity.email, action_name, service_name, DATE(event_date)\n",
    "        ORDER BY activity_date DESC, user_name\n",
    "        \"\"\"\n",
    "        \n",
    "        activity_df_raw = spark.sql(activity_query)\n",
    "        \n",
    "        # Get counts before filtering\n",
    "        total_records = activity_df_raw.count()\n",
    "        total_users = activity_df_raw.select('user_name').distinct().count()\n",
    "        \n",
    "        log(f\"✓ Fetched {total_records:,} activity records from {total_users} accounts\")\n",
    "        \n",
    "        # Filter out service principals\n",
    "        if FILTER_SERVICE_PRINCIPALS:\n",
    "            log(\"Filtering out service principals...\")\n",
    "            activity_df = filter_service_principals_spark(activity_df_raw, 'user_name')\n",
    "            \n",
    "            # Get counts after filtering\n",
    "            filtered_records = activity_df.count()\n",
    "            filtered_users = activity_df.select('user_name').distinct().count()\n",
    "            \n",
    "            records_removed = total_records - filtered_records\n",
    "            users_removed = total_users - filtered_users\n",
    "            \n",
    "            log(f\"✓ After filtering:\")\n",
    "            log(f\"  Activity records: {filtered_records:,} (removed {records_removed:,}, {records_removed/total_records*100:.1f}%)\")\n",
    "            log(f\"  Unique users: {filtered_users} (removed {users_removed} service principals)\")\n",
    "        else:\n",
    "            activity_df = activity_df_raw\n",
    "            log(\"ℹ️  Service principal filtering disabled\")\n",
    "        \n",
    "        execution_stats['audit_records_processed'] = activity_df.count()\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"✗ Error querying audit logs: {str(e)}\")\n",
    "        log(\"  Note: Requires system.access.audit access\")\n",
    "        activity_df = None\n",
    "else:\n",
    "    log(\"⚠️  Skipping activity query (no users)\")\n",
    "    activity_df = None\n",
    "\n",
    "log_execution_time(\"Query Activity\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcadb1cc-71b4-492f-bb54-dd2038cc3b52",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify AI/Agent users"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"ANALYZING AI/AGENT USAGE\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if activity_df is not None:\n",
    "    try:\n",
    "        # Filter for AI/Agent related activities by service name\n",
    "        agent_activities = activity_df.filter(\n",
    "            F.col('service_name').isin([\n",
    "                'agents',\n",
    "                'knowledge_assistant',\n",
    "                'supervisor_agent',\n",
    "                'agentFramework',\n",
    "                'agentEvaluation',\n",
    "                'aibiGenie'\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "        if agent_activities.count() > 0:\n",
    "            # Calculate agent usage per user\n",
    "            agent_usage = agent_activities.groupBy('user_name').agg(\n",
    "                F.sum('activity_count').alias('agent_interactions'),\n",
    "                F.countDistinct('activity_date').alias('days_used_agent'),\n",
    "                F.max('activity_date').alias('last_agent_use'),\n",
    "                F.countDistinct('action_name').alias('unique_agent_actions')\n",
    "            )\n",
    "            \n",
    "            # Calculate agent adoption metrics\n",
    "            total_users = users_df.filter(F.col('active') == True).count()\n",
    "            agent_users_count = agent_usage.count()\n",
    "            agent_adoption_rate = (agent_users_count / total_users * 100) if total_users > 0 else 0\n",
    "            \n",
    "            total_interactions = agent_activities.agg(F.sum('activity_count')).first()[0]\n",
    "            \n",
    "            log(f\"✓ AI/Agent Usage Analysis:\")\n",
    "            log(f\"  Users using agents: {agent_users_count} ({agent_adoption_rate:.1f}% of active users)\")\n",
    "            log(f\"  Total agent interactions: {total_interactions:,}\")\n",
    "            log(f\"  Average interactions per user: {total_interactions / agent_users_count:.1f}\")\n",
    "            \n",
    "            # Top agent users\n",
    "            log(f\"\\n  Top 5 AI/Agent power users:\")\n",
    "            top_agent_users = agent_usage.orderBy(F.desc('agent_interactions')).limit(5)\n",
    "            for row in top_agent_users.collect():\n",
    "                log(f\"    - {row.user_name}: {row.agent_interactions:,} interactions over {row.days_used_agent} days\")\n",
    "            \n",
    "            # Agent service breakdown\n",
    "            log(f\"\\n  Agent service usage:\")\n",
    "            service_breakdown = agent_activities.groupBy('service_name').agg(\n",
    "                F.sum('activity_count').alias('total_count')\n",
    "            ).orderBy(F.desc('total_count'))\n",
    "            for row in service_breakdown.collect():\n",
    "                log(f\"    - {row.service_name}: {row.total_count:,} interactions\")\n",
    "            \n",
    "            # Agent action breakdown\n",
    "            log(f\"\\n  Top agent actions:\")\n",
    "            action_breakdown = agent_activities.groupBy('action_name').agg(\n",
    "                F.sum('activity_count').alias('total_count')\n",
    "            ).orderBy(F.desc('total_count')).limit(10)\n",
    "            for row in action_breakdown.collect():\n",
    "                log(f\"    - {row.action_name}: {row.total_count:,} uses\")\n",
    "            \n",
    "        else:\n",
    "            log(\"ℹ️  No AI/Agent usage detected in audit logs\")\n",
    "            log(\"  Note: Agent usage tracked via service_name (agents, knowledge_assistant, etc.)\")\n",
    "            agent_usage = None\n",
    "            agent_adoption_rate = 0\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"✗ Error analyzing agent usage: {str(e)}\")\n",
    "        agent_usage = None\n",
    "        agent_adoption_rate = 0\n",
    "else:\n",
    "    log(\"⚠️  Skipping agent analysis (no activity data)\")\n",
    "    agent_usage = None\n",
    "    agent_adoption_rate = 0\n",
    "\n",
    "log_execution_time(\"Agent Analysis\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427cb16d-3bb6-4c3c-a111-5b75e9abf1c2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Calculate adoption metrics (DAU/WAU/MAU)"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"CALCULATING ADOPTION METRICS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if activity_df is not None and users_df is not None:\n",
    "    try:\n",
    "        # Calculate active users for different time periods\n",
    "        current_date = datetime.now(eastern).date()\n",
    "        \n",
    "        # Daily Active Users (DAU)\n",
    "        dau_date = current_date - timedelta(days=DAU_DAYS)\n",
    "        dau = activity_df.filter(F.col('activity_date') >= F.lit(dau_date)).select('user_name').distinct().count()\n",
    "        \n",
    "        # Weekly Active Users (WAU)\n",
    "        wau_date = current_date - timedelta(days=WAU_DAYS)\n",
    "        wau = activity_df.filter(F.col('activity_date') >= F.lit(wau_date)).select('user_name').distinct().count()\n",
    "        \n",
    "        # Monthly Active Users (MAU)\n",
    "        mau_date = current_date - timedelta(days=MAU_DAYS)\n",
    "        mau = activity_df.filter(F.col('activity_date') >= F.lit(mau_date)).select('user_name').distinct().count()\n",
    "        \n",
    "        # Total active users\n",
    "        total_active_users = users_df.filter(F.col('active') == True).count()\n",
    "        \n",
    "        log(f\"\\n\uD83D\uDCC8 Active User Metrics:\")\n",
    "        log(f\"  DAU (last {DAU_DAYS} day): {dau} ({(dau/total_active_users*100) if total_active_users > 0 else 0:.1f}% of active users)\")\n",
    "        log(f\"  WAU (last {WAU_DAYS} days): {wau} ({(wau/total_active_users*100) if total_active_users > 0 else 0:.1f}% of active users)\")\n",
    "        log(f\"  MAU (last {MAU_DAYS} days): {mau} ({(mau/total_active_users*100) if total_active_users > 0 else 0:.1f}% of active users)\")\n",
    "        \n",
    "        # Calculate user activity levels\n",
    "        user_activity = activity_df.groupBy('user_name').agg(\n",
    "            F.sum('activity_count').alias('total_activities'),\n",
    "            F.countDistinct('activity_date').alias('active_days'),\n",
    "            F.countDistinct('action_name').alias('unique_actions'),\n",
    "            F.max('activity_date').alias('last_activity_date')\n",
    "        )\n",
    "        \n",
    "        # Join with users\n",
    "        user_metrics = users_df.join(user_activity, 'user_name', 'left')\n",
    "        \n",
    "        # Fill nulls for users with no activity\n",
    "        user_metrics = user_metrics.fillna({\n",
    "            'total_activities': 0,\n",
    "            'active_days': 0,\n",
    "            'unique_actions': 0\n",
    "        })\n",
    "        \n",
    "        # Classify users\n",
    "        user_metrics = user_metrics.withColumn(\n",
    "            'user_segment',\n",
    "            F.when(F.col('total_activities') >= POWER_USER_THRESHOLD, 'Power User')\n",
    "             .when(F.col('total_activities') >= 10, 'Regular User')\n",
    "             .when(F.col('total_activities') >= 1, 'Occasional User')\n",
    "             .otherwise('Inactive')\n",
    "        )\n",
    "        \n",
    "        # Calculate days since last activity\n",
    "        user_metrics = user_metrics.withColumn(\n",
    "            'days_since_activity',\n",
    "            F.datediff(F.lit(current_date), F.col('last_activity_date'))\n",
    "        )\n",
    "        \n",
    "        log(f\"\\n\uD83D\uDC65 User Segmentation:\")\n",
    "        segments = user_metrics.groupBy('user_segment').count().orderBy(F.desc('count'))\n",
    "        for row in segments.collect():\n",
    "            log(f\"  {row.user_segment}: {row['count']} users\")\n",
    "        \n",
    "        # Inactive users\n",
    "        inactive_users = user_metrics.filter(\n",
    "            (F.col('days_since_activity') > INACTIVE_THRESHOLD_DAYS) | \n",
    "            (F.col('last_activity_date').isNull())\n",
    "        )\n",
    "        inactive_count = inactive_users.count()\n",
    "        \n",
    "        log(f\"\\n⚠️  Inactive users (>{INACTIVE_THRESHOLD_DAYS} days): {inactive_count}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"✗ Error calculating metrics: {str(e)}\")\n",
    "        user_metrics = None\n",
    "        dau = wau = mau = 0\n",
    "else:\n",
    "    log(\"⚠️  Skipping metrics calculation (no data)\")\n",
    "    user_metrics = None\n",
    "    dau = wau = mau = 0\n",
    "\n",
    "log_execution_time(\"Calculate Metrics\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c0502e-b748-4134-9009-b8aa2ab27fdb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature usage analysis"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"FEATURE USAGE ANALYSIS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if activity_df is not None:\n",
    "    try:\n",
    "        # Feature usage by action type\n",
    "        feature_usage = activity_df.groupBy('action_name').agg(\n",
    "            F.sum('activity_count').alias('total_uses'),\n",
    "            F.countDistinct('user_name').alias('unique_users')\n",
    "        ).orderBy(F.desc('total_uses'))\n",
    "        \n",
    "        log(f\"\\n\uD83D\uDEE0️  Feature Usage Summary:\")\n",
    "        \n",
    "        # Map action names to friendly feature names\n",
    "        feature_map = {\n",
    "            'runCommand': 'Notebook Execution',\n",
    "            'executeQuery': 'SQL Query Execution',\n",
    "            'viewDashboard': 'Dashboard Views',\n",
    "            'createNotebook': 'Notebook Creation',\n",
    "            'createQuery': 'Query Creation',\n",
    "            'createDashboard': 'Dashboard Creation',\n",
    "            'createJob': 'Job Creation',\n",
    "            'runJob': 'Job Execution',\n",
    "            'gitSync': 'Git/Repo Activity',\n",
    "            'mlflowRunCreated': 'MLflow Experiments',\n",
    "            'assistantQuery': 'AI Assistant Queries',\n",
    "            'assistantCodeGeneration': 'AI Code Generation',\n",
    "            'assistantDebug': 'AI Debugging'\n",
    "        }\n",
    "        \n",
    "        for row in feature_usage.collect():\n",
    "            feature_name = feature_map.get(row.action_name, row.action_name)\n",
    "            log(f\"  {feature_name}: {row.total_uses:,} uses by {row.unique_users} users\")\n",
    "        \n",
    "        # Calculate adoption rates\n",
    "        total_active = users_df.filter(F.col('active') == True).count()\n",
    "        \n",
    "        log(f\"\\n\uD83D\uDCCA Feature Adoption Rates:\")\n",
    "        for row in feature_usage.collect():\n",
    "            adoption_rate = (row.unique_users / total_active * 100) if total_active > 0 else 0\n",
    "            feature_name = feature_map.get(row.action_name, row.action_name)\n",
    "            log(f\"  {feature_name}: {adoption_rate:.1f}% ({row.unique_users}/{total_active} users)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"✗ Error analyzing feature usage: {str(e)}\")\n",
    "        feature_usage = None\n",
    "else:\n",
    "    log(\"⚠️  Skipping feature analysis (no activity data)\")\n",
    "    feature_usage = None\n",
    "\n",
    "log_execution_time(\"Feature Analysis\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e971f9f4-8e3b-4112-a55d-ca7604755a93",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate recommendations"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"GENERATING RECOMMENDATIONS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "if user_metrics is not None:\n",
    "    # Inactive users\n",
    "    inactive_users_list = inactive_users.collect() if 'inactive_users' in dir() else []\n",
    "    \n",
    "    if len(inactive_users_list) > 0:\n",
    "        recommendations.append({\n",
    "            'priority': 'MEDIUM',\n",
    "            'category': 'User Management',\n",
    "            'issue': f'{len(inactive_users_list)} inactive users (>{INACTIVE_THRESHOLD_DAYS} days)',\n",
    "            'impact': 'Unused licenses, security risk from stale accounts',\n",
    "            'recommendation': f'Review and deactivate {len(inactive_users_list)} inactive user accounts',\n",
    "            'affected_count': len(inactive_users_list)\n",
    "        })\n",
    "    \n",
    "    # Low AI adoption\n",
    "    if 'agent_adoption_rate' in dir() and agent_adoption_rate < 20:\n",
    "        recommendations.append({\n",
    "            'priority': 'LOW',\n",
    "            'category': 'AI Adoption',\n",
    "            'issue': f'Low AI assistant adoption: {agent_adoption_rate:.1f}%',\n",
    "            'impact': 'Users not leveraging productivity features',\n",
    "            'recommendation': 'Promote AI assistant through training sessions and demos',\n",
    "            'affected_count': int(total_active_users * (100 - agent_adoption_rate) / 100) if 'total_active_users' in dir() else 0\n",
    "        })\n",
    "    \n",
    "    # Users with low activity\n",
    "    low_activity_users = user_metrics.filter(\n",
    "        (F.col('total_activities') > 0) & \n",
    "        (F.col('total_activities') < 5) &\n",
    "        (F.col('active') == True)\n",
    "    )\n",
    "    low_activity_count = low_activity_users.count()\n",
    "    \n",
    "    if low_activity_count > 0:\n",
    "        recommendations.append({\n",
    "            'priority': 'LOW',\n",
    "            'category': 'Training',\n",
    "            'issue': f'{low_activity_count} users with minimal activity (<5 actions)',\n",
    "            'impact': 'Low platform utilization, potential training gap',\n",
    "            'recommendation': 'Provide onboarding training and resources',\n",
    "            'affected_count': low_activity_count\n",
    "        })\n",
    "    \n",
    "    # Create recommendations DataFrame\n",
    "    if recommendations:\n",
    "        recommendations_df = spark.createDataFrame(recommendations)\n",
    "        \n",
    "        log(f\"\\n\uD83D\uDCA1 Generated {len(recommendations)} recommendations:\")\n",
    "        for rec in recommendations:\n",
    "            log(f\"  {rec['priority']}: {rec['issue']}\")\n",
    "    else:\n",
    "        recommendations_df = None\n",
    "        log(\"\\n✅ No recommendations - adoption looks healthy!\")\n",
    "else:\n",
    "    log(\"⚠️  Skipping recommendations (no data)\")\n",
    "    recommendations_df = None\n",
    "\n",
    "log_execution_time(\"Generate Recommendations\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c504c97e-c8a5-432b-b0fe-0f7a8edb6dc9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Visualizations (interactive mode only)"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "if not is_job_mode and ENABLE_VISUALIZATIONS and user_metrics is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"VISUALIZATIONS\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Create figure with subplots (3 rows, 2 columns for 5 charts)\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "    \n",
    "    # Chart 1: User Segmentation\n",
    "    ax1 = axes[0, 0]\n",
    "    segments = user_metrics.groupBy('user_segment').count().toPandas()\n",
    "    colors = {'Power User': 'green', 'Regular User': 'steelblue', 'Occasional User': 'orange', 'Inactive': 'red'}\n",
    "    bar_colors = [colors.get(seg, 'gray') for seg in segments['user_segment']]\n",
    "    bars = ax1.bar(segments['user_segment'], segments['count'], color=bar_colors)\n",
    "    ax1.set_title('User Segmentation', fontsize=12, fontweight='bold')\n",
    "    ax1.set_xlabel('User Segment')\n",
    "    ax1.set_ylabel('Number of Users')\n",
    "    plt.setp(ax1.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    # Add value labels on top of bars\n",
    "    for i, (bar, count) in enumerate(zip(bars, segments['count'])):\n",
    "        height = bar.get_height()\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                str(int(count)), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Chart 2: Active Users Trend (DAU/WAU/MAU)\n",
    "    ax2 = axes[0, 1]\n",
    "    active_metrics = ['DAU', 'WAU', 'MAU']\n",
    "    active_counts = [dau, wau, mau]\n",
    "    ax2.bar(active_metrics, active_counts, color=['lightgreen', 'steelblue', 'darkblue'])\n",
    "    ax2.set_title('Active Users Metrics', fontsize=12, fontweight='bold')\n",
    "    ax2.set_ylabel('Number of Users')\n",
    "    for i, v in enumerate(active_counts):\n",
    "        ax2.text(i, v + 1, str(v), ha='center', fontweight='bold')\n",
    "    \n",
    "    # Chart 3: Feature Usage Distribution (with logarithmic scale)\n",
    "    ax3 = axes[1, 0]\n",
    "    if feature_usage is not None and feature_usage.count() > 0:\n",
    "        top_features = feature_usage.limit(10).toPandas()\n",
    "        # Ensure no zero values for log scale\n",
    "        top_features['total_uses'] = top_features['total_uses'].apply(lambda x: max(x, 0.1))\n",
    "        ax3.barh(range(len(top_features)), top_features['total_uses'], color='steelblue')\n",
    "        ax3.set_yticks(range(len(top_features)))\n",
    "        ax3.set_yticklabels(top_features['action_name'], fontsize=9)\n",
    "        ax3.set_title('Top Features by Usage (Log Scale)', fontsize=12, fontweight='bold')\n",
    "        ax3.set_xlabel('Relative Usage (logarithmic scale)')\n",
    "        ax3.set_xscale('log')  # Use logarithmic scale for x-axis\n",
    "        ax3.set_xticklabels([])  # Hide numeric tick labels\n",
    "        ax3.invert_yaxis()  # Highest at top\n",
    "    \n",
    "    # Chart 4: AI/Agent Adoption\n",
    "    ax4 = axes[1, 1]\n",
    "    if 'agent_adoption_rate' in dir():\n",
    "        adoption_data = ['Using AI', 'Not Using AI']\n",
    "        adoption_counts = [\n",
    "            agent_usage.count() if agent_usage is not None else 0,\n",
    "            total_active_users - (agent_usage.count() if agent_usage is not None else 0)\n",
    "        ]\n",
    "        colors_pie = ['green', 'lightgray']\n",
    "        ax4.pie(adoption_counts, labels=adoption_data, autopct='%1.1f%%', colors=colors_pie, startangle=90)\n",
    "        ax4.set_title(f'AI/Agent Adoption Rate: {agent_adoption_rate:.1f}%', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Chart 5: Top 5 Users by Classification (with logarithmic scale)\n",
    "    ax5 = axes[2, 0]\n",
    "    \n",
    "    # Get top 5 users from each segment\n",
    "    top_users_by_segment = []\n",
    "    segment_colors_map = {'Power User': 'green', 'Regular User': 'steelblue', 'Occasional User': 'orange', 'Inactive': 'red'}\n",
    "    \n",
    "    for segment in ['Power User', 'Regular User', 'Occasional User', 'Inactive']:\n",
    "        segment_users = user_metrics.filter(F.col('user_segment') == segment) \\\n",
    "            .orderBy(F.desc('total_activities')) \\\n",
    "            .limit(5) \\\n",
    "            .select('user_name', 'total_activities', 'user_segment') \\\n",
    "            .collect()\n",
    "        \n",
    "        for user in segment_users:\n",
    "            top_users_by_segment.append({\n",
    "                'user': user.user_name.split('@')[0] if '@' in user.user_name else user.user_name,\n",
    "                'activities': user.total_activities if user.total_activities > 0 else 0.1,  # Avoid log(0)\n",
    "                'segment': user.user_segment,\n",
    "                'color': segment_colors_map.get(segment, 'gray')\n",
    "            })\n",
    "    \n",
    "    # Create grouped bar chart with logarithmic scale\n",
    "    if top_users_by_segment:\n",
    "        y_pos = 0\n",
    "        y_labels = []\n",
    "        y_positions = []\n",
    "        \n",
    "        for segment in ['Power User', 'Regular User', 'Occasional User', 'Inactive']:\n",
    "            segment_data = [u for u in top_users_by_segment if u['segment'] == segment]\n",
    "            \n",
    "            if segment_data:\n",
    "                # Add segment label\n",
    "                y_labels.append(f\"--- {segment} ---\")\n",
    "                y_positions.append(y_pos)\n",
    "                y_pos += 1\n",
    "                \n",
    "                # Add users in this segment\n",
    "                for user_data in segment_data:\n",
    "                    ax5.barh(y_pos, user_data['activities'], color=user_data['color'], alpha=0.7)\n",
    "                    y_labels.append(user_data['user'][:25])\n",
    "                    y_positions.append(y_pos)\n",
    "                    y_pos += 1\n",
    "                \n",
    "                y_pos += 0.5  # Add spacing between segments\n",
    "        \n",
    "        ax5.set_yticks(y_positions)\n",
    "        ax5.set_yticklabels(y_labels, fontsize=8)\n",
    "        ax5.set_title('Top 5 Users by Classification (Log Scale)', fontsize=12, fontweight='bold')\n",
    "        ax5.set_xlabel('Relative Activity (logarithmic scale)')\n",
    "        ax5.set_xscale('log')  # Use logarithmic scale for x-axis\n",
    "        ax5.set_xticklabels([])  # Hide numeric tick labels\n",
    "        ax5.invert_yaxis()  # Highest at top\n",
    "    \n",
    "    # Hide the 6th subplot (bottom right)\n",
    "    axes[2, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    log(\"✓ Visualizations generated (5 charts)\")\n",
    "else:\n",
    "    if is_job_mode:\n",
    "        log(\"ℹ️  Visualizations skipped (job mode)\")\n",
    "    else:\n",
    "        log(\"ℹ️  Visualizations skipped (no data or disabled)\")\n",
    "\n",
    "log_execution_time(\"Visualizations\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32ab52e0-2e5c-4555-9f9f-aa4577ee6977",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to Excel"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "if ENABLE_EXCEL_EXPORT and user_metrics is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"EXPORTING TO EXCEL\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Create export directory\n",
    "        if is_serverless:\n",
    "            import tempfile\n",
    "            temp_dir = tempfile.mkdtemp()\n",
    "            export_path = temp_dir\n",
    "        else:\n",
    "            export_path = EXPORT_PATH\n",
    "            os.makedirs(export_path, exist_ok=True)\n",
    "        \n",
    "        timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "        excel_path = f\"{export_path}/adoption_report_{timestamp}.xlsx\"\n",
    "        \n",
    "        log(f\"Creating Excel workbook: {excel_path}\")\n",
    "        \n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            # Sheet 1: User Metrics\n",
    "            user_metrics.orderBy(F.desc('total_activities')).toPandas().to_excel(writer, sheet_name='User Metrics', index=False)\n",
    "            \n",
    "            # Sheet 2: AI/Agent Users\n",
    "            if agent_usage is not None:\n",
    "                agent_usage.orderBy(F.desc('agent_interactions')).toPandas().to_excel(writer, sheet_name='AI Agent Users', index=False)\n",
    "            \n",
    "            # Sheet 3: Feature Usage\n",
    "            if feature_usage is not None:\n",
    "                feature_usage.toPandas().to_excel(writer, sheet_name='Feature Usage', index=False)\n",
    "            \n",
    "            # Sheet 4: Inactive Users\n",
    "            if 'inactive_users' in dir() and inactive_users is not None:\n",
    "                inactive_users.toPandas().to_excel(writer, sheet_name='Inactive Users', index=False)\n",
    "            \n",
    "            # Sheet 5: Recommendations\n",
    "            if recommendations_df is not None:\n",
    "                recommendations_df.toPandas().to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "            \n",
    "            # Sheet 6: Summary\n",
    "            summary_data = {\n",
    "                'Metric': [\n",
    "                    'Total Users',\n",
    "                    'Active Users',\n",
    "                    'DAU',\n",
    "                    'WAU',\n",
    "                    'MAU',\n",
    "                    'AI/Agent Adoption Rate (%)',\n",
    "                    'Power Users',\n",
    "                    'Inactive Users',\n",
    "                    'Analysis Period (days)',\n",
    "                    'Analysis Date'\n",
    "                ],\n",
    "                'Value': [\n",
    "                    users_df.count(),\n",
    "                    users_df.filter(F.col('active') == True).count(),\n",
    "                    dau,\n",
    "                    wau,\n",
    "                    mau,\n",
    "                    f\"{agent_adoption_rate:.1f}\" if 'agent_adoption_rate' in dir() else '0',\n",
    "                    user_metrics.filter(F.col('user_segment') == 'Power User').count(),\n",
    "                    inactive_count if 'inactive_count' in dir() else 0,\n",
    "                    LOOKBACK_DAYS,\n",
    "                    datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S')\n",
    "                ]\n",
    "            }\n",
    "            pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "        \n",
    "        # Apply formatting\n",
    "        from openpyxl import load_workbook\n",
    "        from openpyxl.styles import Font, PatternFill, Alignment\n",
    "        \n",
    "        wb = load_workbook(excel_path)\n",
    "        for sheet_name in wb.sheetnames:\n",
    "            ws = wb[sheet_name]\n",
    "            \n",
    "            # Format header row\n",
    "            for cell in ws[1]:\n",
    "                cell.font = Font(bold=True, color='FFFFFF')\n",
    "                cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "                cell.alignment = Alignment(horizontal='center')\n",
    "            \n",
    "            # Auto-adjust column widths\n",
    "            for column in ws.columns:\n",
    "                max_length = 0\n",
    "                column_letter = column[0].column_letter\n",
    "                for cell in column:\n",
    "                    if cell.value:\n",
    "                        max_length = max(max_length, len(str(cell.value)))\n",
    "                ws.column_dimensions[column_letter].width = min(max_length + 2, 50)\n",
    "        \n",
    "        wb.save(excel_path)\n",
    "        \n",
    "        log(f\"✓ Excel workbook created: {excel_path}\")\n",
    "        log(f\"  Sheets: {len(wb.sheetnames)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"✗ Excel export failed: {str(e)}\")\n",
    "else:\n",
    "    log(\"ℹ️  Excel export skipped\")\n",
    "\n",
    "log_execution_time(\"Excel Export\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7286b8da-1c90-45e3-b94b-a5139c837997",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to Delta table"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "if ENABLE_DELTA_EXPORT and user_metrics is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"EXPORTING TO DELTA TABLE\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Add audit metadata\n",
    "        user_metrics_export = user_metrics.withColumn('audit_timestamp', F.current_timestamp())\n",
    "        user_metrics_export = user_metrics_export.withColumn('lookback_days', F.lit(LOOKBACK_DAYS))\n",
    "        user_metrics_export = user_metrics_export.withColumn('dau', F.lit(dau))\n",
    "        user_metrics_export = user_metrics_export.withColumn('wau', F.lit(wau))\n",
    "        user_metrics_export = user_metrics_export.withColumn('mau', F.lit(mau))\n",
    "        user_metrics_export = user_metrics_export.withColumn('agent_adoption_rate', F.lit(agent_adoption_rate if 'agent_adoption_rate' in dir() else 0))\n",
    "        \n",
    "        # Write to Delta table (append mode)\n",
    "        user_metrics_export.write \\\n",
    "            .format('delta') \\\n",
    "            .mode('append') \\\n",
    "            .option('mergeSchema', 'true') \\\n",
    "            .saveAsTable(DELTA_TABLE_NAME)\n",
    "        \n",
    "        log(f\"✓ Delta table updated: {DELTA_TABLE_NAME}\")\n",
    "        log(f\"  Mode: append (historical retention)\")\n",
    "        log(f\"  Rows added: {user_metrics.count()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"✗ Delta export failed: {str(e)}\")\n",
    "else:\n",
    "    log(\"ℹ️  Delta export skipped\")\n",
    "\n",
    "log_execution_time(\"Delta Export\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41dc542b-73e2-4fc8-87cb-529eeef0888f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execution summary"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "execution_time = time.time() - execution_stats['start_time']\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"EXECUTION SUMMARY\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "log(f\"\\n⏱️  Total execution time: {execution_time:.2f} seconds\")\n",
    "\n",
    "log(f\"\\n\uD83D\uDCCA Statistics:\")\n",
    "log(f\"  Users analyzed: {execution_stats['users_processed']}\")\n",
    "log(f\"  Audit records processed: {execution_stats['audit_records_processed']:,}\")\n",
    "log(f\"  API calls: {execution_stats['api_calls']}\")\n",
    "log(f\"  API failures: {execution_stats['api_failures']}\")\n",
    "\n",
    "if user_metrics is not None:\n",
    "    log(f\"\\n\uD83D\uDC65 Adoption Summary:\")\n",
    "    log(f\"  DAU: {dau}\")\n",
    "    log(f\"  WAU: {wau}\")\n",
    "    log(f\"  MAU: {mau}\")\n",
    "    if 'agent_adoption_rate' in dir():\n",
    "        log(f\"  AI/Agent adoption: {agent_adoption_rate:.1f}%\")\n",
    "    log(f\"  Power users: {user_metrics.filter(F.col('user_segment') == 'Power User').count()}\")\n",
    "    log(f\"  Inactive users: {inactive_count if 'inactive_count' in dir() else 0}\")\n",
    "\n",
    "if recommendations_df is not None:\n",
    "    log(f\"\\n\uD83D\uDCA1 Recommendations: {recommendations_df.count()}\")\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"✓ COLLABORATION & ADOPTION ANALYSIS COMPLETE\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Return JSON summary for job mode\n",
    "if is_job_mode:\n",
    "    import json\n",
    "    summary = {\n",
    "        'status': 'success',\n",
    "        'execution_time_seconds': execution_time,\n",
    "        'users_analyzed': execution_stats['users_processed'],\n",
    "        'dau': dau,\n",
    "        'wau': wau,\n",
    "        'mau': mau,\n",
    "        'agent_adoption_rate': agent_adoption_rate if 'agent_adoption_rate' in dir() else 0,\n",
    "        'inactive_users': inactive_count if 'inactive_count' in dir() else 0,\n",
    "        'recommendations': recommendations_df.count() if recommendations_df is not None else 0,\n",
    "        'timestamp': datetime.now(eastern).isoformat()\n",
    "    }\n",
    "    dbutils.notebook.exit(json.dumps(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366f5feb-6f6c-40fd-ac47-3550d71e88ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Collaboration & Adoption Monitor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
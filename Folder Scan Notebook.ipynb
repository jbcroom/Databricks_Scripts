{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d174041b-d5f3-4170-b536-888a8ae2134a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Audit Report Header"
    }
   },
   "source": [
    "# Workspace Access Review Audit\n",
    "\n",
    "## Purpose\n",
    "This notebook performs a comprehensive access review audit for workspace folders in Databricks. It identifies and tracks all notebook modifications within a configurable date range, with optional long-term retention and content snapshots.\n",
    "\n",
    "## Execution Modes\n",
    "\n",
    "This notebook automatically detects and optimizes for two execution modes:\n",
    "\n",
    "* **Interactive Mode**: When run manually in the notebook UI\n",
    "  * Displays results with `display()` and `print()` statements\n",
    "  * Shows progress messages and sample data\n",
    "  * Ideal for exploration and ad-hoc analysis\n",
    "\n",
    "* **Job Mode**: When run as a scheduled job or workflow\n",
    "  * Suppresses all `print()` and `display()` output for performance\n",
    "  * Optimized for automated execution\n",
    "  * No code changes required - automatically detected\n",
    "  * Automatically exports to Delta tables when enabled\n",
    "\n",
    "## What This Code Does\n",
    "\n",
    "1. **Recursive Notebook Scanning**: Scans all notebooks within a specified folder (or all workspace folders) and its subdirectories to build a complete inventory of workspace assets.\n",
    "\n",
    "2. **Audit Log Analysis**: Queries the `system.access.audit` table to retrieve all modification events including:\n",
    "   * Notebook updates and edits\n",
    "   * New notebook creation\n",
    "   * Notebook deletions\n",
    "   * Command executions\n",
    "   * Notebook attach/detach operations\n",
    "\n",
    "3. **Enhanced Change Tracking**: Analyzes modification patterns over time:\n",
    "   * Modification activity by day of week and hour\n",
    "   * Daily modification trends\n",
    "   * Peak activity periods\n",
    "   * User activity patterns\n",
    "\n",
    "4. **Content Snapshots**: Captures point-in-time notebook metadata:\n",
    "   * Current notebook state (size, language, last modified)\n",
    "   * Modification history per notebook\n",
    "   * Days active and modification frequency\n",
    "   * Complete modifier list\n",
    "\n",
    "5. **Long-Term Retention**: Exports audit data to Delta tables:\n",
    "   * Historical accumulation of audit events\n",
    "   * Snapshot tables for current state\n",
    "   * Enables trend analysis across multiple audit runs\n",
    "   * Query historical changes over time\n",
    "\n",
    "6. **Detailed Reporting**: Produces multiple views of the audit data:\n",
    "   * Per-event log with notebook path, action type, user, and timestamp\n",
    "   * Summary statistics by action type\n",
    "   * User activity analysis\n",
    "   * Per-notebook modification history\n",
    "   * Time-based modification patterns\n",
    "\n",
    "## Configuration\n",
    "Set these variables in Cell 3 to customize the audit scope:\n",
    "\n",
    "### Basic Configuration\n",
    "\n",
    "* **`BASE_PATH`**: Specify the folder to audit\n",
    "  * Set to a specific path (e.g., `\"/Shared/ProjectName\"`) to audit a particular folder\n",
    "  * Set to `\"\"` (empty string) to audit all workspace folders\n",
    "\n",
    "* **`START_DATE`**: Beginning of the audit period (format: `\"YYYY-MM-DD\"`)\n",
    "  * Example: `\"2025-01-01\"`\n",
    "\n",
    "* **`END_DATE`**: End of the audit period (format: `\"YYYY-MM-DD\"`, exclusive)\n",
    "  * Example: `\"2026-01-01\"` (will query up to but not including this date)\n",
    "\n",
    "* **`TIMEZONE`**: Timezone for displaying event timestamps (default: `\"America/New_York\"`)\n",
    "  * Common values: `\"America/New_York\"` (Eastern), `\"America/Chicago\"` (Central), `\"America/Denver\"` (Mountain), `\"America/Los_Angeles\"` (Pacific), `\"UTC\"`, `\"Europe/London\"`, etc.\n",
    "  * Audit logs are stored in UTC and automatically converted to the specified timezone\n",
    "\n",
    "* **`MAX_DETAILED_EVENTS`**: Maximum number of events to display in detailed logs (default: `1000`)\n",
    "  * Set to `9999` to retrieve **ALL events without any limit**\n",
    "  * Any other value limits the results to that number (prevents memory issues with large datasets)\n",
    "  * Increase if you need more detailed history, decrease for faster performance\n",
    "  * **Note**: Using `9999` (no limit) may cause memory issues with very large audit datasets\n",
    "\n",
    "### Advanced Features\n",
    "\n",
    "* **`ENABLE_DELTA_EXPORT`**: Export audit data to Delta table (default: `False`)\n",
    "  * Set to `True` to enable long-term retention\n",
    "  * Accumulates historical audit data across multiple runs\n",
    "  * Enables trend analysis and compliance reporting\n",
    "\n",
    "* **`DELTA_TABLE_NAME`**: Target Delta table name (default: `\"main.default.notebook_audit_history\"`)\n",
    "  * Format: `\"catalog.schema.table\"`\n",
    "  * Requires CREATE TABLE permissions\n",
    "  * Automatically creates snapshot table with `_snapshot` suffix\n",
    "\n",
    "* **`ENABLE_CONTENT_SNAPSHOT`**: Capture notebook metadata (default: `True`)\n",
    "  * Captures current notebook state (size, language, modified time)\n",
    "  * Combines with audit history for comprehensive view\n",
    "  * Provides point-in-time snapshot of workspace state\n",
    "\n",
    "## Performance Optimizations\n",
    "\n",
    "* **Partition Pruning**: Date range filters enable efficient partition pruning on the audit table\n",
    "* **Selective Column Selection**: Queries select only needed columns to reduce data transfer\n",
    "* **Configurable Limits**: MAX_DETAILED_EVENTS prevents memory issues with large result sets (unless set to 9999)\n",
    "* **No Unnecessary Caching**: Optimized for serverless compute environments\n",
    "* **Recursion Protection**: Max depth limit prevents infinite loops in folder scanning\n",
    "* **Mode-Specific Execution**: Job mode skips all display operations for better performance\n",
    "* **Delta Table Optimization**: Uses append mode for history, overwrite for snapshots\n",
    "\n",
    "## Output\n",
    "The audit report provides:\n",
    "* Complete trail of who accessed and modified notebooks\n",
    "* Time-based modification patterns and trends\n",
    "* Point-in-time snapshots of notebook state\n",
    "* Optional long-term retention in Delta tables\n",
    "* Suitable for compliance, security reviews, and access governance\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "1. **Compliance Auditing**: Track all modifications for regulatory requirements\n",
    "2. **Security Reviews**: Identify unusual access patterns or unauthorized changes\n",
    "3. **Team Analytics**: Understand team collaboration patterns and peak activity times\n",
    "4. **Change Management**: Monitor notebook lifecycle and modification frequency\n",
    "5. **Historical Analysis**: Query trends across multiple audit periods using Delta tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01bf528e-f67c-4f58-bf34-c3e89ccdc3a3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Version Control Table"
    }
   },
   "source": [
    "## Version Control\n",
    "\n",
    "| Version | Date | Author | Changes |\n",
    "|---------|------|--------|----------|\n",
    "| 1.0 | 2026-02-10 | Brandon Croom | Created comprehensive audit framework with recursive notebook scanning using Databricks SDK WorkspaceClient; Configurable BASE_PATH for specific folders or all workspace folders; Configurable date range (START_DATE, END_DATE) and timezone (TIMEZONE); Audit log queries for notebook modification events (modifyNotebook, createNotebook, deleteNotebook, runCommand, etc.); Dynamic path filtering with trailing slash handling; UTC to local timezone conversion for all timestamps; Multiple reporting views (per-event log, summary statistics, user activity, per-notebook history); Performance optimizations (removed caching for serverless compatibility, optimized queries); Report generation timestamp in configured timezone |\n",
    "| 1.1 | 2026-02-11 | Brandon Croom | Added automatic job vs interactive mode detection; Conditional display/print statements for performance in job mode; Removed unnecessary cache operations; Added MAX_DETAILED_EVENTS configuration to prevent memory issues with special value 9999 to retrieve ALL events without any limit; Conditional LIMIT clause in detailed log and final report queries; Enhanced error handling (permission denied, max recursion depth protection); Optimized final report query to select only needed columns; Added performance comments throughout code; Improved WorkspaceClient initialization with error handling; Updated documentation to explain unlimited data retrieval option; Dynamic messaging based on limit setting; Enhanced change tracking with time-based analysis (modification patterns by day/hour, daily trends); Implemented content snapshot feature to capture point-in-time notebook metadata (size, language, modified time) combined with audit history; Added Delta table export for long-term retention with configurable ENABLE_DELTA_EXPORT and DELTA_TABLE_NAME; Automatic snapshot table creation with _snapshot suffix; Historical accumulation mode for audit events; Schema merge support for evolving data structures; Enhanced notebook scanning to capture metadata when snapshots enabled; Comprehensive documentation of new features and use cases |\n",
    "| 1.2 | 2026-02-12 | Brandon Croom | Added serverless vs traditional cluster detection with automatic compute type identification; Implemented compute-aware optimizations (no caching on serverless, automatic memory management); Added clear logging about compute environment and execution mode; Enhanced environment information display showing job mode and compute type; Improved portability across all compute types (serverless, traditional clusters, interactive clusters); Updated documentation to reflect serverless optimization capabilities; Added comprehensive enhancement suite: Stale notebook detection with multiple thresholds (90/180/365 days) for compliance and workspace cleanup; Notebook ownership identification tracking who created each notebook for accountability; Execution failure analysis identifying notebooks with failed runCommand events for quality insights; Language distribution analysis; Folder-level statistics with notebook counts and language usage per folder; Notebook size analysis with large notebook detection (>1MB); Security and compliance analysis including after-hours modification detection (outside 8 AM-6 PM), notebook deletion tracking (high-risk events), and external user activity detection with configurable company domain; Collaboration metrics identifying notebooks with multiple editors and highly collaborative notebooks (3+ editors); Visual analytics with activity heatmaps (day/hour patterns, daily trends, 4-chart dashboard); Enhanced Excel export with 9-sheet comprehensive workbook (Executive Summary, Events, Users, Notebooks, Stale, Ownership, Failures, Languages, Security); JSON export for API integration and programmatic access; All features gracefully handle empty data and work on serverless compute; Genericized all code by removing EAP references and replacing email addresses with author name; Updated BASE_PATH to generic empty string with helpful examples; Complete documentation updates |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4f2ed16-4425-4d69-b027-1675437a25dc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Recursively scan notebooks"
    }
   },
   "outputs": [],
   "source": [
    "# Recursively scan all notebooks in workspace folders\n",
    "import os\n",
    "from datetime import datetime\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# ============================================================================\n",
    "# MODE DETECTION: Automatically detect job vs interactive mode\n",
    "# ============================================================================\n",
    "try:\n",
    "    # Check if running in a job context\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().currentRunId().isDefined()\n",
    "    IS_JOB_MODE = True\n",
    "except:\n",
    "    IS_JOB_MODE = False\n",
    "\n",
    "# ============================================================================\n",
    "# COMPUTE DETECTION: Automatically detect serverless vs traditional cluster\n",
    "# ============================================================================\n",
    "try:\n",
    "    test_df = spark.range(1)\n",
    "    test_df.cache()\n",
    "    test_df.count()\n",
    "    test_df.unpersist()\n",
    "    IS_SERVERLESS = False\n",
    "except Exception as e:\n",
    "    # If caching fails with serverless error, we're on serverless\n",
    "    IS_SERVERLESS = 'SERVERLESS' in str(e) or 'PERSIST TABLE is not supported' in str(e)\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION: Set the folder path, date range, and timezone\n",
    "# ============================================================================\n",
    "# Set to a specific folder path or leave blank (\"\") to scan all workspace folders\n",
    "# Examples:\n",
    "#   BASE_PATH = \"/Shared/YourFolder/\"        # Scan a specific shared folder\n",
    "#   BASE_PATH = \"/Users/your.email@company.com/\"  # Scan your user folder\n",
    "#   BASE_PATH = \"\"                           # Scan ALL workspace folders\n",
    "BASE_PATH = \"\"\n",
    "\n",
    "# Set the date range for audit log queries (format: 'YYYY-MM-DD')\n",
    "START_DATE = \"2025-01-01\"\n",
    "END_DATE = \"2026-01-01\"  # Exclusive - will query up to but not including this date\n",
    "\n",
    "# Set the timezone for displaying event timestamps\n",
    "# Common values: 'America/New_York' (Eastern), 'America/Chicago' (Central),\n",
    "#                'America/Denver' (Mountain), 'America/Los_Angeles' (Pacific),\n",
    "#                'UTC', 'Europe/London', etc.\n",
    "TIMEZONE = \"America/New_York\"\n",
    "\n",
    "# Maximum number of events to display in detailed log\n",
    "# Set to 9999 to retrieve ALL events without any limit\n",
    "# Any other value will limit the results to that number (prevents memory issues)\n",
    "MAX_DETAILED_EVENTS = 9999\n",
    "\n",
    "# ============================================================================\n",
    "# LONG-TERM RETENTION: Export audit data to Delta table\n",
    "# ============================================================================\n",
    "# Enable exporting audit results to a Delta table for long-term retention\n",
    "ENABLE_DELTA_EXPORT = False\n",
    "\n",
    "# Target Delta table name (format: catalog.schema.table)\n",
    "# Example: \"main.audit_reports.notebook_modifications\"\n",
    "DELTA_TABLE_NAME = \"main.default.notebook_audit_history\"\n",
    "\n",
    "# ============================================================================\n",
    "# CONTENT SNAPSHOTS: Capture notebook metadata at time of audit\n",
    "# ============================================================================\n",
    "# Enable capturing current notebook metadata (size, language, modified time)\n",
    "# This provides a point-in-time snapshot of notebook state\n",
    "ENABLE_CONTENT_SNAPSHOT = True\n",
    "# ============================================================================\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Print only in interactive mode\"\"\"\n",
    "    if not IS_JOB_MODE:\n",
    "        print(message)\n",
    "\n",
    "def list_workspace_notebooks_recursive(w, path, depth=0, max_depth=50):\n",
    "    \"\"\"\n",
    "    Recursively list all notebooks in a given workspace path.\n",
    "    Returns a list of dictionaries with notebook details.\n",
    "    \n",
    "    Args:\n",
    "        w: WorkspaceClient instance\n",
    "        path: Path to scan\n",
    "        depth: Current recursion depth (for protection against infinite loops)\n",
    "        max_depth: Maximum recursion depth allowed\n",
    "    \"\"\"\n",
    "    # Protection against infinite recursion\n",
    "    if depth > max_depth:\n",
    "        log(f\"⚠️  Max recursion depth reached at {path}\")\n",
    "        return []\n",
    "    \n",
    "    notebooks = []\n",
    "    \n",
    "    try:\n",
    "        # Use Databricks SDK WorkspaceClient to list workspace objects\n",
    "        items = w.workspace.list(path)\n",
    "        \n",
    "        for item in items:\n",
    "            if item.object_type and item.object_type.name == \"DIRECTORY\":\n",
    "                # Recursively scan subdirectories\n",
    "                notebooks.extend(list_workspace_notebooks_recursive(w, item.path, depth + 1, max_depth))\n",
    "            elif item.object_type and item.object_type.name == \"NOTEBOOK\":\n",
    "                # Notebook object - capture enhanced metadata if enabled\n",
    "                notebook_info = {\n",
    "                    'path': item.path,\n",
    "                    'name': os.path.basename(item.path),\n",
    "                    'object_type': item.object_type.name,\n",
    "                    'language': getattr(item, 'language', 'UNKNOWN')\n",
    "                }\n",
    "                \n",
    "                # Add snapshot metadata if enabled\n",
    "                if ENABLE_CONTENT_SNAPSHOT:\n",
    "                    notebook_info['modified_at'] = getattr(item, 'modified_at', None)\n",
    "                    notebook_info['size'] = getattr(item, 'size', None)\n",
    "                    notebook_info['created_at'] = getattr(item, 'created_at', None)\n",
    "                \n",
    "                notebooks.append(notebook_info)\n",
    "    except Exception as e:\n",
    "        # If path doesn't exist or can't be accessed\n",
    "        error_str = str(e)\n",
    "        if \"does not exist\" in error_str or \"RESOURCE_DOES_NOT_EXIST\" in error_str:\n",
    "            log(f\"⚠️  Folder not found: {path}\")\n",
    "            log(f\"   Please verify the folder exists in your workspace.\")\n",
    "        elif \"PERMISSION_DENIED\" in error_str:\n",
    "            log(f\"⚠️  Permission denied: {path}\")\n",
    "        else:\n",
    "            log(f\"Error accessing {path}: {e}\")\n",
    "        return notebooks\n",
    "    \n",
    "    return notebooks\n",
    "\n",
    "# Initialize WorkspaceClient with error handling\n",
    "try:\n",
    "    w = WorkspaceClient()\n",
    "except Exception as e:\n",
    "    log(f\"❌ Failed to initialize WorkspaceClient: {e}\")\n",
    "    raise\n",
    "\n",
    "# Display environment information\n",
    "log(\"=\"*60)\n",
    "log(\"WORKSPACE ACCESS REVIEW AUDIT\")\n",
    "log(\"=\"*60)\n",
    "log(f\"Execution Mode: {'JOB' if IS_JOB_MODE else 'INTERACTIVE'}\")\n",
    "log(f\"Compute Type: {'SERVERLESS' if IS_SERVERLESS else 'TRADITIONAL CLUSTER'}\")\n",
    "\n",
    "if IS_SERVERLESS:\n",
    "    log(\"\\n⚡ Serverless optimizations enabled:\")\n",
    "    log(\"  - Automatic memory management\")\n",
    "    log(\"  - No explicit caching needed\")\n",
    "    log(\"  - Optimized for fast startup\")\n",
    "else:\n",
    "    log(\"\\n\uD83D\uDD27 Traditional cluster:\")\n",
    "    log(\"  - Manual memory management available\")\n",
    "    log(\"  - Persistent compute resources\")\n",
    "\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Determine scan path\n",
    "if BASE_PATH == \"\" or BASE_PATH is None:\n",
    "    scan_path = \"/\"\n",
    "    log(\"Scanning ALL workspace folders (this may take a while)...\")\n",
    "else:\n",
    "    scan_path = BASE_PATH\n",
    "    log(f\"Scanning workspace folder: {scan_path}\")\n",
    "\n",
    "log(\"=\"*60)\n",
    "\n",
    "notebooks = list_workspace_notebooks_recursive(w, scan_path)\n",
    "\n",
    "if len(notebooks) == 0:\n",
    "    log(f\"\\n❌ No notebooks found in {scan_path}\")\n",
    "    log(f\"\\nPossible reasons:\")\n",
    "    log(f\"  1. The folder doesn't exist in this workspace\")\n",
    "    log(f\"  2. The folder is empty\")\n",
    "    log(f\"  3. You don't have permission to access it\")\n",
    "    log(f\"\\n\uD83D\uDCA1 To verify, check if the folder exists in the workspace browser.\")\n",
    "else:\n",
    "    log(f\"\\n✅ Found {len(notebooks)} notebook(s) in {scan_path}\")\n",
    "    if not IS_JOB_MODE:\n",
    "        log(f\"\\nSample notebooks:\")\n",
    "        for nb in notebooks[:10]:\n",
    "            log(f\"  - {nb['path']} ({nb.get('language', 'UNKNOWN')})\")\n",
    "        \n",
    "        if len(notebooks) > 10:\n",
    "            log(f\"  ... and {len(notebooks) - 10} more\")\n",
    "\n",
    "# Store notebook count for downstream use\n",
    "notebook_count = len(notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81abd399-0ef2-417c-91bc-30307639cf5c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Query audit logs for modification events"
    }
   },
   "outputs": [],
   "source": [
    "# Query audit logs for notebook modification events\n",
    "# Uses the BASE_PATH, START_DATE, END_DATE, and TIMEZONE variables from the previous cell\n",
    "\n",
    "# Build the path filter condition\n",
    "# Remove trailing slash from BASE_PATH if present\n",
    "base_path_clean = BASE_PATH.rstrip('/') if BASE_PATH else \"\"\n",
    "\n",
    "if base_path_clean == \"\" or base_path_clean is None:\n",
    "    path_filter = \"\"  # No path filter - scan all notebooks\n",
    "    filter_description = \"all workspace folders\"\n",
    "else:\n",
    "    path_filter = f\"AND request_params.path LIKE '{base_path_clean}/%'\"\n",
    "    filter_description = base_path_clean\n",
    "\n",
    "log(f\"Querying audit logs for: {filter_description}\")\n",
    "log(f\"Date range: {START_DATE} to {END_DATE} (exclusive)\")\n",
    "log(f\"Timezone: {TIMEZONE}\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Create the temp view with dynamic path filter and date range\n",
    "# NOTE: service_name is 'notebook' (singular), not 'notebooks'\n",
    "# Action names: modifyNotebook, createNotebook, deleteNotebook, etc.\n",
    "# This query focuses on MODIFICATION events only (not read/access events like openNotebook)\n",
    "# PERFORMANCE OPTIMIZATIONS:\n",
    "# - Removed ORDER BY from view definition - ordering happens in individual queries\n",
    "# - event_date filter enables partition pruning on the audit table\n",
    "# - service_name and action_name filters reduce data scanned\n",
    "# TIMEZONE: Convert event_time from UTC to configured timezone\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW audit_events AS\n",
    "SELECT \n",
    "  from_utc_timestamp(event_time, '{TIMEZONE}') as event_time,\n",
    "  event_date,\n",
    "  action_name,\n",
    "  user_identity.email as user_email,\n",
    "  request_params.path as notebook_path,\n",
    "  request_params.notebook_id as notebook_id,\n",
    "  service_name,\n",
    "  request_id,\n",
    "  response.status_code\n",
    "FROM system.access.audit\n",
    "WHERE \n",
    "  event_date >= '{START_DATE}' \n",
    "  AND event_date < '{END_DATE}'\n",
    "  AND service_name = 'notebook'\n",
    "  AND action_name IN (\n",
    "    'modifyNotebook',\n",
    "    'createNotebook', \n",
    "    'deleteNotebook',\n",
    "    'runCommand',\n",
    "    'submitCommand',\n",
    "    'attachNotebook',\n",
    "    'detachNotebook',\n",
    "    'renameNotebook',\n",
    "    'moveNotebook',\n",
    "    'cloneNotebook',\n",
    "    'importNotebook'\n",
    "  )\n",
    "  {path_filter}\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query)\n",
    "\n",
    "# PERFORMANCE: Use a lightweight query to check if data exists (LIMIT 1 instead of COUNT)\n",
    "has_data = spark.sql(\"SELECT 1 FROM audit_events LIMIT 1\").count() > 0\n",
    "\n",
    "log(f\"\\n✅ Temp view 'audit_events' created successfully\")\n",
    "log(f\"   All timestamps converted to {TIMEZONE}\")\n",
    "\n",
    "if not has_data:\n",
    "    log(f\"\\n⚠️  No modification events found for {filter_description}\")\n",
    "    log(f\"   Date range: {START_DATE} to {END_DATE}\")\n",
    "    log(f\"   This means no notebooks were created, modified, or deleted in this folder during this period.\")\n",
    "    log(f\"   Note: Read-only events like 'openNotebook' are excluded from this audit.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c71508e-72ba-4c00-80d0-c03c275dd701",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display audit events summary"
    }
   },
   "outputs": [],
   "source": [
    "# Summary of modification events by action type\n",
    "# PERFORMANCE: This aggregation is efficient as it groups by a low-cardinality column\n",
    "summary_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  action_name,\n",
    "  COUNT(*) as event_count,\n",
    "  COUNT(DISTINCT user_email) as unique_users,\n",
    "  COUNT(DISTINCT notebook_path) as unique_notebooks,\n",
    "  MIN(event_date) as first_event,\n",
    "  MAX(event_date) as last_event\n",
    "FROM audit_events\n",
    "GROUP BY action_name\n",
    "ORDER BY event_count DESC\n",
    "\"\"\")\n",
    "\n",
    "# Display only in interactive mode\n",
    "if not IS_JOB_MODE:\n",
    "    display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43963f40-7745-410d-b9bf-3c4cbaf0119e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Detailed per-event modification log"
    }
   },
   "outputs": [],
   "source": [
    "# Detailed per-event log showing notebook path, action type, user, and timestamp\n",
    "# PERFORMANCE: LIMIT prevents memory issues with large result sets\n",
    "# Set MAX_DETAILED_EVENTS to 9999 to retrieve ALL events without limit\n",
    "\n",
    "# Build query with conditional LIMIT clause\n",
    "if MAX_DETAILED_EVENTS == 9999:\n",
    "    limit_clause = \"\"  # No limit - retrieve all events\n",
    "else:\n",
    "    limit_clause = f\"LIMIT {MAX_DETAILED_EVENTS}\"\n",
    "\n",
    "detailed_log_df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  event_time,\n",
    "  notebook_path,\n",
    "  action_name,\n",
    "  user_email,\n",
    "  status_code as status,\n",
    "  request_id\n",
    "FROM audit_events\n",
    "ORDER BY event_time DESC\n",
    "{limit_clause}\n",
    "\"\"\")\n",
    "\n",
    "# Display only in interactive mode\n",
    "if not IS_JOB_MODE:\n",
    "    display(detailed_log_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87ba0182-748d-4eec-b4fc-2d260af5baac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "User activity summary"
    }
   },
   "outputs": [],
   "source": [
    "# Summary of modifications by user\n",
    "# PERFORMANCE: COLLECT_SET can be expensive with many distinct values\n",
    "# Consider the cardinality of action_name per user (typically low)\n",
    "user_activity_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  user_email,\n",
    "  COUNT(*) as total_modifications,\n",
    "  COUNT(DISTINCT notebook_path) as notebooks_modified,\n",
    "  MIN(event_date) as first_activity,\n",
    "  MAX(event_date) as last_activity,\n",
    "  COLLECT_SET(action_name) as action_types\n",
    "FROM audit_events\n",
    "GROUP BY user_email\n",
    "ORDER BY total_modifications DESC\n",
    "\"\"\")\n",
    "\n",
    "# Display only in interactive mode\n",
    "if not IS_JOB_MODE:\n",
    "    display(user_activity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643b5fbf-6f50-43b4-b97b-c0e525c7e5f8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notebook modification history"
    }
   },
   "outputs": [],
   "source": [
    "# Per-notebook modification history\n",
    "# PERFORMANCE: COLLECT_SET operations can be expensive with high cardinality\n",
    "# For notebooks with many modifiers, consider limiting or using alternative aggregations\n",
    "notebook_history_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  notebook_path,\n",
    "  COUNT(*) as modification_count,\n",
    "  COUNT(DISTINCT user_email) as unique_modifiers,\n",
    "  COLLECT_SET(user_email) as modifiers,\n",
    "  MIN(event_time) as first_modified,\n",
    "  MAX(event_time) as last_modified,\n",
    "  COLLECT_SET(action_name) as action_types\n",
    "FROM audit_events\n",
    "GROUP BY notebook_path\n",
    "ORDER BY modification_count DESC\n",
    "\"\"\")\n",
    "\n",
    "# Display only in interactive mode\n",
    "if not IS_JOB_MODE:\n",
    "    display(notebook_history_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "494210e9-6d4f-4a46-9ef6-1636bfbad937",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enhanced change tracking with time-based analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced change tracking: Analyze modification patterns over time\n",
    "# This provides insights into when notebooks are most actively modified\n",
    "\n",
    "from pyspark.sql.functions import date_format, hour, dayofweek, count, col\n",
    "\n",
    "# Modification patterns by day of week and hour\n",
    "time_pattern_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  DAYOFWEEK(event_time) as day_of_week,\n",
    "  CASE DAYOFWEEK(event_time)\n",
    "    WHEN 1 THEN 'Sunday'\n",
    "    WHEN 2 THEN 'Monday'\n",
    "    WHEN 3 THEN 'Tuesday'\n",
    "    WHEN 4 THEN 'Wednesday'\n",
    "    WHEN 5 THEN 'Thursday'\n",
    "    WHEN 6 THEN 'Friday'\n",
    "    WHEN 7 THEN 'Saturday'\n",
    "  END as day_name,\n",
    "  HOUR(event_time) as hour_of_day,\n",
    "  COUNT(*) as modification_count,\n",
    "  COUNT(DISTINCT notebook_path) as unique_notebooks,\n",
    "  COUNT(DISTINCT user_email) as unique_users\n",
    "FROM audit_events\n",
    "GROUP BY DAYOFWEEK(event_time), HOUR(event_time)\n",
    "ORDER BY day_of_week, hour_of_day\n",
    "\"\"\")\n",
    "\n",
    "log(\"\\n\uD83D\uDCCA Modification Patterns by Time\")\n",
    "log(\"   Analyzing when notebooks are most actively modified...\\n\")\n",
    "\n",
    "if not IS_JOB_MODE:\n",
    "    display(time_pattern_df)\n",
    "\n",
    "# Daily modification trends\n",
    "daily_trend_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  DATE(event_time) as modification_date,\n",
    "  COUNT(*) as total_modifications,\n",
    "  COUNT(DISTINCT notebook_path) as notebooks_modified,\n",
    "  COUNT(DISTINCT user_email) as active_users,\n",
    "  COLLECT_SET(action_name) as action_types\n",
    "FROM audit_events\n",
    "GROUP BY DATE(event_time)\n",
    "ORDER BY modification_date DESC\n",
    "\"\"\")\n",
    "\n",
    "log(\"\\n\uD83D\uDCC8 Daily Modification Trends\")\n",
    "log(\"   Day-by-day breakdown of modification activity...\\n\")\n",
    "\n",
    "if not IS_JOB_MODE:\n",
    "    display(daily_trend_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad72dc1-965e-47f5-bf8f-4e55ca4b2444",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stale notebook detection"
    }
   },
   "outputs": [],
   "source": [
    "# Stale Notebook Detection: Identify notebooks not modified recently\n",
    "# This helps with workspace cleanup and compliance\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"STALE NOTEBOOK DETECTION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if len(notebooks) > 0:\n",
    "    from pyspark.sql.functions import datediff, current_date, lit, to_date\n",
    "    \n",
    "    # Convert notebooks to DataFrame with explicit string conversion for timestamps\n",
    "    notebooks_simple = [{\n",
    "        'path': nb['path'],\n",
    "        'name': nb['name'],\n",
    "        'language': str(nb.get('language', 'UNKNOWN')),\n",
    "        'size': int(nb.get('size', 0)) if nb.get('size') else 0,\n",
    "        'modified_at': str(nb.get('modified_at', '')) if nb.get('modified_at') else None,\n",
    "        'created_at': str(nb.get('created_at', '')) if nb.get('created_at') else None\n",
    "    } for nb in notebooks]\n",
    "    \n",
    "    notebooks_df = spark.createDataFrame(notebooks_simple)\n",
    "    \n",
    "    # Get last modification from audit logs\n",
    "    last_modified_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      notebook_path,\n",
    "      MAX(event_date) as last_modified_date,\n",
    "      MAX(event_time) as last_modified_time,\n",
    "      COUNT(*) as total_modifications\n",
    "    FROM audit_events\n",
    "    GROUP BY notebook_path\n",
    "    \"\"\")\n",
    "    \n",
    "    # Join with notebooks to find stale ones\n",
    "    notebooks_with_activity = notebooks_df.join(\n",
    "        last_modified_df,\n",
    "        notebooks_df.path == last_modified_df.notebook_path,\n",
    "        'left'\n",
    "    )\n",
    "    \n",
    "    # Calculate days since last modification\n",
    "    stale_analysis = notebooks_with_activity.withColumn(\n",
    "        'days_since_modification',\n",
    "        datediff(current_date(), F.col('last_modified_date'))\n",
    "    )\n",
    "    \n",
    "    # Define stale thresholds\n",
    "    stale_90_days = stale_analysis.filter(\n",
    "        (F.col('days_since_modification') >= 90) | F.col('last_modified_date').isNull()\n",
    "    )\n",
    "    \n",
    "    stale_180_days = stale_analysis.filter(\n",
    "        (F.col('days_since_modification') >= 180) | F.col('last_modified_date').isNull()\n",
    "    )\n",
    "    \n",
    "    stale_365_days = stale_analysis.filter(\n",
    "        (F.col('days_since_modification') >= 365) | F.col('last_modified_date').isNull()\n",
    "    )\n",
    "    \n",
    "    # No modifications in audit period\n",
    "    no_activity = stale_analysis.filter(F.col('last_modified_date').isNull())\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCCA Stale Notebook Summary:\")\n",
    "    log(f\"  Total notebooks scanned: {notebooks_df.count()}\")\n",
    "    log(f\"  Not modified in 90+ days: {stale_90_days.count()}\")\n",
    "    log(f\"  Not modified in 180+ days: {stale_180_days.count()}\")\n",
    "    log(f\"  Not modified in 365+ days: {stale_365_days.count()}\")\n",
    "    log(f\"  No activity in audit period: {no_activity.count()}\")\n",
    "    \n",
    "    if stale_90_days.count() > 0:\n",
    "        log(f\"\\n⚠️  COMPLIANCE: {stale_90_days.count()} notebooks may be candidates for cleanup\")\n",
    "        \n",
    "        if not IS_JOB_MODE:\n",
    "            log(\"\\nTop 20 stale notebooks (90+ days):\")\n",
    "            display(stale_90_days.select(\n",
    "                'path', 'language', 'days_since_modification', 'last_modified_date'\n",
    "            ).orderBy(F.desc('days_since_modification')).limit(20))\n",
    "        \n",
    "        log(f\"\\n\uD83D\uDCA1 RECOMMENDATION: Review stale notebooks for archival or deletion\")\n",
    "        log(f\"   Consider workspace cleanup policies for notebooks inactive >90 days\")\n",
    "    else:\n",
    "        log(\"\\n✓ All notebooks have been modified within 90 days\")\n",
    "    \n",
    "    # Store for export\n",
    "    stale_notebooks_df = stale_90_days\n",
    "    \n",
    "else:\n",
    "    log(\"\\n⚠️  No notebooks found - skipping stale detection\")\n",
    "    stale_notebooks_df = spark.createDataFrame([], 'path STRING, language STRING, days_since_modification INT, last_modified_date DATE')\n",
    "\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe1a2700-b66a-4105-996c-9ec5c44a3619",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Notebook ownership identification"
    }
   },
   "outputs": [],
   "source": [
    "# Notebook Ownership: Identify who created each notebook\n",
    "# Helps with accountability and cleanup decisions\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"NOTEBOOK OWNERSHIP IDENTIFICATION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if len(notebooks) > 0:\n",
    "    # Find the first createNotebook event for each notebook\n",
    "    ownership_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      notebook_path,\n",
    "      user_email as owner,\n",
    "      MIN(event_time) as created_time,\n",
    "      MIN(event_date) as created_date\n",
    "    FROM audit_events\n",
    "    WHERE action_name = 'createNotebook'\n",
    "    GROUP BY notebook_path, user_email\n",
    "    \"\"\")\n",
    "    \n",
    "    # Get the earliest creation event per notebook (in case multiple users created it)\n",
    "    first_creation = spark.sql(\"\"\"\n",
    "    WITH ranked_creations AS (\n",
    "      SELECT \n",
    "        notebook_path,\n",
    "        user_email as owner,\n",
    "        event_time as created_time,\n",
    "        event_date as created_date,\n",
    "        ROW_NUMBER() OVER (PARTITION BY notebook_path ORDER BY event_time ASC) as rn\n",
    "      FROM audit_events\n",
    "      WHERE action_name = 'createNotebook'\n",
    "    )\n",
    "    SELECT notebook_path, owner, created_time, created_date\n",
    "    FROM ranked_creations\n",
    "    WHERE rn = 1\n",
    "    \"\"\")\n",
    "    \n",
    "    ownership_count = first_creation.count()\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCCB Ownership Summary:\")\n",
    "    log(f\"  Notebooks with identified owners: {ownership_count}\")\n",
    "    log(f\"  Notebooks without owner info: {len(notebooks) - ownership_count}\")\n",
    "    \n",
    "    if ownership_count > 0:\n",
    "        # Owner statistics\n",
    "        owner_stats = first_creation.groupBy('owner') \\\n",
    "            .agg(\n",
    "                F.count('*').alias('notebooks_owned')\n",
    "            ) \\\n",
    "            .orderBy(F.desc('notebooks_owned'))\n",
    "        \n",
    "        log(f\"\\n\uD83D\uDC65 Top notebook owners:\")\n",
    "        if not IS_JOB_MODE:\n",
    "            display(owner_stats.limit(20))\n",
    "        \n",
    "        # Notebooks without owners (created before audit period)\n",
    "        notebooks_df = spark.createDataFrame(notebooks)\n",
    "        notebooks_without_owners = notebooks_df.join(\n",
    "            first_creation,\n",
    "            notebooks_df.path == first_creation.notebook_path,\n",
    "            'left_anti'\n",
    "        )\n",
    "        \n",
    "        orphaned_count = notebooks_without_owners.count()\n",
    "        \n",
    "        if orphaned_count > 0:\n",
    "            log(f\"\\n⚠️  {orphaned_count} notebooks have no owner information\")\n",
    "            log(f\"   These were likely created before {START_DATE}\")\n",
    "            log(f\"   Consider extending the audit date range to capture creation events\")\n",
    "        \n",
    "        # Store for export\n",
    "        notebook_ownership_df = first_creation\n",
    "    else:\n",
    "        log(\"\\n⚠️  No ownership information found in audit period\")\n",
    "        log(f\"   Notebooks may have been created before {START_DATE}\")\n",
    "        notebook_ownership_df = spark.createDataFrame([], 'notebook_path STRING, owner STRING, created_time TIMESTAMP, created_date DATE')\n",
    "else:\n",
    "    log(\"\\n⚠️  No notebooks found - skipping ownership identification\")\n",
    "    notebook_ownership_df = spark.createDataFrame([], 'notebook_path STRING, owner STRING, created_time TIMESTAMP, created_date DATE')\n",
    "\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "478ce252-da63-4c96-bda9-3a513524117d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execution failure analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Execution Failure Analysis: Identify notebooks with failed executions\n",
    "# Helps identify quality and reliability issues\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"EXECUTION FAILURE ANALYSIS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Query for failed runCommand events\n",
    "failed_executions_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  notebook_path,\n",
    "  user_email,\n",
    "  event_time,\n",
    "  event_date,\n",
    "  status_code,\n",
    "  request_id\n",
    "FROM audit_events\n",
    "WHERE action_name = 'runCommand' \n",
    "  AND (status_code != '200' OR status_code IS NULL)\n",
    "ORDER BY event_time DESC\n",
    "\"\"\")\n",
    "\n",
    "failed_count = failed_executions_df.count()\n",
    "\n",
    "log(f\"\\n\uD83D\uDD0D Execution Failure Summary:\")\n",
    "log(f\"  Total failed executions: {failed_count}\")\n",
    "\n",
    "if failed_count > 0:\n",
    "    log(f\"\\n⚠️  QUALITY ALERT: Notebooks with execution failures detected\")\n",
    "    \n",
    "    # Notebooks with most failures\n",
    "    failure_summary = failed_executions_df.groupBy('notebook_path') \\\n",
    "        .agg(\n",
    "            F.count('*').alias('failure_count'),\n",
    "            F.countDistinct('user_email').alias('users_affected'),\n",
    "            F.max('event_date').alias('last_failure_date')\n",
    "        ) \\\n",
    "        .orderBy(F.desc('failure_count'))\n",
    "    \n",
    "    log(f\"  Notebooks with failures: {failure_summary.count()}\")\n",
    "    \n",
    "    if not IS_JOB_MODE:\n",
    "        log(\"\\nTop notebooks by failure count:\")\n",
    "        display(failure_summary.limit(20))\n",
    "    \n",
    "    # Users with most failures\n",
    "    user_failures = failed_executions_df.groupBy('user_email') \\\n",
    "        .agg(\n",
    "            F.count('*').alias('failure_count'),\n",
    "            F.countDistinct('notebook_path').alias('notebooks_affected')\n",
    "        ) \\\n",
    "        .orderBy(F.desc('failure_count'))\n",
    "    \n",
    "    if not IS_JOB_MODE:\n",
    "        log(\"\\nUsers with most execution failures:\")\n",
    "        display(user_failures.limit(10))\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCA1 RECOMMENDATION: Investigate notebooks with high failure rates\")\n",
    "    log(f\"   Failed executions may indicate code quality or data issues\")\n",
    "    \n",
    "    # Store for export\n",
    "    notebook_failures_df = failure_summary\n",
    "else:\n",
    "    log(\"\\n✓ No execution failures detected in audit period\")\n",
    "    log(f\"   All runCommand events completed successfully\")\n",
    "    notebook_failures_df = spark.createDataFrame([], 'notebook_path STRING, failure_count LONG, users_affected LONG, last_failure_date DATE')\n",
    "\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28e292fa-fbda-466e-85e0-a5f75dd6a397",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Language distribution and folder statistics"
    }
   },
   "outputs": [],
   "source": [
    "# Language Distribution and Folder Statistics\n",
    "# Provides insights into notebook composition and organization\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"LANGUAGE DISTRIBUTION & FOLDER STATISTICS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if len(notebooks) > 0:\n",
    "    # Reuse notebooks_df from previous cell if available, otherwise create it\n",
    "    if 'notebooks_df' not in dir():\n",
    "        notebooks_simple = [{\n",
    "            'path': nb['path'],\n",
    "            'name': nb['name'],\n",
    "            'language': str(nb.get('language', 'UNKNOWN')),\n",
    "            'size': int(nb.get('size', 0)) if nb.get('size') else 0\n",
    "        } for nb in notebooks]\n",
    "        notebooks_df = spark.createDataFrame(notebooks_simple)\n",
    "    \n",
    "    # Language distribution\n",
    "    log(\"\\n\uD83D\uDCCA Language Distribution:\")\n",
    "    language_dist = notebooks_df.groupBy('language') \\\n",
    "        .agg(F.count('*').alias('notebook_count')) \\\n",
    "        .orderBy(F.desc('notebook_count'))\n",
    "    \n",
    "    for row in language_dist.collect():\n",
    "        percentage = (row['notebook_count'] / len(notebooks)) * 100\n",
    "        log(f\"  {row['language']}: {row['notebook_count']} notebooks ({percentage:.1f}%)\")\n",
    "    \n",
    "    if not IS_JOB_MODE:\n",
    "        display(language_dist)\n",
    "    \n",
    "    # Folder-level statistics\n",
    "    log(\"\\n\uD83D\uDCC1 Folder-Level Statistics:\")\n",
    "    \n",
    "    # Extract folder path from notebook path\n",
    "    notebooks_with_folder = notebooks_df.withColumn(\n",
    "        'folder_path',\n",
    "        F.expr(\"substring(path, 1, length(path) - length(name) - 1)\")\n",
    "    )\n",
    "    \n",
    "    folder_stats = notebooks_with_folder.groupBy('folder_path') \\\n",
    "        .agg(\n",
    "            F.count('*').alias('notebook_count'),\n",
    "            F.collect_set('language').alias('languages_used')\n",
    "        ) \\\n",
    "        .orderBy(F.desc('notebook_count'))\n",
    "    \n",
    "    log(f\"  Total folders: {folder_stats.count()}\")\n",
    "    log(f\"  Average notebooks per folder: {len(notebooks) / folder_stats.count():.1f}\")\n",
    "    \n",
    "    if not IS_JOB_MODE:\n",
    "        log(\"\\nTop folders by notebook count:\")\n",
    "        display(folder_stats.limit(20))\n",
    "    \n",
    "    # Notebook size analysis (if available)\n",
    "    if 'size' in notebooks_df.columns:\n",
    "        size_stats = notebooks_df.filter(F.col('size') > 0).agg(\n",
    "            F.avg('size').alias('avg_size'),\n",
    "            F.max('size').alias('max_size'),\n",
    "            F.min('size').alias('min_size'),\n",
    "            F.sum('size').alias('total_size')\n",
    "        ).collect()\n",
    "        \n",
    "        if len(size_stats) > 0 and size_stats[0].avg_size is not None:\n",
    "            size_row = size_stats[0]\n",
    "            \n",
    "            log(f\"\\n\uD83D\uDCBE Notebook Size Analysis:\")\n",
    "            log(f\"  Average size: {size_row.avg_size / 1024:.1f} KB\")\n",
    "            log(f\"  Largest notebook: {size_row.max_size / 1024:.1f} KB\")\n",
    "            log(f\"  Total size: {size_row.total_size / (1024 * 1024):.1f} MB\")\n",
    "            \n",
    "            # Identify large notebooks (>1MB)\n",
    "            large_notebooks = notebooks_df.filter(F.col('size') > 1024 * 1024)\n",
    "            \n",
    "            if large_notebooks.count() > 0:\n",
    "                log(f\"\\n⚠️  Large notebooks (>1MB): {large_notebooks.count()}\")\n",
    "                if not IS_JOB_MODE:\n",
    "                    display(large_notebooks.select('path', 'language', (F.col('size') / 1024).alias('size_kb')).orderBy(F.desc('size')))\n",
    "    \n",
    "    # Store for export\n",
    "    language_distribution_df = language_dist\n",
    "    folder_statistics_df = folder_stats\n",
    "    \n",
    "else:\n",
    "    log(\"\\n⚠️  No notebooks found - skipping analysis\")\n",
    "    language_distribution_df = spark.createDataFrame([], 'language STRING, notebook_count LONG')\n",
    "    folder_statistics_df = spark.createDataFrame([], 'folder_path STRING, notebook_count LONG, languages_used ARRAY<STRING>')\n",
    "\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b79e4bf-726c-4067-8561-9e8a27451be8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Security and compliance analysis"
    }
   },
   "outputs": [],
   "source": [
    "# Security and Compliance Analysis\n",
    "# Detect after-hours modifications, deletions, and external users\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"SECURITY & COMPLIANCE ANALYSIS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# 1. After-hours modifications (outside 8 AM - 6 PM)\n",
    "log(\"\\n1. After-Hours Modifications:\")\n",
    "\n",
    "after_hours_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  notebook_path,\n",
    "  user_email,\n",
    "  event_time,\n",
    "  action_name,\n",
    "  HOUR(event_time) as hour_of_day\n",
    "FROM audit_events\n",
    "WHERE HOUR(event_time) < 8 OR HOUR(event_time) >= 18\n",
    "ORDER BY event_time DESC\n",
    "\"\"\")\n",
    "\n",
    "after_hours_count = after_hours_df.count()\n",
    "\n",
    "log(f\"  After-hours modifications: {after_hours_count}\")\n",
    "\n",
    "if after_hours_count > 0:\n",
    "    log(f\"\\n⚠️  SECURITY REVIEW: Modifications outside business hours (8 AM - 6 PM)\")\n",
    "    \n",
    "    # Users with most after-hours activity\n",
    "    after_hours_users = after_hours_df.groupBy('user_email') \\\n",
    "        .agg(F.count('*').alias('after_hours_count')) \\\n",
    "        .orderBy(F.desc('after_hours_count'))\n",
    "    \n",
    "    if not IS_JOB_MODE:\n",
    "        log(\"\\nUsers with after-hours activity:\")\n",
    "        display(after_hours_users.limit(10))\n",
    "else:\n",
    "    log(\"  ✓ All modifications occurred during business hours\")\n",
    "\n",
    "# 2. Deletion events (high-risk)\n",
    "log(\"\\n2. Notebook Deletions:\")\n",
    "\n",
    "deletions_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  notebook_path,\n",
    "  user_email,\n",
    "  event_time,\n",
    "  event_date\n",
    "FROM audit_events\n",
    "WHERE action_name = 'deleteNotebook'\n",
    "ORDER BY event_time DESC\n",
    "\"\"\")\n",
    "\n",
    "deletion_count = deletions_df.count()\n",
    "\n",
    "log(f\"  Notebooks deleted: {deletion_count}\")\n",
    "\n",
    "if deletion_count > 0:\n",
    "    log(f\"\\n⚠️  HIGH-RISK: {deletion_count} notebooks were deleted in audit period\")\n",
    "    \n",
    "    if not IS_JOB_MODE:\n",
    "        log(\"\\nDeleted notebooks:\")\n",
    "        display(deletions_df)\n",
    "    \n",
    "    # Users who deleted notebooks\n",
    "    deleters = deletions_df.groupBy('user_email') \\\n",
    "        .agg(F.count('*').alias('deletions')) \\\n",
    "        .orderBy(F.desc('deletions'))\n",
    "    \n",
    "    log(f\"\\n  Users who deleted notebooks: {deleters.count()}\")\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCA1 RECOMMENDATION: Review deletion events for compliance\")\n",
    "    log(f\"   Ensure deletions were authorized and documented\")\n",
    "else:\n",
    "    log(\"  ✓ No notebooks were deleted in audit period\")\n",
    "\n",
    "# 3. External user modifications\n",
    "log(\"\\n3. External User Activity:\")\n",
    "\n",
    "COMPANY_DOMAIN = 'bat.com'  # Customize for your organization\n",
    "\n",
    "external_users_df = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "  user_email,\n",
    "  COUNT(*) as modification_count,\n",
    "  COUNT(DISTINCT notebook_path) as notebooks_modified,\n",
    "  MIN(event_date) as first_activity,\n",
    "  MAX(event_date) as last_activity\n",
    "FROM audit_events\n",
    "WHERE user_email NOT LIKE '%@{COMPANY_DOMAIN}'\n",
    "GROUP BY user_email\n",
    "ORDER BY modification_count DESC\n",
    "\"\"\")\n",
    "\n",
    "external_count = external_users_df.count()\n",
    "\n",
    "log(f\"  External users (non-@{COMPANY_DOMAIN}): {external_count}\")\n",
    "\n",
    "if external_count > 0:\n",
    "    log(f\"\\n⚠️  SECURITY REVIEW: External users modified notebooks\")\n",
    "    \n",
    "    if not IS_JOB_MODE:\n",
    "        display(external_users_df)\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCA1 RECOMMENDATION: Review external user access\")\n",
    "    log(f\"   Verify external users have appropriate business justification\")\n",
    "else:\n",
    "    log(f\"  ✓ All modifications by @{COMPANY_DOMAIN} users\")\n",
    "\n",
    "# Store for export\n",
    "after_hours_modifications_df = after_hours_df\n",
    "notebook_deletions_df = deletions_df\n",
    "external_user_activity_df = external_users_df\n",
    "\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa34063-564a-41c8-8fbe-5fcf8f65ac8b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Collaboration metrics"
    }
   },
   "outputs": [],
   "source": [
    "# Collaboration Metrics: Analyze team collaboration patterns\n",
    "# Identifies notebooks with multiple editors and collaboration intensity\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"COLLABORATION METRICS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Notebooks with multiple editors\n",
    "collaboration_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  notebook_path,\n",
    "  COUNT(DISTINCT user_email) as unique_editors,\n",
    "  COUNT(*) as total_modifications,\n",
    "  COLLECT_SET(user_email) as editors,\n",
    "  MIN(event_date) as first_modified,\n",
    "  MAX(event_date) as last_modified\n",
    "FROM audit_events\n",
    "GROUP BY notebook_path\n",
    "HAVING COUNT(DISTINCT user_email) > 1\n",
    "ORDER BY unique_editors DESC, total_modifications DESC\n",
    "\"\"\")\n",
    "\n",
    "collab_count = collaboration_df.count()\n",
    "\n",
    "log(f\"\\n\uD83E\uDD1D Collaboration Summary:\")\n",
    "log(f\"  Notebooks with multiple editors: {collab_count}\")\n",
    "\n",
    "if collab_count > 0:\n",
    "    # Highly collaborative notebooks (3+ editors)\n",
    "    highly_collaborative = collaboration_df.filter(F.col('unique_editors') >= 3)\n",
    "    \n",
    "    log(f\"  Highly collaborative (3+ editors): {highly_collaborative.count()}\")\n",
    "    \n",
    "    if not IS_JOB_MODE:\n",
    "        log(\"\\nMost collaborative notebooks:\")\n",
    "        display(collaboration_df.limit(20))\n",
    "    \n",
    "    # Collaboration statistics\n",
    "    collab_stats = collaboration_df.agg(\n",
    "        F.avg('unique_editors').alias('avg_editors'),\n",
    "        F.max('unique_editors').alias('max_editors'),\n",
    "        F.avg('total_modifications').alias('avg_modifications')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCCA Collaboration Statistics:\")\n",
    "    log(f\"  Average editors per collaborative notebook: {collab_stats.avg_editors:.1f}\")\n",
    "    log(f\"  Most editors on single notebook: {collab_stats.max_editors}\")\n",
    "    log(f\"  Average modifications per collaborative notebook: {collab_stats.avg_modifications:.1f}\")\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCA1 INSIGHT: High collaboration indicates active team development\")\n",
    "    log(f\"   Consider code review processes for highly collaborative notebooks\")\n",
    "else:\n",
    "    log(\"\\n  ℹ️  No collaborative notebooks found (all notebooks have single editor)\")\n",
    "\n",
    "# Store for export\n",
    "collaborative_notebooks_df = collaboration_df\n",
    "\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e2f6a87-1c22-4a5a-8b3e-0aa0305a324b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Visualization - Activity heatmaps and trends"
    }
   },
   "outputs": [],
   "source": [
    "# Visualization: Activity heatmaps and trend charts\n",
    "# Visual representation of modification patterns\n",
    "\n",
    "if not IS_JOB_MODE:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"ACTIVITY VISUALIZATIONS\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Get time pattern data\n",
    "    time_data = time_pattern_df.toPandas()\n",
    "    \n",
    "    if len(time_data) > 0:\n",
    "        # Create figure with subplots\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Chart 1: Activity by day of week\n",
    "        day_summary = time_data.groupby('day_name')['modification_count'].sum().reindex(\n",
    "            ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        )\n",
    "        axes[0, 0].bar(day_summary.index, day_summary.values, color='steelblue')\n",
    "        axes[0, 0].set_title('Modifications by Day of Week', fontsize=14, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Day')\n",
    "        axes[0, 0].set_ylabel('Modification Count')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "        axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Chart 2: Activity by hour of day\n",
    "        hour_summary = time_data.groupby('hour_of_day')['modification_count'].sum().sort_index()\n",
    "        axes[0, 1].plot(hour_summary.index, hour_summary.values, marker='o', linewidth=2, color='darkgreen')\n",
    "        axes[0, 1].set_title('Modifications by Hour of Day', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Hour (24-hour format)')\n",
    "        axes[0, 1].set_ylabel('Modification Count')\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "        axes[0, 1].set_xticks(range(0, 24, 2))\n",
    "        \n",
    "        # Chart 3: Heatmap (day vs hour)\n",
    "        pivot_data = time_data.pivot_table(\n",
    "            values='modification_count',\n",
    "            index='day_name',\n",
    "            columns='hour_of_day',\n",
    "            aggfunc='sum',\n",
    "            fill_value=0\n",
    "        ).reindex(['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday'])\n",
    "        \n",
    "        im = axes[1, 0].imshow(pivot_data.values, cmap='YlOrRd', aspect='auto')\n",
    "        axes[1, 0].set_title('Activity Heatmap (Day vs Hour)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Hour of Day')\n",
    "        axes[1, 0].set_ylabel('Day of Week')\n",
    "        axes[1, 0].set_yticks(range(len(pivot_data.index)))\n",
    "        axes[1, 0].set_yticklabels(pivot_data.index)\n",
    "        axes[1, 0].set_xticks(range(0, 24, 2))\n",
    "        axes[1, 0].set_xticklabels(range(0, 24, 2))\n",
    "        plt.colorbar(im, ax=axes[1, 0], label='Modifications')\n",
    "        \n",
    "        # Chart 4: Daily trend\n",
    "        daily_data = daily_trend_df.toPandas()\n",
    "        if len(daily_data) > 0:\n",
    "            axes[1, 1].plot(daily_data['event_date'], daily_data['modification_count'], \n",
    "                          marker='o', linewidth=2, color='purple')\n",
    "            axes[1, 1].set_title('Daily Modification Trend', fontsize=14, fontweight='bold')\n",
    "            axes[1, 1].set_xlabel('Date')\n",
    "            axes[1, 1].set_ylabel('Modification Count')\n",
    "            axes[1, 1].grid(alpha=0.3)\n",
    "            axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        log(\"\\n✓ Activity visualizations generated\")\n",
    "    else:\n",
    "        log(\"\\n⚠️  No time pattern data available for visualization\")\n",
    "else:\n",
    "    log(\"\\n⏭️  Visualizations skipped in job mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bcc11d1-b25b-40f4-a82b-04556dadd964",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enhanced Excel export with summary dashboard"
    }
   },
   "outputs": [],
   "source": [
    "# Enhanced Excel Export with Summary Dashboard\n",
    "# Comprehensive workbook with multiple analysis sheets\n",
    "\n",
    "ENABLE_EXCEL_EXPORT = False  # Set to True to enable Excel export\n",
    "\n",
    "if ENABLE_EXCEL_EXPORT:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"ENHANCED EXCEL EXPORT\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        import tempfile\n",
    "        \n",
    "        # Use temp directory for serverless compatibility\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        temp_excel_file = f\"{temp_dir}/folder_audit_{timestamp}.xlsx\"\n",
    "        \n",
    "        log(f\"\\nCreating Excel workbook with multiple sheets...\")\n",
    "        \n",
    "        with pd.ExcelWriter(temp_excel_file, engine='openpyxl') as writer:\n",
    "            \n",
    "            # Sheet 1: Executive Summary\n",
    "            exec_summary = pd.DataFrame([{\n",
    "                'Metric': 'Audit Period',\n",
    "                'Value': f\"{START_DATE} to {END_DATE}\"\n",
    "            }, {\n",
    "                'Metric': 'Folder Scanned',\n",
    "                'Value': BASE_PATH if BASE_PATH else 'All Workspace Folders'\n",
    "            }, {\n",
    "                'Metric': 'Total Notebooks',\n",
    "                'Value': len(notebooks)\n",
    "            }, {\n",
    "                'Metric': 'Total Modifications',\n",
    "                'Value': spark.sql(\"SELECT COUNT(*) as cnt FROM audit_events\").collect()[0]['cnt']\n",
    "            }, {\n",
    "                'Metric': 'Unique Users',\n",
    "                'Value': spark.sql(\"SELECT COUNT(DISTINCT user_email) as cnt FROM audit_events\").collect()[0]['cnt']\n",
    "            }, {\n",
    "                'Metric': 'Stale Notebooks (90+ days)',\n",
    "                'Value': stale_notebooks_df.count() if 'stale_notebooks_df' in dir() else 'N/A'\n",
    "            }, {\n",
    "                'Metric': 'Execution Failures',\n",
    "                'Value': notebook_failures_df.count() if 'notebook_failures_df' in dir() else 'N/A'\n",
    "            }, {\n",
    "                'Metric': 'After-Hours Modifications',\n",
    "                'Value': after_hours_count\n",
    "            }, {\n",
    "                'Metric': 'Deletions',\n",
    "                'Value': deletions_df.count() if 'deletions_df' in dir() else 'N/A'\n",
    "            }, {\n",
    "                'Metric': 'Audit Timestamp',\n",
    "                'Value': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            }])\n",
    "            \n",
    "            exec_summary.to_excel(writer, sheet_name='Executive Summary', index=False)\n",
    "            log(\"  ✓ Sheet 1: Executive Summary\")\n",
    "            \n",
    "            # Sheet 2: Audit Events Summary\n",
    "            summary_df.toPandas().to_excel(writer, sheet_name='Events Summary', index=False)\n",
    "            log(\"  ✓ Sheet 2: Events Summary\")\n",
    "            \n",
    "            # Sheet 3: User Activity\n",
    "            user_activity_df.toPandas().to_excel(writer, sheet_name='User Activity', index=False)\n",
    "            log(\"  ✓ Sheet 3: User Activity\")\n",
    "            \n",
    "            # Sheet 4: Notebook History\n",
    "            notebook_history_df.toPandas().to_excel(writer, sheet_name='Notebook History', index=False)\n",
    "            log(\"  ✓ Sheet 4: Notebook History\")\n",
    "            \n",
    "            # Sheet 5: Stale Notebooks\n",
    "            if 'stale_notebooks_df' in dir():\n",
    "                stale_notebooks_df.toPandas().to_excel(writer, sheet_name='Stale Notebooks', index=False)\n",
    "                log(\"  ✓ Sheet 5: Stale Notebooks\")\n",
    "            \n",
    "            # Sheet 6: Ownership\n",
    "            if 'notebook_ownership_df' in dir():\n",
    "                notebook_ownership_df.toPandas().to_excel(writer, sheet_name='Ownership', index=False)\n",
    "                log(\"  ✓ Sheet 6: Ownership\")\n",
    "            \n",
    "            # Sheet 7: Execution Failures\n",
    "            if 'notebook_failures_df' in dir():\n",
    "                notebook_failures_df.toPandas().to_excel(writer, sheet_name='Execution Failures', index=False)\n",
    "                log(\"  ✓ Sheet 7: Execution Failures\")\n",
    "            \n",
    "            # Sheet 8: Language Distribution\n",
    "            if 'language_distribution_df' in dir():\n",
    "                language_distribution_df.toPandas().to_excel(writer, sheet_name='Language Distribution', index=False)\n",
    "                log(\"  ✓ Sheet 8: Language Distribution\")\n",
    "            \n",
    "            # Sheet 9: Security Alerts\n",
    "            security_summary = pd.DataFrame([{\n",
    "                'Alert Type': 'After-Hours Modifications',\n",
    "                'Count': after_hours_count,\n",
    "                'Severity': 'Medium' if after_hours_count > 0 else 'None'\n",
    "            }, {\n",
    "                'Alert Type': 'Notebook Deletions',\n",
    "                'Count': deletion_count,\n",
    "                'Severity': 'High' if deletion_count > 0 else 'None'\n",
    "            }, {\n",
    "                'Alert Type': 'External User Activity',\n",
    "                'Count': external_count,\n",
    "                'Severity': 'Medium' if external_count > 0 else 'None'\n",
    "            }])\n",
    "            security_summary.to_excel(writer, sheet_name='Security Alerts', index=False)\n",
    "            log(\"  ✓ Sheet 9: Security Alerts\")\n",
    "        \n",
    "        file_size_kb = os.path.getsize(temp_excel_file) / 1024\n",
    "        log(f\"\\n✓ Enhanced Excel export complete!\")\n",
    "        log(f\"  File: {temp_excel_file}\")\n",
    "        log(f\"  Sheets: 9\")\n",
    "        log(f\"  Size: {file_size_kb:.1f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Excel export failed: {str(e)}\")\n",
    "else:\n",
    "    log(\"\\nℹ️  Excel export disabled (ENABLE_EXCEL_EXPORT=False)\")\n",
    "    log(\"   Set ENABLE_EXCEL_EXPORT=True in this cell to enable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f39f1971-dde3-444a-986b-b898e3f1b53b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON export for API integration"
    }
   },
   "outputs": [],
   "source": [
    "# JSON Export: Structured data for API integration and programmatic access\n",
    "\n",
    "ENABLE_JSON_EXPORT = False  # Set to True to enable JSON export\n",
    "\n",
    "if ENABLE_JSON_EXPORT:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"JSON EXPORT\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        import json\n",
    "        import tempfile\n",
    "        \n",
    "        # Use temp directory for serverless compatibility\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        json_file = f\"{temp_dir}/folder_audit_{timestamp}.json\"\n",
    "        \n",
    "        # Prepare JSON structure\n",
    "        export_data = {\n",
    "            'metadata': {\n",
    "                'audit_timestamp': datetime.now().isoformat(),\n",
    "                'audit_period_start': START_DATE,\n",
    "                'audit_period_end': END_DATE,\n",
    "                'folder_scanned': BASE_PATH if BASE_PATH else 'All Workspace Folders',\n",
    "                'timezone': TIMEZONE,\n",
    "                'execution_mode': 'Job' if IS_JOB_MODE else 'Interactive',\n",
    "                'compute_type': 'Serverless' if IS_SERVERLESS else 'Traditional'\n",
    "            },\n",
    "            'summary': {\n",
    "                'total_notebooks': len(notebooks),\n",
    "                'total_modifications': spark.sql(\"SELECT COUNT(*) as cnt FROM audit_events\").collect()[0]['cnt'],\n",
    "                'unique_users': spark.sql(\"SELECT COUNT(DISTINCT user_email) as cnt FROM audit_events\").collect()[0]['cnt'],\n",
    "                'stale_notebooks': stale_notebooks_df.count() if 'stale_notebooks_df' in dir() else 0,\n",
    "                'execution_failures': notebook_failures_df.count() if 'notebook_failures_df' in dir() else 0,\n",
    "                'after_hours_modifications': after_hours_count if 'after_hours_count' in dir() else 0,\n",
    "                'deletions': deletion_count if 'deletion_count' in dir() else 0,\n",
    "                'collaborative_notebooks': collab_count if 'collab_count' in dir() else 0\n",
    "            },\n",
    "            'notebooks': notebooks,\n",
    "            'audit_events_summary': summary_df.toPandas().to_dict('records'),\n",
    "            'user_activity': user_activity_df.toPandas().to_dict('records'),\n",
    "            'language_distribution': language_distribution_df.toPandas().to_dict('records') if 'language_distribution_df' in dir() else []\n",
    "        }\n",
    "        \n",
    "        # Write JSON file\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2, default=str)\n",
    "        \n",
    "        file_size_kb = os.path.getsize(json_file) / 1024\n",
    "        log(f\"\\n✓ JSON export complete!\")\n",
    "        log(f\"  File: {json_file}\")\n",
    "        log(f\"  Size: {file_size_kb:.1f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ JSON export failed: {str(e)}\")\n",
    "else:\n",
    "    log(\"\\nℹ️  JSON export disabled (ENABLE_JSON_EXPORT=False)\")\n",
    "    log(\"   Set ENABLE_JSON_EXPORT=True in this cell to enable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f96e7de6-67b6-4831-be78-f11bbda0023b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Content snapshot view"
    }
   },
   "outputs": [],
   "source": [
    "# Content Snapshot: Combine current notebook metadata with audit history\n",
    "# This provides a point-in-time view of notebook state with modification history\n",
    "\n",
    "if ENABLE_CONTENT_SNAPSHOT and len(notebooks) > 0:\n",
    "    from pyspark.sql.functions import lit, current_timestamp\n",
    "    \n",
    "    log(\"\\n\uD83D\uDCF8 Creating Content Snapshot\")\n",
    "    log(\"   Combining current notebook metadata with audit history...\\n\")\n",
    "    \n",
    "    # Convert notebooks list to DataFrame\n",
    "    notebooks_df = spark.createDataFrame(notebooks)\n",
    "    \n",
    "    # Create a comprehensive view joining notebook metadata with audit summary\n",
    "    snapshot_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "      notebook_path,\n",
    "      COUNT(*) as total_modifications,\n",
    "      COUNT(DISTINCT user_email) as unique_modifiers,\n",
    "      MIN(event_time) as first_modification,\n",
    "      MAX(event_time) as last_modification,\n",
    "      DATEDIFF(MAX(event_time), MIN(event_time)) as days_active,\n",
    "      COLLECT_SET(action_name) as modification_types,\n",
    "      COLLECT_SET(user_email) as modifier_list\n",
    "    FROM audit_events\n",
    "    GROUP BY notebook_path\n",
    "    \"\"\")\n",
    "    \n",
    "    # Join with current notebook metadata\n",
    "    if 'modified_at' in notebooks_df.columns:\n",
    "        enriched_snapshot_df = notebooks_df.join(\n",
    "            snapshot_df,\n",
    "            notebooks_df.path == snapshot_df.notebook_path,\n",
    "            'left'\n",
    "        ).select(\n",
    "            notebooks_df.path.alias('notebook_path'),\n",
    "            notebooks_df.name.alias('notebook_name'),\n",
    "            notebooks_df.language,\n",
    "            notebooks_df.modified_at.alias('current_modified_at'),\n",
    "            notebooks_df.size.alias('current_size_bytes'),\n",
    "            snapshot_df.total_modifications,\n",
    "            snapshot_df.unique_modifiers,\n",
    "            snapshot_df.first_modification,\n",
    "            snapshot_df.last_modification,\n",
    "            snapshot_df.days_active,\n",
    "            snapshot_df.modification_types,\n",
    "            snapshot_df.modifier_list\n",
    "        ).orderBy(col('total_modifications').desc_nulls_last())\n",
    "    else:\n",
    "        enriched_snapshot_df = notebooks_df.join(\n",
    "            snapshot_df,\n",
    "            notebooks_df.path == snapshot_df.notebook_path,\n",
    "            'left'\n",
    "        ).select(\n",
    "            notebooks_df.path.alias('notebook_path'),\n",
    "            notebooks_df.name.alias('notebook_name'),\n",
    "            notebooks_df.language,\n",
    "            snapshot_df.total_modifications,\n",
    "            snapshot_df.unique_modifiers,\n",
    "            snapshot_df.first_modification,\n",
    "            snapshot_df.last_modification,\n",
    "            snapshot_df.days_active,\n",
    "            snapshot_df.modification_types,\n",
    "            snapshot_df.modifier_list\n",
    "        ).orderBy(col('total_modifications').desc_nulls_last())\n",
    "    \n",
    "    log(\"✅ Content snapshot created successfully\")\n",
    "    log(f\"   Captured metadata for {notebooks_df.count()} notebooks\")\n",
    "    log(f\"   Matched with audit history for {snapshot_df.count()} notebooks\\n\")\n",
    "    \n",
    "    if not IS_JOB_MODE:\n",
    "        display(enriched_snapshot_df)\n",
    "    \n",
    "    # Store for potential export\n",
    "    content_snapshot_df = enriched_snapshot_df\n",
    "else:\n",
    "    log(\"\\n⏭️  Content snapshot disabled or no notebooks found\")\n",
    "    log(\"   Set ENABLE_CONTENT_SNAPSHOT = True to enable this feature\\n\")\n",
    "    content_snapshot_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5e1d3fb-7c4a-4c9e-b886-4eec31f05303",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to Delta table for long-term retention"
    }
   },
   "outputs": [],
   "source": [
    "# Export audit data to Delta table for long-term retention and historical analysis\n",
    "# This enables tracking changes over multiple audit runs\n",
    "\n",
    "if ENABLE_DELTA_EXPORT:\n",
    "    from pyspark.sql.functions import current_timestamp, lit\n",
    "    \n",
    "    log(\"\\n\uD83D\uDCBE Exporting Audit Data to Delta Table\")\n",
    "    log(f\"   Target table: {DELTA_TABLE_NAME}\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Prepare the export DataFrame with metadata\n",
    "        export_df = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "          event_time,\n",
    "          event_date,\n",
    "          notebook_path,\n",
    "          action_name,\n",
    "          user_email,\n",
    "          status_code,\n",
    "          request_id,\n",
    "          notebook_id\n",
    "        FROM audit_events\n",
    "        ORDER BY event_time DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        # Add audit run metadata\n",
    "        export_df = export_df \\\n",
    "            .withColumn('audit_run_timestamp', current_timestamp()) \\\n",
    "            .withColumn('audit_start_date', lit(START_DATE)) \\\n",
    "            .withColumn('audit_end_date', lit(END_DATE)) \\\n",
    "            .withColumn('audit_base_path', lit(BASE_PATH if BASE_PATH else 'ALL_FOLDERS'))\n",
    "        \n",
    "        # Write to Delta table with merge capability\n",
    "        # Using append mode to accumulate historical data\n",
    "        export_df.write \\\n",
    "            .format('delta') \\\n",
    "            .mode('append') \\\n",
    "            .option('mergeSchema', 'true') \\\n",
    "            .saveAsTable(DELTA_TABLE_NAME)\n",
    "        \n",
    "        record_count = export_df.count()\n",
    "        \n",
    "        log(f\"\\n✅ Successfully exported {record_count} audit records\")\n",
    "        log(f\"   Table: {DELTA_TABLE_NAME}\")\n",
    "        log(f\"   Mode: APPEND (historical accumulation)\")\n",
    "        log(f\"   Schema merge: ENABLED\")\n",
    "        \n",
    "        # Also export content snapshot if available\n",
    "        if ENABLE_CONTENT_SNAPSHOT and content_snapshot_df is not None:\n",
    "            snapshot_table_name = DELTA_TABLE_NAME.replace('_history', '_snapshot')\n",
    "            \n",
    "            content_snapshot_df \\\n",
    "                .withColumn('snapshot_timestamp', current_timestamp()) \\\n",
    "                .withColumn('audit_base_path', lit(BASE_PATH if BASE_PATH else 'ALL_FOLDERS')) \\\n",
    "                .write \\\n",
    "                .format('delta') \\\n",
    "                .mode('overwrite') \\\n",
    "                .option('overwriteSchema', 'true') \\\n",
    "                .saveAsTable(snapshot_table_name)\n",
    "            \n",
    "            log(f\"\\n✅ Successfully exported content snapshot\")\n",
    "            log(f\"   Table: {snapshot_table_name}\")\n",
    "            log(f\"   Mode: OVERWRITE (latest snapshot)\")\n",
    "        \n",
    "        log(\"\\n\uD83D\uDCCA Query your audit history:\")\n",
    "        log(f\"   SELECT * FROM {DELTA_TABLE_NAME} WHERE audit_run_timestamp >= current_date()\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"\\n❌ Failed to export to Delta table: {e}\")\n",
    "        log(f\"   Please verify:\")\n",
    "        log(f\"   1. You have CREATE TABLE permissions\")\n",
    "        log(f\"   2. The catalog and schema exist: {DELTA_TABLE_NAME.rsplit('.', 1)[0]}\")\n",
    "        log(f\"   3. The table name is valid\")\n",
    "        raise\n",
    "else:\n",
    "    log(\"\\n⏭️  Delta table export disabled\")\n",
    "    log(\"   Set ENABLE_DELTA_EXPORT = True to enable long-term retention\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe2aa375-9279-41ac-8855-e95bd475ce3f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export complete audit report"
    }
   },
   "outputs": [],
   "source": [
    "# Create a comprehensive audit report combining all data\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Determine folder description\n",
    "if BASE_PATH == \"\" or BASE_PATH is None:\n",
    "    folder_description = \"All Workspace Folders\"\n",
    "else:\n",
    "    folder_description = BASE_PATH\n",
    "\n",
    "# Get current date/time when report was generated in the configured timezone\n",
    "# PERFORMANCE: Use Spark SQL for timezone conversion instead of Python datetime\n",
    "report_time_df = spark.sql(f\"SELECT from_utc_timestamp(current_timestamp(), '{TIMEZONE}') as report_time\")\n",
    "report_generated = report_time_df.collect()[0]['report_time'].strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "log(\"\\n\" + \"=\"*80)\n",
    "log(\"ACCESS REVIEW AUDIT REPORT\")\n",
    "log(\"=\"*80)\n",
    "log(f\"\\nReport Generated: {report_generated}\")\n",
    "log(f\"Folder: {folder_description}\")\n",
    "log(f\"Date Range: {START_DATE} to {END_DATE}\")\n",
    "log(f\"Timezone: {TIMEZONE}\")\n",
    "\n",
    "# Display the detailed event log\n",
    "if MAX_DETAILED_EVENTS == 9999:\n",
    "    log(\"\\nDetailed Modification Events:\")\n",
    "    log(\"(Showing ALL events - no limit applied)\\n\")\n",
    "else:\n",
    "    log(\"\\nDetailed Modification Events:\")\n",
    "    log(f\"(Showing up to {MAX_DETAILED_EVENTS} most recent events)\\n\")\n",
    "\n",
    "# PERFORMANCE: Select only needed columns instead of SELECT *\n",
    "# Build query with conditional LIMIT clause\n",
    "if MAX_DETAILED_EVENTS == 9999:\n",
    "    limit_clause = \"\"  # No limit - retrieve all events\n",
    "else:\n",
    "    limit_clause = f\"LIMIT {MAX_DETAILED_EVENTS}\"\n",
    "\n",
    "final_report_df = spark.sql(f\"\"\"\n",
    "  SELECT \n",
    "    event_time,\n",
    "    notebook_path, \n",
    "    action_name,\n",
    "    user_email,\n",
    "    status_code\n",
    "  FROM audit_events\n",
    "  ORDER BY event_time DESC\n",
    "  {limit_clause}\n",
    "\"\"\")\n",
    "\n",
    "# Display only in interactive mode\n",
    "if not IS_JOB_MODE:\n",
    "    display(final_report_df)\n",
    "else:\n",
    "    # In job mode, you might want to write to a Delta table instead\n",
    "    # Example: final_report_df.write.mode(\"overwrite\").saveAsTable(\"audit_reports.notebook_modifications\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6166eb18-41bd-42a3-a904-be48ec3647c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleanup cached data"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up cached data to free memory\n",
    "# Run this cell when you're done with the audit report\n",
    "\n",
    "try:\n",
    "    spark.sql(\"UNCACHE TABLE IF EXISTS audit_events\")\n",
    "    log(\"✅ Cached data cleared successfully\")\n",
    "except Exception as e:\n",
    "    log(f\"⚠️  Note: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6153c2ac-3e85-4681-a86a-8919a7c16fa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDF89 Audit Complete!\n",
    "\n",
    "### What You Have Now\n",
    "\n",
    "This notebook has generated comprehensive audit reports with:\n",
    "\n",
    "* **\uD83D\uDCC4 Detailed Event Logs**: Every modification event with user, timestamp, and action type\n",
    "* **\uD83D\uDCCA Summary Statistics**: Aggregated views by action type, user, and notebook\n",
    "* **⏰ Time-Based Analysis**: Modification patterns by day of week and hour\n",
    "* **\uD83D\uDCF8 Content Snapshots**: Point-in-time view of notebook state with history\n",
    "* **\uD83D\uDCBE Long-Term Retention**: Optional Delta table export for historical analysis\n",
    "\n",
    "---\n",
    "\n",
    "### Querying Exported Data\n",
    "\n",
    "If you enabled `ENABLE_DELTA_EXPORT = True`, you can query your audit history:\n",
    "\n",
    "```sql\n",
    "-- View all audit events from today's run\n",
    "SELECT * FROM main.default.notebook_audit_history \n",
    "WHERE audit_run_timestamp >= current_date()\n",
    "ORDER BY event_time DESC;\n",
    "\n",
    "-- Find all modifications to a specific notebook\n",
    "SELECT * FROM main.default.notebook_audit_history \n",
    "WHERE notebook_path LIKE '%YourNotebookName%'\n",
    "ORDER BY event_time DESC;\n",
    "\n",
    "-- Analyze modification trends over time\n",
    "SELECT \n",
    "  DATE(audit_run_timestamp) as audit_date,\n",
    "  COUNT(*) as total_events,\n",
    "  COUNT(DISTINCT notebook_path) as notebooks_modified,\n",
    "  COUNT(DISTINCT user_email) as active_users\n",
    "FROM main.default.notebook_audit_history\n",
    "GROUP BY DATE(audit_run_timestamp)\n",
    "ORDER BY audit_date DESC;\n",
    "\n",
    "-- View current snapshot\n",
    "SELECT * FROM main.default.notebook_audit_snapshot\n",
    "ORDER BY total_modifications DESC;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Scheduling This Notebook\n",
    "\n",
    "To run this audit automatically:\n",
    "\n",
    "1. **Create a Job**: Go to Workflows → Create Job\n",
    "2. **Add this notebook** as a task\n",
    "3. **Set schedule**: Daily, weekly, or monthly\n",
    "4. **Enable Delta export**: Set `ENABLE_DELTA_EXPORT = True`\n",
    "5. **Configure alerts**: Add email notifications for job completion\n",
    "\n",
    "The notebook will automatically run in job mode (no display output) and export results to Delta tables.\n",
    "\n",
    "---\n",
    "\n",
    "### Customization Tips\n",
    "\n",
    "* **Narrow the scope**: Set `BASE_PATH` to specific folders for faster execution\n",
    "* **Adjust date range**: Use shorter periods for recent activity analysis\n",
    "* **Increase detail**: Set `MAX_DETAILED_EVENTS = 9999` for complete history\n",
    "* **Add filters**: Modify SQL queries to focus on specific users or action types\n",
    "* **Create dashboards**: Use the Delta tables as data sources for Lakeview dashboards\n",
    "\n",
    "---\n",
    "\n",
    "### Compliance & Security\n",
    "\n",
    "This audit framework helps with:\n",
    "\n",
    "* ✅ **SOX Compliance**: Track all changes to critical notebooks\n",
    "* ✅ **Access Reviews**: Identify who modified what and when\n",
    "* ✅ **Security Monitoring**: Detect unusual modification patterns\n",
    "* ✅ **Change Management**: Document notebook lifecycle\n",
    "* ✅ **Team Analytics**: Understand collaboration patterns\n",
    "\n",
    "---\n",
    "\n",
    "### Need Help?\n",
    "\n",
    "Refer to the configuration section in Cell 3 to:\n",
    "* Change audit scope and date range\n",
    "* Enable/disable features\n",
    "* Configure Delta table export\n",
    "* Adjust performance settings"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Folder Scan Notebook",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14c521f2-2513-4c17-ab33-9969c4699b71",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Documentation Header"
    }
   },
   "source": [
    "# Workspace Asset Scanner - Default Naming Convention Detector\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **comprehensive workspace audit** by scanning all notebooks, SQL queries, dashboards, Genie spaces, jobs, SQL alerts, DLT pipelines, and MLflow experiments to identify assets using default Databricks naming conventions (e.g., \"Untitled Notebook\", \"Untitled Query\", \"New Dashboard\", \"Untitled Job\", \"Default\" experiment). The scanner supports both interactive and job execution modes with enterprise-grade optimization features.\n",
    "\n",
    "**✨ Enterprise-grade workspace auditing with complete coverage across 8 asset types, owner tracking, multiple export formats, and performance optimization.**\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "\n",
    "### Core Functionality\n",
    "* Scans **8 asset types** across entire workspace:\n",
    "  * Notebooks\n",
    "  * SQL Queries\n",
    "  * Dashboards\n",
    "  * Genie Spaces\n",
    "  * Jobs/Workflows\n",
    "  * SQL Alerts\n",
    "  * DLT Pipelines (Lakeflow Spark Declarative Pipelines)\n",
    "  * MLflow Experiments\n",
    "* Identifies assets with default naming patterns\n",
    "* Extracts owner/user information for all assets\n",
    "* Flexible execution mode (interactive vs job)\n",
    "* Path exclusion filters (e.g., skip /Repos)\n",
    "* Age-based filtering (optional)\n",
    "\n",
    "### Performance & Reliability\n",
    "* **Retry Logic**: Automatic retry with exponential backoff for transient API failures\n",
    "* **Progress Tracking**: Real-time progress updates during long-running operations\n",
    "* **Execution Time Tracking**: Per-cell execution time monitoring\n",
    "* **Configuration Validation**: Validates all configuration parameters before execution\n",
    "* **Error Handling**: Comprehensive error handling with detailed logging\n",
    "* **Execution Statistics**: Tracks API calls, failures, retries, success rates\n",
    "\n",
    "### Data Quality\n",
    "* **Owner Information**: Captures owner/user for all assets\n",
    "  * Notebooks in /Users/ paths: Extracted from path\n",
    "  * Notebooks in shared folders: Fetched via workspace API\n",
    "  * SQL queries: Extracted from query metadata\n",
    "  * Dashboards: Extracted from dashboard metadata\n",
    "  * Genie spaces: Extracted from space metadata\n",
    "  * Jobs: Extracted from creator_user_name\n",
    "  * SQL Alerts: Extracted from owner or user metadata\n",
    "  * DLT Pipelines: Extracted from creator_user_name\n",
    "  * MLflow Experiments: Extracted from tags or artifact location\n",
    "* **Data Validation**: Validates DataFrame integrity before export\n",
    "* **Quality Metrics**: Reports on data completeness and coverage\n",
    "* **Deduplication**: Automatic duplicate detection and removal\n",
    "\n",
    "### Export Formats\n",
    "* **Delta Table**: Historical accumulation with schema evolution (append mode)\n",
    "* **Excel Workbook**: Multi-sheet workbook with:\n",
    "  * All Assets sheet\n",
    "  * Summary sheet with metrics\n",
    "  * By Type breakdown\n",
    "  * By Owner analysis\n",
    "  * Cleanup Recommendations (stale assets)\n",
    "  * Execution Statistics\n",
    "* **HTML Report**: Styled web report with summary, analysis, and recommendations\n",
    "* **Timezone Support**: All timestamps converted to Eastern Time\n",
    "\n",
    "### Analysis & Reporting\n",
    "* **Summary Reports**: Comprehensive statistics and breakdowns\n",
    "* **Owner Analysis**: Top users with most default-named assets\n",
    "* **Recent Activity**: Assets modified in last 90 days\n",
    "* **Age Distribution**: Histogram of asset ages by bucket\n",
    "* **Cleanup Recommendations**: Identifies stale assets for deletion\n",
    "* **Data Quality Metrics**: Coverage and completeness reporting\n",
    "* **Execution Summary**: API performance and success rates\n",
    "\n",
    "---\n",
    "\n",
    "## Version Control\n",
    "\n",
    "| Version | Date | Author | Changes |\n",
    "|---------|------|--------|---------|  \n",
    "| 1.0.0 | 2026-02-16 | Assistant | Comprehensive workspace asset scanner with complete coverage across 8 asset types (Notebooks, SQL Queries, Dashboards, Genie Spaces, Jobs/Workflows, SQL Alerts, DLT Pipelines, MLflow Experiments). Features include: owner tracking for all asset types, performance presets (Quick/Full mode), widget parameters (output_catalog, output_schema, exclude_repos, min_age_days, stale_threshold_days), concurrent API calls with ThreadPoolExecutor, retry logic with exponential backoff, memory usage monitoring, data deduplication, incremental scan mode, multiple export formats (Delta table with append mode, Excel multi-sheet workbook, HTML report), execution statistics tracking, asset age distribution analysis, cleanup recommendations for stale assets, email notifications for job mode, configuration validation, and comprehensive error handling. |\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Widget Parameters (Dynamic Configuration):\n",
    "* `execution_mode` - Interactive or Job mode (dropdown)\n",
    "* `output_catalog` - Target catalog for Delta table (default: main)\n",
    "* `output_schema` - Target schema for Delta table (default: default)\n",
    "* `exclude_repos` - Exclude /Repos folder from scan (Yes/No dropdown)\n",
    "* `min_age_days` - Filter assets older than X days (leave empty for all)\n",
    "* `stale_threshold_days` - Days threshold for cleanup recommendations (default: 180)\n",
    "\n",
    "### Performance Settings (Cell 2):\n",
    "* `MAX_RETRIES = 3` - Retries for failed API calls\n",
    "* `RETRY_DELAY = 2` - Seconds between retries (with exponential backoff)\n",
    "* `MAX_WORKERS = 10` - Parallel threads for API calls\n",
    "* `MAX_NOTEBOOKS_FOR_OWNER_LOOKUP = 100` - Limit for detailed owner lookup in shared folders\n",
    "\n",
    "### Performance Presets (Cell 3):\n",
    "* **Quick Mode**: Limits to 1,000 notebooks, skips shared folder owner lookup (∼5-10 min)\n",
    "* **Full Mode**: No limits, complete owner info (∼15-30 min)\n",
    "* **Custom Mode**: Use manual configuration from Cell 2\n",
    "* **Auto-enables Full Mode** when running as scheduled job\n",
    "\n",
    "### Export Format Settings:\n",
    "* `ENABLE_EXCEL_EXPORT = True` - Excel workbook generation (multi-sheet)\n",
    "* `ENABLE_HTML_EXPORT = True` - HTML report generation\n",
    "* `ENABLE_DELTA_EXPORT = True` - Delta table for long-term retention\n",
    "* `ENABLE_EMAIL_NOTIFICATIONS = False` - Email alerts for job mode\n",
    "* `ENABLE_INCREMENTAL_SCAN = False` - Only scan changed assets\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Interactive Mode\n",
    "1. Configure widget parameters at the top of the notebook\n",
    "2. Optionally uncomment a performance preset in Cell 3 (Quick/Full Mode)\n",
    "3. Run all cells to scan your workspace\n",
    "4. View detailed analysis and visualizations\n",
    "5. Download exported files from `/dbfs/tmp/workspace_scan_export/`\n",
    "\n",
    "### Job Mode\n",
    "1. Schedule as a Databricks job\n",
    "2. Set widget parameters in job configuration\n",
    "3. Automatically runs in 'job' mode with Full Mode enabled\n",
    "4. Exports results to configured locations\n",
    "5. Returns JSON summary for orchestration\n",
    "6. Optionally sends email notifications\n",
    "\n",
    "---\n",
    "\n",
    "## Asset Types Scanned\n",
    "\n",
    "| Asset Type | API Endpoint | Default Patterns Detected |\n",
    "|------------|--------------|---------------------------|\n",
    "| Notebooks | /api/2.0/workspace/list | \"Untitled Notebook\", \"Untitled\", \"New Notebook\" |\n",
    "| SQL Queries | /api/2.0/preview/sql/queries | \"Untitled Query\", \"New Query\", \"Untitled\" |\n",
    "| Dashboards | /api/2.0/preview/sql/dashboards | \"New Dashboard\", \"Untitled Dashboard\", \"Untitled\" |\n",
    "| Genie Spaces | /api/2.0/genie/spaces | \"New Genie Space\", \"Untitled Space\", \"Untitled\", \"New Space\" |\n",
    "| Jobs | /api/2.1/jobs/list | \"Untitled Job\", \"New Job\", \"Untitled\" |\n",
    "| SQL Alerts | /api/2.0/alerts (+ legacy) | \"New Alert\", \"Untitled Alert\", \"Untitled\" |\n",
    "| DLT Pipelines | /api/2.0/pipelines | \"Untitled Pipeline\", \"New Pipeline\", \"Untitled\" |\n",
    "| MLflow Experiments | /api/2.0/mlflow/experiments/search | \"Default\", \"Untitled Experiment\", \"Untitled\" |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "✓ **8 Asset Types**: Complete coverage across notebooks, queries, dashboards, Genie spaces, jobs, alerts, pipelines, experiments  \n",
    "✓ **Automatic Job Mode Detection**: Detects scheduled job execution  \n",
    "✓ **Performance Presets**: Quick Mode (fast) vs Full Mode (comprehensive)  \n",
    "✓ **Widget Parameters**: Dynamic configuration without code edits  \n",
    "✓ **Configuration Validation**: Validates all parameters before execution  \n",
    "✓ **Concurrent API Calls**: Parallel processing with ThreadPoolExecutor  \n",
    "✓ **Execution Statistics**: Tracks API calls, failures, retries, success rates  \n",
    "✓ **Memory Monitoring**: Tracks memory usage and warns on high consumption  \n",
    "✓ **Progress Tracking**: Real-time progress updates during scans  \n",
    "✓ **Retry Logic**: Automatic retry with exponential backoff  \n",
    "✓ **Data Deduplication**: Automatic duplicate detection and removal  \n",
    "✓ **Multiple Export Formats**: Excel (multi-sheet), HTML report, Delta table  \n",
    "✓ **Timezone Handling**: All timestamps in Eastern Time  \n",
    "✓ **Data Quality Checks**: Validates data integrity before export  \n",
    "✓ **Owner Tracking**: Captures owner/user for all asset types  \n",
    "✓ **Incremental Scan**: Only process changed assets for faster daily runs  \n",
    "✓ **Age Distribution**: Histogram analysis of asset ages  \n",
    "✓ **Cleanup Recommendations**: Identifies stale assets for deletion  \n",
    "✓ **Historical Retention**: Delta table with append mode for trend analysis  \n",
    "✓ **Email Notifications**: Automated alerts in job mode  \n",
    "✓ **Error Handling**: Comprehensive error handling with detailed logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "385e0286-cd0f-4dce-8d12-8ea00e915f2a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup and Configuration"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import pytz\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Detect if running in job mode or interactive mode\n",
    "try:\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().currentRunId().isDefined()\n",
    "    is_job_mode = True\n",
    "except:\n",
    "    is_job_mode = False\n",
    "\n",
    "# Get parameters from widgets\n",
    "if not is_job_mode:\n",
    "    execution_mode = dbutils.widgets.get(\"execution_mode\")\n",
    "    output_catalog = dbutils.widgets.get(\"output_catalog\")\n",
    "    output_schema = dbutils.widgets.get(\"output_schema\")\n",
    "    exclude_repos_param = dbutils.widgets.get(\"exclude_repos\")\n",
    "    min_age_days_param = dbutils.widgets.get(\"min_age_days\")\n",
    "    stale_threshold_days_param = dbutils.widgets.get(\"stale_threshold_days\")\n",
    "else:\n",
    "    execution_mode = 'job'\n",
    "    output_catalog = 'main'\n",
    "    output_schema = 'default'\n",
    "    exclude_repos_param = 'Yes'\n",
    "    min_age_days_param = ''\n",
    "    stale_threshold_days_param = '180'\n",
    "\n",
    "# ============================================================================\n",
    "# PERFORMANCE CONFIGURATION\n",
    "# ============================================================================\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2\n",
    "MAX_WORKERS = 10  # Parallel API calls for owner lookup\n",
    "MAX_NOTEBOOKS_FOR_OWNER_LOOKUP = 100  # Limit notebooks for detailed owner lookup in shared folders\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT PATH CONFIGURATION\n",
    "# ============================================================================\n",
    "EXPORT_PATH = '/dbfs/tmp/workspace_scan_export'\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# EXCEL EXPORT: Export scan results to Excel files\n",
    "# ============================================================================\n",
    "ENABLE_EXCEL_EXPORT = True\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# HTML EXPORT: Export scan results to HTML report\n",
    "# ============================================================================\n",
    "ENABLE_HTML_EXPORT = True\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# LONG-TERM RETENTION: Export scan results to Delta table\n",
    "# ============================================================================\n",
    "ENABLE_DELTA_EXPORT = True\n",
    "DELTA_TABLE_NAME = f'{output_catalog}.{output_schema}.workspace_default_named_assets'\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# EMAIL NOTIFICATIONS: Send email when job completes\n",
    "# ============================================================================\n",
    "ENABLE_EMAIL_NOTIFICATIONS = False\n",
    "EMAIL_RECIPIENTS = []  # List of email addresses\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# INCREMENTAL SCAN: Only scan assets modified since last run\n",
    "# ============================================================================\n",
    "ENABLE_INCREMENTAL_SCAN = False\n",
    "# ============================================================================\n",
    "\n",
    "# Default naming patterns to detect\n",
    "DEFAULT_PATTERNS = {\n",
    "    'notebooks': ['Untitled Notebook', 'Untitled', 'New Notebook'],\n",
    "    'queries': ['Untitled Query', 'New Query', 'Untitled'],\n",
    "    'dashboards': ['New Dashboard', 'Untitled Dashboard', 'Untitled'],\n",
    "    'genie_spaces': ['New Genie Space', 'Untitled Space', 'Untitled', 'New Space'],\n",
    "    'jobs': ['Untitled Job', 'New Job', 'Untitled'],\n",
    "    'alerts': ['New Alert', 'Untitled Alert', 'Untitled'],\n",
    "    'pipelines': ['Untitled Pipeline', 'New Pipeline', 'Untitled'],\n",
    "    'experiments': ['Default', 'Untitled Experiment', 'Untitled']\n",
    "}\n",
    "\n",
    "# Filtering options (from widgets)\n",
    "EXCLUDE_PATHS = ['/Repos'] if exclude_repos_param == 'Yes' else []\n",
    "MIN_AGE_DAYS = int(min_age_days_param) if min_age_days_param and min_age_days_param.strip() else None\n",
    "STALE_THRESHOLD_DAYS = int(stale_threshold_days_param) if stale_threshold_days_param and stale_threshold_days_param.strip() else 180\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION: Validate configuration\n",
    "# ============================================================================\n",
    "def validate_config():\n",
    "    \"\"\"Validate configuration parameters\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    if not isinstance(MAX_RETRIES, int) or MAX_RETRIES < 1:\n",
    "        errors.append(\"MAX_RETRIES must be a positive integer\")\n",
    "    \n",
    "    if not isinstance(RETRY_DELAY, (int, float)) or RETRY_DELAY < 0:\n",
    "        errors.append(\"RETRY_DELAY must be a non-negative number\")\n",
    "    \n",
    "    if ENABLE_DELTA_EXPORT:\n",
    "        if not re.match(r'^[a-zA-Z0-9_]+\\.[a-zA-Z0-9_]+\\.[a-zA-Z0-9_]+$', DELTA_TABLE_NAME):\n",
    "            errors.append(f\"DELTA_TABLE_NAME must be in format 'catalog.schema.table', got: {DELTA_TABLE_NAME}\")\n",
    "    \n",
    "    if MIN_AGE_DAYS is not None and MIN_AGE_DAYS < 0:\n",
    "        errors.append(\"MIN_AGE_DAYS must be a positive integer or None\")\n",
    "    \n",
    "    if STALE_THRESHOLD_DAYS < 1:\n",
    "        errors.append(\"STALE_THRESHOLD_DAYS must be a positive integer\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "config_errors = validate_config()\n",
    "if config_errors:\n",
    "    error_msg = \"Configuration validation failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in config_errors)\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITIES: Helper functions\n",
    "# ============================================================================\n",
    "\n",
    "# Execution statistics tracking\n",
    "execution_stats = {\n",
    "    'start_time': time.time(),\n",
    "    'api_calls': 0,\n",
    "    'api_failures': 0,\n",
    "    'api_retries': 0,\n",
    "    'resources_processed': 0,\n",
    "    'resources_skipped': 0,\n",
    "    'memory_usage_mb': 0\n",
    "}\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Print messages (always in interactive, selectively in job mode)\"\"\"\n",
    "    if not is_job_mode:\n",
    "        print(message)\n",
    "    else:\n",
    "        print(message)  # Also print in job mode for logs\n",
    "\n",
    "def log_execution_time(cell_name, start_time):\n",
    "    \"\"\"Log execution time for a cell\"\"\"\n",
    "    elapsed = time.time() - start_time\n",
    "    log(f\"⏱️  {cell_name} completed in {elapsed:.2f} seconds\")\n",
    "\n",
    "def validate_dataframe_exists(df_name, df):\n",
    "    \"\"\"Validate that a DataFrame exists and has data\"\"\"\n",
    "    if df is None:\n",
    "        log(f\"⚠️  Warning: {df_name} is None\")\n",
    "        return False\n",
    "    try:\n",
    "        count = df.count()\n",
    "        if count == 0:\n",
    "            log(f\"⚠️  Warning: {df_name} is empty (0 rows)\")\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log(f\"⚠️  Warning: Error checking {df_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024 / 1024\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def print_execution_summary():\n",
    "    \"\"\"Print execution statistics summary\"\"\"\n",
    "    elapsed = time.time() - execution_stats['start_time']\n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(\"EXECUTION SUMMARY\")\n",
    "    log(f\"{'='*60}\")\n",
    "    log(f\"Total execution time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "    log(f\"API calls made: {execution_stats['api_calls']}\")\n",
    "    log(f\"Resources processed: {execution_stats['resources_processed']}\")\n",
    "    log(f\"Resources skipped: {execution_stats['resources_skipped']}\")\n",
    "    log(f\"API failures: {execution_stats['api_failures']}\")\n",
    "    log(f\"API retries: {execution_stats['api_retries']}\")\n",
    "    if execution_stats['api_calls'] > 0:\n",
    "        success_rate = ((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls']) * 100\n",
    "        log(f\"Success rate: {success_rate:.1f}%\")\n",
    "    if execution_stats['memory_usage_mb'] > 0:\n",
    "        log(f\"Peak memory usage: {execution_stats['memory_usage_mb']:.2f} MB\")\n",
    "    log(f\"{'='*60}\")\n",
    "\n",
    "log(\"✓ Setup complete\")\n",
    "log(f\"Configuration: MAX_RETRIES={MAX_RETRIES}, RETRY_DELAY={RETRY_DELAY}s, MAX_WORKERS={MAX_WORKERS}\")\n",
    "log(f\"Execution Mode: {execution_mode}\")\n",
    "log(f\"Output Location: {output_catalog}.{output_schema}\")\n",
    "log(f\"Export Path: {EXPORT_PATH}\")\n",
    "log(f\"Excluded Paths: {EXCLUDE_PATHS}\")\n",
    "if MIN_AGE_DAYS:\n",
    "    log(f\"Age Filter: Assets older than {MIN_AGE_DAYS} days\")\n",
    "log(f\"Stale Asset Threshold: {STALE_THRESHOLD_DAYS} days\")\n",
    "log(\"\")\n",
    "log(\"Export formats enabled:\")\n",
    "if ENABLE_EXCEL_EXPORT:\n",
    "    log(\"\uD83D\uDCCA Excel export enabled\")\n",
    "if ENABLE_HTML_EXPORT:\n",
    "    log(\"\uD83C\uDF10 HTML export enabled\")\n",
    "if ENABLE_DELTA_EXPORT:\n",
    "    log(f\"\uD83D\uDCBE Delta export enabled: {DELTA_TABLE_NAME}\")\n",
    "if ENABLE_EMAIL_NOTIFICATIONS:\n",
    "    log(f\"\uD83D\uDCE7 Email notifications enabled: {len(EMAIL_RECIPIENTS)} recipients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c17d3c6-0ef4-4319-942d-19aa064b4587",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Performance Presets (Quick Mode vs Full Mode)"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PERFORMANCE PRESETS: Choose your execution mode\n",
    "# ============================================================================\n",
    "# Uncomment ONE of the following presets, or use custom configuration from Cell 2\n",
    "\n",
    "# PRESET 1: QUICK MODE (5-10 minutes) - Fast scanning with limits\n",
    "# Recommended for: Daily monitoring, quick audits, testing, interactive development\n",
    "# USE_QUICK_MODE = True\n",
    "\n",
    "# PRESET 2: FULL MODE (15-30 minutes) - Complete scanning without limits\n",
    "# Recommended for: Compliance audits, weekly reviews, comprehensive analysis, scheduled jobs\n",
    "# USE_FULL_MODE = True\n",
    "\n",
    "# PRESET 3: CUSTOM MODE - Use configuration from Cell 2\n",
    "# Recommended for: Specific use cases, targeted audits\n",
    "# (Default if no preset is uncommented)\n",
    "\n",
    "# ============================================================================\n",
    "# Apply preset configurations\n",
    "# ============================================================================\n",
    "\n",
    "if 'USE_QUICK_MODE' in dir() and USE_QUICK_MODE:\n",
    "    log(\"\\n\uD83D\uDE80 QUICK MODE ENABLED\")\n",
    "    log(\"=\"*60)\n",
    "    MAX_NOTEBOOKS_FOR_OWNER_LOOKUP = 0  # Skip shared folder owner lookup\n",
    "    MAX_WORKSPACE_SCAN_LIMIT = 1000  # Limit notebooks scanned\n",
    "    ENABLE_SHARED_FOLDER_OWNER_LOOKUP = False\n",
    "    log(\"  Notebook scan limit: 1,000\")\n",
    "    log(\"  Shared folder owner lookup: DISABLED\")\n",
    "    log(\"  Estimated time: 5-10 minutes\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "elif 'USE_FULL_MODE' in dir() and USE_FULL_MODE:\n",
    "    log(\"\\n\uD83D\uDD0D FULL MODE ENABLED\")\n",
    "    log(\"=\"*60)\n",
    "    MAX_NOTEBOOKS_FOR_OWNER_LOOKUP = 999  # No limit\n",
    "    MAX_WORKSPACE_SCAN_LIMIT = 999999  # No limit\n",
    "    ENABLE_SHARED_FOLDER_OWNER_LOOKUP = True\n",
    "    log(\"  Notebook scan limit: NONE (complete scan)\")\n",
    "    log(\"  Shared folder owner lookup: ENABLED\")\n",
    "    log(\"  Estimated time: 15-30 minutes\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    log(\"\\n⚙️ CUSTOM MODE - Using configuration from Cell 2\")\n",
    "    log(\"=\"*60)\n",
    "    # Use values from Cell 2\n",
    "    if 'MAX_WORKSPACE_SCAN_LIMIT' not in dir():\n",
    "        MAX_WORKSPACE_SCAN_LIMIT = 999999  # Default: no limit\n",
    "    if 'ENABLE_SHARED_FOLDER_OWNER_LOOKUP' not in dir():\n",
    "        ENABLE_SHARED_FOLDER_OWNER_LOOKUP = True  # Default: enabled\n",
    "\n",
    "# ============================================================================\n",
    "# JOB MODE OVERRIDE: Always use Full Mode for scheduled jobs\n",
    "# ============================================================================\n",
    "if is_job_mode:\n",
    "    log(\"\\n\uD83E\uDD16 JOB MODE DETECTED - Forcing Full Mode\")\n",
    "    log(\"=\"*60)\n",
    "    MAX_NOTEBOOKS_FOR_OWNER_LOOKUP = 999\n",
    "    MAX_WORKSPACE_SCAN_LIMIT = 999999\n",
    "    ENABLE_SHARED_FOLDER_OWNER_LOOKUP = True\n",
    "    log(\"  All limits removed for comprehensive audit\")\n",
    "    log(\"  Shared folder owner lookup: ENABLED\")\n",
    "    log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1456ea29-40f7-4078-88f1-0a465cc39e39",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare Export Directory"
    }
   },
   "outputs": [],
   "source": [
    "# Create export directory if it doesn't exist (using dbutils.fs for serverless compatibility)\n",
    "try:\n",
    "    dbutils.fs.ls(EXPORT_PATH.replace('/dbfs', 'dbfs:'))\n",
    "    log(f\"✓ Export directory ready: {EXPORT_PATH}\")\n",
    "except:\n",
    "    dbutils.fs.mkdirs(EXPORT_PATH.replace('/dbfs', 'dbfs:'))\n",
    "    log(f\"✓ Created export directory: {EXPORT_PATH}\")\n",
    "\n",
    "# Test write permissions\n",
    "try:\n",
    "    test_file = f\"{EXPORT_PATH}/.test\".replace('/dbfs', 'dbfs:')\n",
    "    dbutils.fs.put(test_file, 'test', overwrite=True)\n",
    "    dbutils.fs.rm(test_file)\n",
    "    log(\"  ✓ Export path is writable\")\n",
    "except Exception as e:\n",
    "    log(f\"  ⚠️ Warning: Export path may not be writable: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77dd9b9c-0f7a-4967-b1bc-bbcd7bc0c4be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check for Incremental Scan"
    }
   },
   "outputs": [],
   "source": [
    "# Check if incremental scan is enabled and get last scan timestamp\n",
    "last_scan_timestamp = None\n",
    "\n",
    "if ENABLE_INCREMENTAL_SCAN and ENABLE_DELTA_EXPORT:\n",
    "    log(\"\\nChecking for incremental scan...\")\n",
    "    try:\n",
    "        # Query Delta table for last scan timestamp\n",
    "        last_scan_df = spark.sql(f\"\"\"\n",
    "            SELECT MAX(scan_timestamp) as last_scan\n",
    "            FROM {DELTA_TABLE_NAME}\n",
    "        \"\"\")\n",
    "        \n",
    "        last_scan_row = last_scan_df.collect()\n",
    "        if last_scan_row and last_scan_row[0]['last_scan']:\n",
    "            last_scan_timestamp = last_scan_row[0]['last_scan']\n",
    "            log(f\"  ✓ Last scan found: {last_scan_timestamp}\")\n",
    "            log(f\"  ✓ Incremental mode: Only scanning assets modified after {last_scan_timestamp}\")\n",
    "        else:\n",
    "            log(\"  ℹ️ No previous scan found, performing full scan\")\n",
    "    except Exception as e:\n",
    "        log(f\"  ℹ️ Could not query previous scan (table may not exist): {str(e)[:100]}\")\n",
    "        log(\"  ✓ Performing full scan\")\n",
    "else:\n",
    "    if not ENABLE_INCREMENTAL_SCAN:\n",
    "        log(\"\\n⏭️ Incremental scan disabled, performing full scan\")\n",
    "    else:\n",
    "        log(\"\\n⏭️ Incremental scan requires Delta export, performing full scan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8edfdf-549c-4610-b576-3b87ff8674bc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "API Helper Functions"
    }
   },
   "outputs": [],
   "source": [
    "def get_api_client():\n",
    "    \"\"\"Get Databricks API client configuration\"\"\"\n",
    "    try:\n",
    "        ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "        api_url = ctx.apiUrl().get()\n",
    "        api_token = ctx.apiToken().get()\n",
    "        return api_url, api_token\n",
    "    except Exception as e:\n",
    "        log(f\"Error getting API client: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def api_call_with_retry(func, *args, **kwargs):\n",
    "    \"\"\"Execute API call with retry logic and stats tracking\"\"\"\n",
    "    execution_stats['api_calls'] += 1\n",
    "    \n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            execution_stats['resources_processed'] += 1\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            execution_stats['api_failures'] += 1\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                execution_stats['api_retries'] += 1\n",
    "                log(f\"  ⚠️ API call failed (attempt {attempt + 1}/{MAX_RETRIES}): {e}\")\n",
    "                time.sleep(RETRY_DELAY * (attempt + 1))  # Exponential backoff\n",
    "            else:\n",
    "                log(f\"  ✗ API call failed after {MAX_RETRIES} attempts: {e}\")\n",
    "                raise\n",
    "\n",
    "def should_exclude_path(path: str) -> bool:\n",
    "    \"\"\"Check if path should be excluded from scanning\"\"\"\n",
    "    return any(path.startswith(excluded) for excluded in EXCLUDE_PATHS)\n",
    "\n",
    "log(\"✓ API helper functions loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d496ff8a-58ff-4e8c-ae2c-41abd3868418",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scan Notebooks"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Fetching notebook permissions...\")\n",
    "\n",
    "api_url, api_token = get_api_client()\n",
    "if not api_url or not api_token:\n",
    "    log(\"  ✗ Failed to get API client\")\n",
    "    notebooks = []\n",
    "    user_id_to_email = {}\n",
    "else:\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    \n",
    "    # First, build a mapping of user IDs to emails\n",
    "    log(\"  Building user ID to email mapping...\")\n",
    "    user_id_to_email = {}\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            f\"{api_url}/api/2.0/preview/scim/v2/Users\",\n",
    "            headers=headers,\n",
    "            params={\"count\": 10000}\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            users_data = response.json()\n",
    "            for user in users_data.get(\"Resources\", []):\n",
    "                user_id = user.get(\"id\")\n",
    "                emails = user.get(\"emails\", [])\n",
    "                if user_id and emails:\n",
    "                    user_id_to_email[user_id] = emails[0].get(\"value\")\n",
    "            log(f\"  ✓ Mapped {len(user_id_to_email)} users\")\n",
    "    except Exception as e:\n",
    "        log(f\"  ⚠️ Warning: Could not fetch user mapping: {e}\")\n",
    "    \n",
    "    # Now scan notebooks\n",
    "    all_notebooks = []\n",
    "    directories_scanned = 0\n",
    "    \n",
    "    def recurse_path(current_path):\n",
    "        global directories_scanned, all_notebooks\n",
    "        \n",
    "        if should_exclude_path(current_path):\n",
    "            return\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(\n",
    "                f\"{api_url}/api/2.0/workspace/list\",\n",
    "                headers=headers,\n",
    "                json={\"path\": current_path}\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if \"objects\" in data:\n",
    "                    for obj in data[\"objects\"]:\n",
    "                        if obj.get(\"object_type\") == \"NOTEBOOK\":\n",
    "                            all_notebooks.append(obj)\n",
    "                        \n",
    "                        if obj.get(\"object_type\") == \"DIRECTORY\":\n",
    "                            directories_scanned += 1\n",
    "                            if directories_scanned % 100 == 0:\n",
    "                                log(f\"  Progress: Scanned {directories_scanned} directories, found {len(all_notebooks)} notebooks...\")\n",
    "                            recurse_path(obj[\"path\"])\n",
    "            elif response.status_code == 403:\n",
    "                pass\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    recurse_path(\"/\")\n",
    "    notebooks = all_notebooks\n",
    "\n",
    "# Calculate age threshold if filtering is enabled\n",
    "age_threshold_ms = None\n",
    "if MIN_AGE_DAYS:\n",
    "    age_threshold_ms = int((datetime.now() - timedelta(days=MIN_AGE_DAYS)).timestamp() * 1000)\n",
    "    log(f\"  Age filter: Assets older than {MIN_AGE_DAYS} days\")\n",
    "\n",
    "# Filter notebooks with default naming\n",
    "import re\n",
    "notebook_data = []\n",
    "shared_folder_notebooks = []\n",
    "\n",
    "for nb in notebooks:\n",
    "    name = nb.get(\"path\", \"\").split(\"/\")[-1]\n",
    "    modified_at = nb.get(\"modified_at\")\n",
    "    path = nb.get(\"path\", \"\")\n",
    "    \n",
    "    if age_threshold_ms and modified_at and int(modified_at) > age_threshold_ms:\n",
    "        continue\n",
    "    \n",
    "    is_default = any(pattern in name for pattern in DEFAULT_PATTERNS['notebooks'])\n",
    "    \n",
    "    if is_default:\n",
    "        owner_match = re.search(r\"/Users/([^/]+)/\", path)\n",
    "        owner = owner_match.group(1) if owner_match else None\n",
    "        \n",
    "        notebook_entry = {\n",
    "            \"asset_type\": \"NOTEBOOK\",\n",
    "            \"asset_name\": name,\n",
    "            \"asset_path\": path,\n",
    "            \"object_id\": str(nb.get(\"object_id\")),\n",
    "            \"owner\": owner,\n",
    "            \"created_at\": None,\n",
    "            \"modified_at\": str(modified_at) if modified_at else None\n",
    "        }\n",
    "        \n",
    "        notebook_data.append(notebook_entry)\n",
    "        \n",
    "        if not owner:\n",
    "            shared_folder_notebooks.append((len(notebook_data) - 1, path))\n",
    "\n",
    "# Fetch owner info for shared folder notebooks using ThreadPoolExecutor\n",
    "if shared_folder_notebooks and user_id_to_email and ENABLE_SHARED_FOLDER_OWNER_LOOKUP:\n",
    "    notebooks_to_lookup = shared_folder_notebooks[:MAX_NOTEBOOKS_FOR_OWNER_LOOKUP]\n",
    "    log(f\"  Fetching owner information for {len(notebooks_to_lookup)} notebooks in shared folders (parallel)...\")\n",
    "    \n",
    "    def fetch_notebook_owner(data_idx, path):\n",
    "        try:\n",
    "            status_response = requests.get(\n",
    "                f\"{api_url}/api/2.0/workspace/get-status\",\n",
    "                headers=headers,\n",
    "                params={\"path\": path}\n",
    "            )\n",
    "            if status_response.status_code == 200:\n",
    "                status_data = status_response.json()\n",
    "                created_by_id = status_data.get(\"created_by\")\n",
    "                if created_by_id and str(created_by_id) in user_id_to_email:\n",
    "                    return (data_idx, user_id_to_email[str(created_by_id)])\n",
    "        except:\n",
    "            pass\n",
    "        return (data_idx, None)\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = [executor.submit(fetch_notebook_owner, idx, path) for idx, path in notebooks_to_lookup]\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(futures):\n",
    "            data_idx, owner = future.result()\n",
    "            if owner:\n",
    "                notebook_data[data_idx][\"owner\"] = owner\n",
    "            completed += 1\n",
    "            if completed % 10 == 0:\n",
    "                log(f\"    Progress: {completed}/{len(notebooks_to_lookup)} lookups completed\")\n",
    "    \n",
    "    log(f\"  ✓ Completed owner lookup for shared folder notebooks\")\n",
    "elif shared_folder_notebooks and not ENABLE_SHARED_FOLDER_OWNER_LOOKUP:\n",
    "    log(f\"  ⏭️ Skipped owner lookup for {len(shared_folder_notebooks)} shared folder notebooks (disabled in Quick Mode)\")\n",
    "\n",
    "log(f\"✓ Found {len(notebook_data)} notebooks with default naming\")\n",
    "log_execution_time(\"Scan notebooks\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3cd5a53d-bd9d-4ec7-ad95-a2aaf0448881",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scan SQL Queries"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Fetching SQL query permissions...\")\n",
    "\n",
    "api_url, api_token = get_api_client()\n",
    "if not api_url or not api_token:\n",
    "    log(\"  ✗ Failed to get API client\")\n",
    "    queries = []\n",
    "else:\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    all_queries = []\n",
    "    page_token = None\n",
    "    page_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            params = {\"page_size\": 100}\n",
    "            if page_token:\n",
    "                params[\"page_token\"] = page_token\n",
    "            \n",
    "            response = api_call_with_retry(\n",
    "                requests.get,\n",
    "                f\"{api_url}/api/2.0/preview/sql/queries\",\n",
    "                headers=headers,\n",
    "                params=params\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if \"results\" in data:\n",
    "                    all_queries.extend(data[\"results\"])\n",
    "                    page_count += 1\n",
    "                    if page_count % 5 == 0:\n",
    "                        log(f\"  Progress: Fetched {page_count} pages ({len(all_queries)} queries)...\")\n",
    "                \n",
    "                page_token = data.get(\"next_page_token\")\n",
    "                if not page_token:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        log(f\"  ⚠️ Error listing SQL queries: {e}\")\n",
    "    \n",
    "    queries = all_queries\n",
    "\n",
    "# Filter queries with default naming\n",
    "query_data = []\n",
    "for query in queries:\n",
    "    name = query.get(\"name\", \"\")\n",
    "    updated_at = query.get(\"updated_at\")\n",
    "    \n",
    "    # Apply age filter if enabled\n",
    "    if age_threshold_ms and updated_at:\n",
    "        try:\n",
    "            query_time_ms = int(datetime.fromisoformat(updated_at.replace('Z', '+00:00')).timestamp() * 1000)\n",
    "            if query_time_ms > age_threshold_ms:\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Check if name matches default patterns\n",
    "    is_default = any(pattern in name for pattern in DEFAULT_PATTERNS['queries'])\n",
    "    \n",
    "    if is_default:\n",
    "        query_data.append({\n",
    "            \"asset_type\": \"QUERY\",\n",
    "            \"asset_name\": name,\n",
    "            \"asset_path\": None,\n",
    "            \"object_id\": query.get(\"id\"),\n",
    "            \"created_at\": query.get(\"created_at\"),\n",
    "            \"modified_at\": updated_at\n",
    "        })\n",
    "\n",
    "log(f\"✓ Found {len(query_data)} SQL queries with default naming\")\n",
    "log(f\"⏱️  Scan SQL queries completed in {time.time() - cell_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ed03c09-e31a-453f-9f27-8c5028ca866d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scan Dashboards"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Fetching dashboard permissions...\")\n",
    "\n",
    "api_url, api_token = get_api_client()\n",
    "if not api_url or not api_token:\n",
    "    log(\"  ✗ Failed to get API client\")\n",
    "    dashboards = []\n",
    "else:\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    all_dashboards = []\n",
    "    page_token = None\n",
    "    page_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            params = {\"page_size\": 100}\n",
    "            if page_token:\n",
    "                params[\"page_token\"] = page_token\n",
    "            \n",
    "            response = api_call_with_retry(\n",
    "                requests.get,\n",
    "                f\"{api_url}/api/2.0/preview/sql/dashboards\",\n",
    "                headers=headers,\n",
    "                params=params\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                if \"results\" in data:\n",
    "                    all_dashboards.extend(data[\"results\"])\n",
    "                    page_count += 1\n",
    "                    if page_count % 5 == 0:\n",
    "                        log(f\"  Progress: Fetched {page_count} pages ({len(all_dashboards)} dashboards)...\")\n",
    "                \n",
    "                page_token = data.get(\"next_page_token\")\n",
    "                if not page_token:\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        log(f\"  ⚠️ Error listing dashboards: {e}\")\n",
    "    \n",
    "    dashboards = all_dashboards\n",
    "\n",
    "# Filter dashboards with default naming\n",
    "dashboard_data = []\n",
    "for dashboard in dashboards:\n",
    "    name = dashboard.get(\"name\", \"\")\n",
    "    updated_at = dashboard.get(\"updated_at\")\n",
    "    \n",
    "    # Apply age filter if enabled\n",
    "    if age_threshold_ms and updated_at:\n",
    "        try:\n",
    "            dashboard_time_ms = int(datetime.fromisoformat(updated_at.replace('Z', '+00:00')).timestamp() * 1000)\n",
    "            if dashboard_time_ms > age_threshold_ms:\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Check if name matches default patterns\n",
    "    is_default = any(pattern in name for pattern in DEFAULT_PATTERNS['dashboards'])\n",
    "    \n",
    "    if is_default:\n",
    "        dashboard_data.append({\n",
    "            \"asset_type\": \"DASHBOARD\",\n",
    "            \"asset_name\": name,\n",
    "            \"asset_path\": None,\n",
    "            \"object_id\": dashboard.get(\"id\"),\n",
    "            \"created_at\": dashboard.get(\"created_at\"),\n",
    "            \"modified_at\": updated_at\n",
    "        })\n",
    "\n",
    "log(f\"✓ Found {len(dashboard_data)} dashboards with default naming\")\n",
    "log(f\"⏱️  Scan dashboards completed in {time.time() - cell_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c9a920-3ab9-412c-85e7-1b2c5d6627cf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scan Genie Spaces"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Fetching Genie spaces...\")\n",
    "\n",
    "api_url, api_token = get_api_client()\n",
    "if not api_url or not api_token:\n",
    "    log(\"  ✗ Failed to get API client\")\n",
    "    genie_spaces = []\n",
    "else:\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    all_genie_spaces = []\n",
    "    page_token = None\n",
    "    page_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            params = {\"page_size\": 100}\n",
    "            if page_token:\n",
    "                params[\"page_token\"] = page_token\n",
    "            \n",
    "            def fetch_genie_spaces():\n",
    "                return requests.get(\n",
    "                    f\"{api_url}/api/2.0/genie/spaces\",\n",
    "                    headers=headers,\n",
    "                    params=params,\n",
    "                    timeout=30\n",
    "                )\n",
    "            \n",
    "            response = api_call_with_retry(fetch_genie_spaces)\n",
    "            \n",
    "            if response and response.status_code == 200:\n",
    "                data = response.json()\n",
    "                spaces = data.get('spaces', [])\n",
    "                all_genie_spaces.extend(spaces)\n",
    "                page_count += 1\n",
    "                \n",
    "                if page_count % 5 == 0:\n",
    "                    log(f\"  Fetched {len(all_genie_spaces)} Genie spaces so far...\")\n",
    "                \n",
    "                page_token = data.get('next_page_token')\n",
    "                if not page_token:\n",
    "                    break\n",
    "            else:\n",
    "                log(f\"  ✗ Failed to fetch Genie spaces: {response.status_code if response else 'No response'}\")\n",
    "                break\n",
    "        \n",
    "        log(f\"  ✓ Fetched {len(all_genie_spaces)} total Genie spaces\")\n",
    "        execution_stats['resources_processed'] += len(all_genie_spaces)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"  ✗ Error fetching Genie spaces: {str(e)}\")\n",
    "        all_genie_spaces = []\n",
    "    \n",
    "    # Filter for default naming patterns\n",
    "    genie_spaces = []\n",
    "    for space in all_genie_spaces:\n",
    "        space_title = space.get('title', '')\n",
    "        space_id = space.get('space_id', '')\n",
    "        \n",
    "        # Check if title matches default patterns\n",
    "        is_default_name = any(\n",
    "            pattern.lower() in space_title.lower() \n",
    "            for pattern in DEFAULT_PATTERNS.get('genie_spaces', ['New Genie Space', 'Untitled Space', 'Untitled'])\n",
    "        )\n",
    "        \n",
    "        if is_default_name:\n",
    "            # Extract owner from created_by or owner_user_name field\n",
    "            owner = space.get('owner_user_name', space.get('created_by', 'Unknown'))\n",
    "            \n",
    "            # Get timestamps\n",
    "            created_at = space.get('created_at')\n",
    "            updated_at = space.get('updated_at')\n",
    "            \n",
    "            # Convert timestamps from milliseconds to datetime if present\n",
    "            eastern = pytz.timezone('America/New_York')\n",
    "            created_timestamp = None\n",
    "            modified_timestamp = None\n",
    "            \n",
    "            if created_at:\n",
    "                try:\n",
    "                    created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if updated_at:\n",
    "                try:\n",
    "                    modified_timestamp = datetime.fromtimestamp(updated_at / 1000, tz=eastern)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Apply age filter if configured\n",
    "            if MIN_AGE_DAYS and modified_timestamp:\n",
    "                age_days = (datetime.now(eastern) - modified_timestamp).days\n",
    "                if age_days < MIN_AGE_DAYS:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Apply incremental scan filter if enabled\n",
    "            if last_scan_timestamp and modified_timestamp:\n",
    "                if modified_timestamp <= last_scan_timestamp:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            genie_spaces.append({\n",
    "                'asset_type': 'genie_space',\n",
    "                'asset_name': space_title,\n",
    "                'asset_id': space_id,\n",
    "                'asset_path': f\"/genie/spaces/{space_id}\",  # Construct path\n",
    "                'owner': owner,\n",
    "                'created_timestamp': created_timestamp,\n",
    "                'modified_timestamp': modified_timestamp\n",
    "            })\n",
    "\n",
    "log(f\"  Found {len(genie_spaces)} Genie spaces with default naming\")\n",
    "genie_space_data = genie_spaces\n",
    "\n",
    "log_execution_time(\"Scan Genie Spaces\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889fe0ed-a3f4-4673-ba79-596dc3e4c5af",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scan Jobs"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Fetching jobs...\")\n",
    "\n",
    "api_url, api_token = get_api_client()\n",
    "if not api_url or not api_token:\n",
    "    log(\"  ✗ Failed to get API client\")\n",
    "    jobs = []\n",
    "else:\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    all_jobs = []\n",
    "    page_token = None\n",
    "    page_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            params = {\"limit\": 25, \"expand_tasks\": False}\n",
    "            if page_token:\n",
    "                params[\"page_token\"] = page_token\n",
    "            \n",
    "            def fetch_jobs():\n",
    "                return requests.get(\n",
    "                    f\"{api_url}/api/2.1/jobs/list\",\n",
    "                    headers=headers,\n",
    "                    params=params,\n",
    "                    timeout=30\n",
    "                )\n",
    "            \n",
    "            response = api_call_with_retry(fetch_jobs)\n",
    "            \n",
    "            if response and response.status_code == 200:\n",
    "                data = response.json()\n",
    "                jobs_list = data.get('jobs', [])\n",
    "                all_jobs.extend(jobs_list)\n",
    "                page_count += 1\n",
    "                \n",
    "                if page_count % 5 == 0:\n",
    "                    log(f\"  Fetched {len(all_jobs)} jobs so far...\")\n",
    "                \n",
    "                # Check for pagination\n",
    "                if data.get('has_more', False):\n",
    "                    page_token = data.get('next_page_token')\n",
    "                    if not page_token:\n",
    "                        break\n",
    "                else:\n",
    "                    break\n",
    "            else:\n",
    "                log(f\"  ✗ Failed to fetch jobs: {response.status_code if response else 'No response'}\")\n",
    "                break\n",
    "        \n",
    "        log(f\"  ✓ Fetched {len(all_jobs)} total jobs\")\n",
    "        execution_stats['resources_processed'] += len(all_jobs)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"  ✗ Error fetching jobs: {str(e)}\")\n",
    "        all_jobs = []\n",
    "    \n",
    "    # Filter for default naming patterns\n",
    "    jobs = []\n",
    "    for job in all_jobs:\n",
    "        job_settings = job.get('settings', {})\n",
    "        job_name = job_settings.get('name', '')\n",
    "        job_id = job.get('job_id', '')\n",
    "        \n",
    "        # Check if name matches default patterns\n",
    "        is_default_name = any(\n",
    "            pattern.lower() in job_name.lower() \n",
    "            for pattern in DEFAULT_PATTERNS.get('jobs', ['Untitled Job', 'New Job', 'Untitled'])\n",
    "        )\n",
    "        \n",
    "        if is_default_name:\n",
    "            # Extract owner from creator_user_name\n",
    "            owner = job.get('creator_user_name', 'Unknown')\n",
    "            \n",
    "            # Get timestamps\n",
    "            created_at = job.get('created_time')\n",
    "            \n",
    "            # Convert timestamps from milliseconds to datetime if present\n",
    "            eastern = pytz.timezone('America/New_York')\n",
    "            created_timestamp = None\n",
    "            modified_timestamp = None\n",
    "            \n",
    "            if created_at:\n",
    "                try:\n",
    "                    created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "                    # Jobs don't have a separate modified timestamp, use created as fallback\n",
    "                    modified_timestamp = created_timestamp\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Apply age filter if configured\n",
    "            if MIN_AGE_DAYS and modified_timestamp:\n",
    "                age_days = (datetime.now(eastern) - modified_timestamp).days\n",
    "                if age_days < MIN_AGE_DAYS:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Apply incremental scan filter if enabled\n",
    "            if last_scan_timestamp and modified_timestamp:\n",
    "                if modified_timestamp <= last_scan_timestamp:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            jobs.append({\n",
    "                'asset_type': 'job',\n",
    "                'asset_name': job_name,\n",
    "                'asset_id': str(job_id),\n",
    "                'asset_path': f\"/jobs/{job_id}\",\n",
    "                'owner': owner,\n",
    "                'created_timestamp': created_timestamp,\n",
    "                'modified_timestamp': modified_timestamp\n",
    "            })\n",
    "\n",
    "log(f\"  Found {len(jobs)} jobs with default naming\")\n",
    "jobs_data = jobs\n",
    "\n",
    "log_execution_time(\"Scan Jobs\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e5fafe-819a-49de-936a-1f97965f5e09",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scan SQL Alerts"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Fetching SQL alerts...\")\n",
    "\n",
    "api_url, api_token = get_api_client()\n",
    "if not api_url or not api_token:\n",
    "    log(\"  ✗ Failed to get API client\")\n",
    "    alerts = []\n",
    "else:\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    all_alerts = []\n",
    "    \n",
    "    # Try both legacy and new alerts APIs\n",
    "    try:\n",
    "        # Try legacy alerts API first (more commonly used)\n",
    "        def fetch_legacy_alerts():\n",
    "            return requests.get(\n",
    "                f\"{api_url}/api/2.0/preview/sql/alerts\",\n",
    "                headers=headers,\n",
    "                timeout=30\n",
    "            )\n",
    "        \n",
    "        response = api_call_with_retry(fetch_legacy_alerts)\n",
    "        \n",
    "        if response and response.status_code == 200:\n",
    "            data = response.json()\n",
    "            legacy_alerts = data if isinstance(data, list) else data.get('results', [])\n",
    "            all_alerts.extend(legacy_alerts)\n",
    "            log(f\"  ✓ Fetched {len(legacy_alerts)} legacy alerts\")\n",
    "        else:\n",
    "            log(f\"  ⚠️  Legacy alerts API returned: {response.status_code if response else 'No response'}\")\n",
    "        \n",
    "        execution_stats['resources_processed'] += len(all_alerts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"  ⚠️  Error fetching legacy alerts: {str(e)}\")\n",
    "    \n",
    "    # Try new alerts API with pagination\n",
    "    try:\n",
    "        page_token = None\n",
    "        page_count = 0\n",
    "        new_alerts_count = 0\n",
    "        \n",
    "        while True:\n",
    "            params = {\"page_size\": 100}\n",
    "            if page_token:\n",
    "                params[\"page_token\"] = page_token\n",
    "            \n",
    "            def fetch_new_alerts():\n",
    "                return requests.get(\n",
    "                    f\"{api_url}/api/2.0/alerts\",\n",
    "                    headers=headers,\n",
    "                    params=params,\n",
    "                    timeout=30\n",
    "                )\n",
    "            \n",
    "            response = api_call_with_retry(fetch_new_alerts)\n",
    "            \n",
    "            if response and response.status_code == 200:\n",
    "                data = response.json()\n",
    "                alerts_list = data.get('alerts', [])\n",
    "                all_alerts.extend(alerts_list)\n",
    "                new_alerts_count += len(alerts_list)\n",
    "                page_count += 1\n",
    "                \n",
    "                page_token = data.get('next_page_token')\n",
    "                if not page_token:\n",
    "                    break\n",
    "            else:\n",
    "                # New alerts API might not be available\n",
    "                break\n",
    "        \n",
    "        if new_alerts_count > 0:\n",
    "            log(f\"  ✓ Fetched {new_alerts_count} new alerts\")\n",
    "            execution_stats['resources_processed'] += new_alerts_count\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"  ⚠️  Error fetching new alerts: {str(e)}\")\n",
    "    \n",
    "    log(f\"  ✓ Fetched {len(all_alerts)} total alerts\")\n",
    "    \n",
    "    # Filter for default naming patterns\n",
    "    alerts = []\n",
    "    for alert in all_alerts:\n",
    "        alert_name = alert.get('name', '')\n",
    "        alert_id = alert.get('id', '')\n",
    "        \n",
    "        # Check if name matches default patterns\n",
    "        is_default_name = any(\n",
    "            pattern.lower() in alert_name.lower() \n",
    "            for pattern in DEFAULT_PATTERNS.get('alerts', ['New Alert', 'Untitled Alert', 'Untitled'])\n",
    "        )\n",
    "        \n",
    "        if is_default_name:\n",
    "            # Extract owner - try multiple fields\n",
    "            owner = alert.get('owner_user_name', alert.get('user', {}).get('email', alert.get('created_by', 'Unknown')))\n",
    "            \n",
    "            # Get timestamps\n",
    "            created_at = alert.get('created_at')\n",
    "            updated_at = alert.get('updated_at')\n",
    "            \n",
    "            # Convert timestamps to datetime if present\n",
    "            eastern = pytz.timezone('America/New_York')\n",
    "            created_timestamp = None\n",
    "            modified_timestamp = None\n",
    "            \n",
    "            # Handle different timestamp formats (milliseconds or ISO string)\n",
    "            if created_at:\n",
    "                try:\n",
    "                    if isinstance(created_at, (int, float)):\n",
    "                        created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "                    else:\n",
    "                        created_timestamp = datetime.fromisoformat(str(created_at).replace('Z', '+00:00')).astimezone(eastern)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if updated_at:\n",
    "                try:\n",
    "                    if isinstance(updated_at, (int, float)):\n",
    "                        modified_timestamp = datetime.fromtimestamp(updated_at / 1000, tz=eastern)\n",
    "                    else:\n",
    "                        modified_timestamp = datetime.fromisoformat(str(updated_at).replace('Z', '+00:00')).astimezone(eastern)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Use created as fallback for modified\n",
    "            if not modified_timestamp and created_timestamp:\n",
    "                modified_timestamp = created_timestamp\n",
    "            \n",
    "            # Apply age filter if configured\n",
    "            if MIN_AGE_DAYS and modified_timestamp:\n",
    "                age_days = (datetime.now(eastern) - modified_timestamp).days\n",
    "                if age_days < MIN_AGE_DAYS:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Apply incremental scan filter if enabled\n",
    "            if last_scan_timestamp and modified_timestamp:\n",
    "                if modified_timestamp <= last_scan_timestamp:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            alerts.append({\n",
    "                'asset_type': 'alert',\n",
    "                'asset_name': alert_name,\n",
    "                'asset_id': str(alert_id),\n",
    "                'asset_path': f\"/alerts/{alert_id}\",\n",
    "                'owner': owner,\n",
    "                'created_timestamp': created_timestamp,\n",
    "                'modified_timestamp': modified_timestamp\n",
    "            })\n",
    "\n",
    "log(f\"  Found {len(alerts)} alerts with default naming\")\n",
    "alerts_data = alerts\n",
    "\n",
    "log_execution_time(\"Scan SQL Alerts\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c80d3752-7ba5-4d8c-8de3-650125926878",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scan DLT Pipelines"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Fetching DLT pipelines...\")\n",
    "\n",
    "api_url, api_token = get_api_client()\n",
    "if not api_url or not api_token:\n",
    "    log(\"  ✗ Failed to get API client\")\n",
    "    pipelines = []\n",
    "else:\n",
    "    headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "    all_pipelines = []\n",
    "    page_token = None\n",
    "    page_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            params = {\"max_results\": 100}\n",
    "            if page_token:\n",
    "                params[\"page_token\"] = page_token\n",
    "            \n",
    "            def fetch_pipelines():\n",
    "                return requests.get(\n",
    "                    f\"{api_url}/api/2.0/pipelines\",\n",
    "                    headers=headers,\n",
    "                    params=params,\n",
    "                    timeout=30\n",
    "                )\n",
    "            \n",
    "            response = api_call_with_retry(fetch_pipelines)\n",
    "            \n",
    "            if response and response.status_code == 200:\n",
    "                data = response.json()\n",
    "                pipelines_list = data.get('statuses', [])\n",
    "                all_pipelines.extend(pipelines_list)\n",
    "                page_count += 1\n",
    "                \n",
    "                if page_count % 5 == 0:\n",
    "                    log(f\"  Fetched {len(all_pipelines)} pipelines so far...\")\n",
    "                \n",
    "                # Check for pagination\n",
    "                page_token = data.get('next_page_token')\n",
    "                if not page_token:\n",
    "                    break\n",
    "            else:\n",
    "                log(f\"  ✗ Failed to fetch pipelines: {response.status_code if response else 'No response'}\")\n",
    "                break\n",
    "        \n",
    "        log(f\"  ✓ Fetched {len(all_pipelines)} total pipelines\")\n",
    "        execution_stats['resources_processed'] += len(all_pipelines)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"  ✗ Error fetching pipelines: {str(e)}\")\n",
    "        all_pipelines = []\n",
    "    \n",
    "    # Filter for default naming patterns\n",
    "    pipelines = []\n",
    "    for pipeline in all_pipelines:\n",
    "        pipeline_name = pipeline.get('name', '')\n",
    "        pipeline_id = pipeline.get('pipeline_id', '')\n",
    "        \n",
    "        # Check if name matches default patterns\n",
    "        is_default_name = any(\n",
    "            pattern.lower() in pipeline_name.lower() \n",
    "            for pattern in DEFAULT_PATTERNS.get('pipelines', ['Untitled Pipeline', 'New Pipeline', 'Untitled'])\n",
    "        )\n",
    "        \n",
    "        if is_default_name:\n",
    "            # Extract owner from creator_user_name or created_by\n",
    "            owner = pipeline.get('creator_user_name', pipeline.get('created_by', 'Unknown'))\n",
    "            \n",
    "            # Get timestamps - pipelines may have creation_time\n",
    "            created_at = pipeline.get('creation_time')\n",
    "            \n",
    "            # Convert timestamps from milliseconds to datetime if present\n",
    "            eastern = pytz.timezone('America/New_York')\n",
    "            created_timestamp = None\n",
    "            modified_timestamp = None\n",
    "            \n",
    "            if created_at:\n",
    "                try:\n",
    "                    created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "                    # Pipelines don't have a separate modified timestamp, use created as fallback\n",
    "                    modified_timestamp = created_timestamp\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Apply age filter if configured\n",
    "            if MIN_AGE_DAYS and modified_timestamp:\n",
    "                age_days = (datetime.now(eastern) - modified_timestamp).days\n",
    "                if age_days < MIN_AGE_DAYS:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Apply incremental scan filter if enabled\n",
    "            if last_scan_timestamp and modified_timestamp:\n",
    "                if modified_timestamp <= last_scan_timestamp:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            pipelines.append({\n",
    "                'asset_type': 'pipeline',\n",
    "                'asset_name': pipeline_name,\n",
    "                'asset_id': str(pipeline_id),\n",
    "                'asset_path': f\"/pipelines/{pipeline_id}\",\n",
    "                'owner': owner,\n",
    "                'created_timestamp': created_timestamp,\n",
    "                'modified_timestamp': modified_timestamp\n",
    "            })\n",
    "\n",
    "log(f\"  Found {len(pipelines)} pipelines with default naming\")\n",
    "pipelines_data = pipelines\n",
    "\n",
    "log_execution_time(\"Scan DLT Pipelines\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c7f773c-47e3-40d3-8d74-4e0b9cacc4c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Scan MLflow Experiments"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Fetching MLflow experiments...\")\n",
    "\n",
    "api_url, api_token = get_api_client()\n",
    "if not api_url or not api_token:\n",
    "    log(\"  ✗ Failed to get API client\")\n",
    "    experiments = []\n",
    "else:\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    all_experiments = []\n",
    "    page_token = None\n",
    "    page_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            payload = {\n",
    "                \"max_results\": 1000,\n",
    "                \"view_type\": \"ACTIVE_ONLY\"\n",
    "            }\n",
    "            if page_token:\n",
    "                payload[\"page_token\"] = page_token\n",
    "            \n",
    "            def fetch_experiments():\n",
    "                return requests.post(\n",
    "                    f\"{api_url}/api/2.0/mlflow/experiments/search\",\n",
    "                    headers=headers,\n",
    "                    json=payload,\n",
    "                    timeout=30\n",
    "                )\n",
    "            \n",
    "            response = api_call_with_retry(fetch_experiments)\n",
    "            \n",
    "            if response and response.status_code == 200:\n",
    "                data = response.json()\n",
    "                experiments_list = data.get('experiments', [])\n",
    "                all_experiments.extend(experiments_list)\n",
    "                page_count += 1\n",
    "                \n",
    "                if page_count % 5 == 0:\n",
    "                    log(f\"  Fetched {len(all_experiments)} experiments so far...\")\n",
    "                \n",
    "                # Check for pagination\n",
    "                page_token = data.get('next_page_token')\n",
    "                if not page_token:\n",
    "                    break\n",
    "            else:\n",
    "                log(f\"  ✗ Failed to fetch experiments: {response.status_code if response else 'No response'}\")\n",
    "                break\n",
    "        \n",
    "        log(f\"  ✓ Fetched {len(all_experiments)} total experiments\")\n",
    "        execution_stats['resources_processed'] += len(all_experiments)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"  ✗ Error fetching experiments: {str(e)}\")\n",
    "        all_experiments = []\n",
    "    \n",
    "    # Filter for default naming patterns\n",
    "    experiments = []\n",
    "    for experiment in all_experiments:\n",
    "        experiment_name = experiment.get('name', '')\n",
    "        experiment_id = experiment.get('experiment_id', '')\n",
    "        \n",
    "        # Check if name matches default patterns\n",
    "        is_default_name = any(\n",
    "            pattern.lower() in experiment_name.lower() \n",
    "            for pattern in DEFAULT_PATTERNS.get('experiments', ['Default', 'Untitled Experiment', 'Untitled'])\n",
    "        )\n",
    "        \n",
    "        if is_default_name:\n",
    "            # Extract owner from tags or artifact_location path\n",
    "            tags = experiment.get('tags', [])\n",
    "            owner = 'Unknown'\n",
    "            \n",
    "            # Try to extract owner from tags\n",
    "            for tag in tags:\n",
    "                if tag.get('key') in ['mlflow.user', 'owner', 'created_by']:\n",
    "                    owner = tag.get('value', 'Unknown')\n",
    "                    break\n",
    "            \n",
    "            # If no owner in tags, try to extract from artifact_location\n",
    "            if owner == 'Unknown':\n",
    "                artifact_location = experiment.get('artifact_location', '')\n",
    "                if '/Users/' in artifact_location:\n",
    "                    try:\n",
    "                        owner = artifact_location.split('/Users/')[1].split('/')[0]\n",
    "                    except:\n",
    "                        pass\n",
    "            \n",
    "            # Get timestamps\n",
    "            created_at = experiment.get('creation_time')\n",
    "            updated_at = experiment.get('last_update_time')\n",
    "            \n",
    "            # Convert timestamps from milliseconds to datetime if present\n",
    "            eastern = pytz.timezone('America/New_York')\n",
    "            created_timestamp = None\n",
    "            modified_timestamp = None\n",
    "            \n",
    "            if created_at:\n",
    "                try:\n",
    "                    created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            if updated_at:\n",
    "                try:\n",
    "                    modified_timestamp = datetime.fromtimestamp(updated_at / 1000, tz=eastern)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            # Use created as fallback for modified\n",
    "            if not modified_timestamp and created_timestamp:\n",
    "                modified_timestamp = created_timestamp\n",
    "            \n",
    "            # Apply age filter if configured\n",
    "            if MIN_AGE_DAYS and modified_timestamp:\n",
    "                age_days = (datetime.now(eastern) - modified_timestamp).days\n",
    "                if age_days < MIN_AGE_DAYS:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            # Apply incremental scan filter if enabled\n",
    "            if last_scan_timestamp and modified_timestamp:\n",
    "                if modified_timestamp <= last_scan_timestamp:\n",
    "                    execution_stats['resources_skipped'] += 1\n",
    "                    continue\n",
    "            \n",
    "            experiments.append({\n",
    "                'asset_type': 'experiment',\n",
    "                'asset_name': experiment_name,\n",
    "                'asset_id': str(experiment_id),\n",
    "                'asset_path': f\"/ml/experiments/{experiment_id}\",\n",
    "                'owner': owner,\n",
    "                'created_timestamp': created_timestamp,\n",
    "                'modified_timestamp': modified_timestamp\n",
    "            })\n",
    "\n",
    "log(f\"  Found {len(experiments)} experiments with default naming\")\n",
    "experiments_data = experiments\n",
    "\n",
    "log_execution_time(\"Scan MLflow Experiments\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4134af1-bcc3-4d60-b390-83387ff4623c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create Consolidated DataFrame"
    }
   },
   "outputs": [],
   "source": [
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"CREATING CONSOLIDATED DATAFRAME\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Track memory usage\n",
    "memory_before = get_memory_usage()\n",
    "\n",
    "# Combine all results\n",
    "all_assets = notebook_data + query_data + dashboard_data + genie_space_data + jobs_data + alerts_data + pipelines_data + experiments_data\n",
    "\n",
    "log(f\"Total assets with default naming: {len(all_assets)}\")\n",
    "log(f\"  - Notebooks: {len(notebook_data)}\")\n",
    "log(f\"  - SQL Queries: {len(query_data)}\")\n",
    "log(f\"  - Dashboards: {len(dashboard_data)}\")\n",
    "log(f\"  - Genie Spaces: {len(genie_space_data)}\")\n",
    "log(f\"  - Jobs: {len(jobs_data)}\")\n",
    "log(f\"  - SQL Alerts: {len(alerts_data)}\")\n",
    "log(f\"  - DLT Pipelines: {len(pipelines_data)}\")\n",
    "log(f\"  - MLflow Experiments: {len(experiments_data)}\")\n",
    "\n",
    "if len(all_assets) == 0:\n",
    "    log(\"\\n✓ No assets with default naming found!\")\n",
    "    df_assets = None\n",
    "else:\n",
    "    # Create DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"asset_type\", StringType(), True),\n",
    "        StructField(\"asset_name\", StringType(), True),\n",
    "        StructField(\"asset_id\", StringType(), True),\n",
    "        StructField(\"asset_path\", StringType(), True),\n",
    "        StructField(\"owner\", StringType(), True),\n",
    "        StructField(\"created_timestamp\", TimestampType(), True),\n",
    "        StructField(\"modified_timestamp\", TimestampType(), True)\n",
    "    ])\n",
    "    \n",
    "    df_assets = spark.createDataFrame(all_assets, schema)\n",
    "    \n",
    "    # Add scan timestamp\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    scan_timestamp = datetime.now(eastern)\n",
    "    df_assets = df_assets.withColumn(\"scan_timestamp\", F.lit(scan_timestamp))\n",
    "    \n",
    "    # Deduplicate based on asset_type + asset_id\n",
    "    initial_count = df_assets.count()\n",
    "    df_assets = df_assets.dropDuplicates([\"asset_type\", \"asset_id\"])\n",
    "    final_count = df_assets.count()\n",
    "    \n",
    "    if initial_count > final_count:\n",
    "        log(f\"\\n⚠️  Removed {initial_count - final_count} duplicate entries\")\n",
    "    \n",
    "    log(f\"\\n✓ Created DataFrame with {final_count} unique assets\")\n",
    "    \n",
    "    # Validate DataFrame\n",
    "    if not validate_dataframe_exists(\"df_assets\", df_assets):\n",
    "        log(\"⚠️  Warning: DataFrame validation failed\")\n",
    "    \n",
    "    # Track memory usage\n",
    "    memory_after = get_memory_usage()\n",
    "    memory_delta = memory_after - memory_before\n",
    "    execution_stats['memory_usage_mb'] = max(execution_stats['memory_usage_mb'], memory_after)\n",
    "    \n",
    "    if memory_delta > 100:\n",
    "        log(f\"\\n⚠️  Memory usage increased by {memory_delta:.2f} MB\")\n",
    "    \n",
    "    # Show sample\n",
    "    log(\"\\nSample of assets found:\")\n",
    "    display(df_assets.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "affdb025-0d1b-4506-b1d5-4e98be1bb4d5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to Delta Table (Long-term Retention)"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE_DELTA_EXPORT and df_assets is not None:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"EXPORTING TO DELTA TABLE (LONG-TERM RETENTION)\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Parse catalog, schema, table from DELTA_TABLE_NAME\n",
    "    parts = DELTA_TABLE_NAME.split('.')\n",
    "    if len(parts) != 3:\n",
    "        log(f\"✗ Invalid DELTA_TABLE_NAME format: {DELTA_TABLE_NAME}\")\n",
    "        log(\"  Expected format: catalog.schema.table\")\n",
    "    else:\n",
    "        catalog, schema, table = parts\n",
    "        log(f\"Target: {DELTA_TABLE_NAME}\")\n",
    "        log(f\"  Catalog: {catalog}\")\n",
    "        log(f\"  Schema: {schema}\")\n",
    "        log(f\"  Table: {table}\")\n",
    "        \n",
    "        try:\n",
    "            # Add timestamp for historical tracking\n",
    "            eastern = pytz.timezone('America/New_York')\n",
    "            df_with_timestamp = df_assets.withColumn(\n",
    "                \"export_timestamp\",\n",
    "                F.lit(datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z'))\n",
    "            )\n",
    "            \n",
    "            # Write to Delta table\n",
    "            df_with_timestamp.write \\\n",
    "                .format(\"delta\") \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"mergeSchema\", \"true\") \\\n",
    "                .saveAsTable(DELTA_TABLE_NAME)\n",
    "            \n",
    "            row_count = df_assets.count()\n",
    "            log(f\"✓ Delta table export successful\")\n",
    "            log(f\"  Rows written: {row_count}\")\n",
    "            log(f\"  Mode: append (historical accumulation)\")\n",
    "            log(f\"  Schema merge: enabled\")\n",
    "            log_execution_time(\"Delta table export\", cell_start_time)\n",
    "            \n",
    "            # Verify table exists\n",
    "            try:\n",
    "                table_info = spark.sql(f\"DESCRIBE EXTENDED {DELTA_TABLE_NAME}\")\n",
    "                log(f\"  ✓ Table verified: {DELTA_TABLE_NAME}\")\n",
    "            except Exception as verify_error:\n",
    "                log(f\"  ⚠️ Warning: Could not verify table: {verify_error}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            log(f\"✗ Delta table export failed: {str(e)[:300]}\")\n",
    "            log(\"  Continuing with other export formats...\")\n",
    "            if is_job_mode:\n",
    "                raise  # Fail job if Delta export fails in job mode\n",
    "else:\n",
    "    if not ENABLE_DELTA_EXPORT:\n",
    "        log(\"\\n⊘ Delta export disabled (ENABLE_DELTA_EXPORT=False)\")\n",
    "        log(\"   Set ENABLE_DELTA_EXPORT = True to enable long-term retention\")\n",
    "    else:\n",
    "        log(\"\\n⊘ Delta export skipped (no data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fc8ce8d-727a-4590-9a6e-0bfda574d838",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to Excel"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE_EXCEL_EXPORT and df_assets is not None:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"\\nExporting to Excel...\")\n",
    "    \n",
    "    # Create timestamp for filename\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "    excel_path = f\"{EXPORT_PATH}/workspace_default_named_assets_{timestamp}.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        # Convert to Pandas\n",
    "        pdf_assets = df_assets.toPandas()\n",
    "        \n",
    "        # Convert any timestamp columns to Eastern Time\n",
    "        for col in pdf_assets.columns:\n",
    "            if pd.api.types.is_datetime64_any_dtype(pdf_assets[col]):\n",
    "                if pdf_assets[col].dt.tz is None:\n",
    "                    pdf_assets[col] = pd.to_datetime(pdf_assets[col]).dt.tz_localize('UTC').dt.tz_convert(eastern)\n",
    "                else:\n",
    "                    pdf_assets[col] = pdf_assets[col].dt.tz_convert(eastern)\n",
    "        \n",
    "        # Create Excel writer\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            # Main sheet with all assets\n",
    "            pdf_assets.to_excel(writer, sheet_name='All Assets', index=False)\n",
    "            \n",
    "            # Summary sheet\n",
    "            summary_data = {\n",
    "                'Metric': ['Total Assets', 'Notebooks', 'SQL Queries', 'Dashboards', 'Scan Date (ET)', 'Execution Time (min)'],\n",
    "                'Value': [\n",
    "                    len(all_assets),\n",
    "                    len(notebook_data),\n",
    "                    len(query_data),\n",
    "                    len(dashboard_data),\n",
    "                    datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z'),\n",
    "                    round((time.time() - execution_stats['start_time']) / 60, 2)\n",
    "                ]\n",
    "            }\n",
    "            pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "            \n",
    "            # Breakdown by type\n",
    "            type_breakdown = pdf_assets.groupby('asset_type').size().reset_index(name='count')\n",
    "            type_breakdown.to_excel(writer, sheet_name='By Type', index=False)\n",
    "            \n",
    "            # Owner analysis\n",
    "            owner_breakdown = pdf_assets[pdf_assets['owner'].notna() & (pdf_assets['owner'] != '')] \\\n",
    "                .groupby('owner').size().reset_index(name='count') \\\n",
    "                .sort_values('count', ascending=False)\n",
    "            owner_breakdown.to_excel(writer, sheet_name='By Owner', index=False)\n",
    "            \n",
    "            # Cleanup recommendations (stale assets)\n",
    "            if 'df_stale_assets' in globals() and df_stale_assets is not None:\n",
    "                pdf_stale = df_stale_assets.toPandas()\n",
    "                if len(pdf_stale) > 0:\n",
    "                    pdf_stale.to_excel(writer, sheet_name='Cleanup Recommendations', index=False)\n",
    "            \n",
    "            # Execution stats sheet\n",
    "            stats_data = {\n",
    "                'Metric': ['API Calls', 'Resources Processed', 'Resources Skipped', 'API Failures', 'API Retries', 'Success Rate (%)'],\n",
    "                'Value': [\n",
    "                    execution_stats['api_calls'],\n",
    "                    execution_stats['resources_processed'],\n",
    "                    execution_stats['resources_skipped'],\n",
    "                    execution_stats['api_failures'],\n",
    "                    execution_stats['api_retries'],\n",
    "                    round(((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls'] * 100), 2) if execution_stats['api_calls'] > 0 else 0\n",
    "                ]\n",
    "            }\n",
    "            pd.DataFrame(stats_data).to_excel(writer, sheet_name='Execution Stats', index=False)\n",
    "        \n",
    "        # Count sheets\n",
    "        sheet_count = 5 + (1 if 'df_stale_assets' in globals() and df_stale_assets is not None and df_stale_assets.count() > 0 else 0)\n",
    "        \n",
    "        log(f\"✓ Excel export successful: {excel_path}\")\n",
    "        log(f\"  File size: {os.path.getsize(excel_path) / 1024:.2f} KB\")\n",
    "        log(f\"  Sheets: {sheet_count} (All Assets, Summary, By Type, By Owner, Execution Stats\" + (\", Cleanup Recommendations\" if sheet_count > 5 else \"\") + \")\")\n",
    "        log(f\"  Rows: {len(pdf_assets)}\")\n",
    "        log_execution_time(\"Excel export\", cell_start_time)\n",
    "    except Exception as e:\n",
    "        log(f\"✗ Excel export failed: {e}\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "else:\n",
    "    if not ENABLE_EXCEL_EXPORT:\n",
    "        log(\"\\n⊘ Excel export disabled (ENABLE_EXCEL_EXPORT=False)\")\n",
    "    else:\n",
    "        log(\"\\n⊘ Excel export skipped (no data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f2f3ad4-8652-48d0-a138-cc5789408b69",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770834492702}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Display Summary Report"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate total execution time\n",
    "total_execution_time = time.time() - execution_stats['start_time']\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"SUMMARY REPORT\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Create timestamp\n",
    "eastern = pytz.timezone('America/New_York')\n",
    "completion_time = datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "log(f\"Scan completed at: {completion_time}\")\n",
    "log(f\"Total execution time: {total_execution_time:.2f} seconds ({total_execution_time/60:.2f} minutes)\")\n",
    "log(f\"Execution mode: {execution_mode}\")\n",
    "log(f\"\\nTotal assets found: {len(all_assets)}\")\n",
    "log(f\"  - Notebooks: {len(notebook_data)}\")\n",
    "log(f\"  - SQL Queries: {len(query_data)}\")\n",
    "log(f\"  - Dashboards: {len(dashboard_data)}\")\n",
    "\n",
    "if df_assets is not None:\n",
    "    log(\"\\nBreakdown by asset type:\")\n",
    "    df_assets.groupBy(\"asset_type\").count().orderBy(\"asset_type\").show()\n",
    "    \n",
    "    # Validate DataFrame before displaying\n",
    "    if validate_dataframe_exists(\"df_assets\", df_assets):\n",
    "        if not is_job_mode:\n",
    "            log(\"\\nSample of assets with default naming (first 20):\")\n",
    "            display(df_assets.select(\"asset_type\", \"asset_name\", \"owner\", \"asset_path\", \"modified_timestamp\").limit(20))\n",
    "else:\n",
    "    log(\"\\n⚠️ No assets found with default naming conventions\")\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"EXPORT LOCATIONS\")\n",
    "log(\"=\"*60)\n",
    "if ENABLE_DELTA_EXPORT:\n",
    "    log(f\"\uD83D\uDCBE Delta Table: {DELTA_TABLE_NAME}\")\n",
    "    log(f\"   Query: SELECT * FROM {DELTA_TABLE_NAME}\")\n",
    "if ENABLE_EXCEL_EXPORT:\n",
    "    log(f\"\uD83D\uDCCA Excel Files: {EXPORT_PATH}/workspace_default_named_assets_*.xlsx\")\n",
    "if ENABLE_HTML_EXPORT:\n",
    "    log(f\"\uD83C\uDF10 HTML Reports: {EXPORT_PATH}/workspace_default_named_assets_*.html\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "log(\"\\n✓ Workspace scan complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef7f643-f207-4a08-84ef-9eadbe842b99",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770836209317}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": "Interactive Mode - Additional Analysis"
    }
   },
   "outputs": [],
   "source": [
    "if execution_mode == 'interactive' and df_assets is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"ADDITIONAL ANALYSIS (Interactive Mode)\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Top users with most default-named assets\n",
    "    log(\"\\nTop 10 Users with Most Default-Named Assets:\")\n",
    "    from pyspark.sql.functions import regexp_extract\n",
    "    \n",
    "    # Use the owner column directly if available, otherwise extract from path\n",
    "    if 'owner' in df_assets.columns:\n",
    "        df_user_summary = df_assets \\\n",
    "            .filter(F.col(\"owner\").isNotNull()) \\\n",
    "            .groupBy(\"owner\") \\\n",
    "            .count() \\\n",
    "            .orderBy(F.col(\"count\").desc()) \\\n",
    "            .limit(10)\n",
    "    else:\n",
    "        df_with_users = df_assets.withColumn(\n",
    "            \"user\",\n",
    "            regexp_extract(F.col(\"asset_path\"), r\"/Users/([^/]+)/\", 1)\n",
    "        )\n",
    "        \n",
    "        df_user_summary = df_with_users \\\n",
    "            .filter(F.col(\"user\") != \"\") \\\n",
    "            .groupBy(\"user\") \\\n",
    "            .count() \\\n",
    "            .orderBy(F.col(\"count\").desc()) \\\n",
    "            .limit(10)\n",
    "    \n",
    "    display(df_user_summary)\n",
    "    \n",
    "    # Recent activity\n",
    "    log(\"\\nRecent Activity (Assets modified in last 90 days):\")\n",
    "    from pyspark.sql.functions import datediff\n",
    "    \n",
    "    df_recent = df_assets \\\n",
    "        .filter(F.col(\"modified_timestamp\").isNotNull()) \\\n",
    "        .withColumn(\"days_since_modified\", datediff(F.current_timestamp(), F.col(\"modified_timestamp\"))) \\\n",
    "        .filter(F.col(\"days_since_modified\") <= 90) \\\n",
    "        .select(\"asset_type\", \"asset_name\", \"owner\", \"asset_path\", \"days_since_modified\") \\\n",
    "        .orderBy(\"days_since_modified\")\n",
    "    \n",
    "    recent_count = df_recent.count()\n",
    "    log(f\"Found {recent_count} recently modified assets with default names\")\n",
    "    \n",
    "    if recent_count > 0:\n",
    "        display(df_recent.limit(20))\n",
    "elif execution_mode == 'job':\n",
    "    log(\"\\n✓ Job mode: Scan complete. Results exported to configured locations.\")\n",
    "    \n",
    "    # Create job completion summary\n",
    "    summary = {\n",
    "        'status': 'SUCCESS',\n",
    "        'completion_time': completion_time,\n",
    "        'execution_time_seconds': round(total_execution_time, 2),\n",
    "        'execution_time_minutes': round(total_execution_time / 60, 2),\n",
    "        'mode': 'JOB',\n",
    "        'data_collected': {\n",
    "            'total_assets': len(all_assets),\n",
    "            'notebooks': len(notebook_data),\n",
    "            'queries': len(query_data),\n",
    "            'dashboards': len(dashboard_data)\n",
    "        },\n",
    "        'execution_stats': execution_stats,\n",
    "        'exports': {\n",
    "            'delta_enabled': ENABLE_DELTA_EXPORT,\n",
    "            'delta_table': DELTA_TABLE_NAME if ENABLE_DELTA_EXPORT else None,\n",
    "            'excel_enabled': ENABLE_EXCEL_EXPORT\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Return summary for job orchestration\n",
    "    dbutils.notebook.exit(json.dumps(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "625af928-54ff-4610-b6db-fd449704ad83",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execution Statistics Summary"
    }
   },
   "outputs": [],
   "source": [
    "# Print detailed execution statistics\n",
    "print_execution_summary()\n",
    "\n",
    "# Additional statistics\n",
    "if df_assets is not None:\n",
    "    log(\"\\nData Quality Metrics:\")\n",
    "    \n",
    "    # Count assets with owner information\n",
    "    assets_with_owner = df_assets.filter(F.col(\"owner\").isNotNull() & (F.col(\"owner\") != \"\")).count()\n",
    "    assets_without_owner = df_assets.filter(F.col(\"owner\").isNull() | (F.col(\"owner\") == \"\")).count()\n",
    "    \n",
    "    log(f\"  Assets with owner info: {assets_with_owner} ({assets_with_owner/df_assets.count()*100:.1f}%)\")\n",
    "    log(f\"  Assets without owner info: {assets_without_owner} ({assets_without_owner/df_assets.count()*100:.1f}%)\")\n",
    "    \n",
    "    # Count by asset type\n",
    "    log(\"\\nAssets by Type:\")\n",
    "    type_counts = df_assets.groupBy(\"asset_type\").count().collect()\n",
    "    for row in type_counts:\n",
    "        log(f\"  {row['asset_type']}: {row['count']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0ca5a0c-e269-4670-969d-e9e605f39057",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Owner Analysis by Asset Type"
    }
   },
   "outputs": [],
   "source": [
    "if execution_mode == 'interactive' and df_assets is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"OWNER ANALYSIS BY ASSET TYPE\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Notebooks by owner\n",
    "    log(\"\\nTop 10 Notebook Owners:\")\n",
    "    df_notebook_owners = df_assets \\\n",
    "        .filter(F.col(\"asset_type\") == \"NOTEBOOK\") \\\n",
    "        .filter(F.col(\"owner\").isNotNull() & (F.col(\"owner\") != \"\")) \\\n",
    "        .groupBy(\"owner\") \\\n",
    "        .count() \\\n",
    "        .orderBy(F.col(\"count\").desc()) \\\n",
    "        .limit(10)\n",
    "    \n",
    "    display(df_notebook_owners)\n",
    "    \n",
    "    # Queries by owner\n",
    "    log(\"\\nTop 10 Query Owners:\")\n",
    "    df_query_owners = df_assets \\\n",
    "        .filter(F.col(\"asset_type\") == \"QUERY\") \\\n",
    "        .filter(F.col(\"owner\").isNotNull() & (F.col(\"owner\") != \"\")) \\\n",
    "        .groupBy(\"owner\") \\\n",
    "        .count() \\\n",
    "        .orderBy(F.col(\"count\").desc()) \\\n",
    "        .limit(10)\n",
    "    \n",
    "    if df_query_owners.count() > 0:\n",
    "        display(df_query_owners)\n",
    "    else:\n",
    "        log(\"  No query owner information available\")\n",
    "    \n",
    "    # Assets without owner (need attention)\n",
    "    log(\"\\nAssets Without Owner Information:\")\n",
    "    df_no_owner = df_assets \\\n",
    "        .filter(F.col(\"owner\").isNull() | (F.col(\"owner\") == \"\")) \\\n",
    "        .select(\"asset_type\", \"asset_name\", \"asset_path\") \\\n",
    "        .orderBy(\"asset_type\", \"asset_name\")\n",
    "    \n",
    "    no_owner_count = df_no_owner.count()\n",
    "    log(f\"Found {no_owner_count} assets without owner information\")\n",
    "    \n",
    "    if no_owner_count > 0:\n",
    "        display(df_no_owner.limit(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1749ab34-b055-4000-a7b3-3742f4e5d21a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Asset Age Distribution Analysis"
    }
   },
   "outputs": [],
   "source": [
    "if execution_mode == 'interactive' and df_assets is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"ASSET AGE DISTRIBUTION ANALYSIS\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    from pyspark.sql.functions import datediff, floor\n",
    "    \n",
    "    # Calculate age in days\n",
    "    df_with_age = df_assets \\\n",
    "        .filter(F.col(\"modified_timestamp\").isNotNull()) \\\n",
    "        .withColumn(\"age_days\", datediff(F.current_timestamp(), F.col(\"modified_timestamp\")))\n",
    "    \n",
    "    # Create age buckets\n",
    "    df_age_buckets = df_with_age.withColumn(\n",
    "        \"age_bucket\",\n",
    "        when(F.col(\"age_days\") <= 30, \"0-30 days\")\n",
    "        .when(F.col(\"age_days\") <= 90, \"31-90 days\")\n",
    "        .when(F.col(\"age_days\") <= 180, \"91-180 days\")\n",
    "        .when(F.col(\"age_days\") <= 365, \"181-365 days\")\n",
    "        .otherwise(\"365+ days\")\n",
    "    )\n",
    "    \n",
    "    log(\"\\nAge Distribution of Default-Named Assets:\")\n",
    "    age_distribution = df_age_buckets.groupBy(\"age_bucket\").count().orderBy(\"age_bucket\")\n",
    "    display(age_distribution)\n",
    "    \n",
    "    log(\"\\nAge Distribution by Asset Type:\")\n",
    "    age_by_type = df_age_buckets.groupBy(\"asset_type\", \"age_bucket\").count() \\\n",
    "        .orderBy(\"asset_type\", \"age_bucket\")\n",
    "    display(age_by_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f704a6ad-89e0-4bb2-8d9d-be64880b13d6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleanup Recommendations"
    }
   },
   "outputs": [],
   "source": [
    "if df_assets is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"CLEANUP RECOMMENDATIONS\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    from pyspark.sql.functions import datediff\n",
    "    \n",
    "    # Use configurable threshold from widget parameter\n",
    "    log(f\"\\nIdentifying stale assets (not modified in {STALE_THRESHOLD_DAYS}+ days)...\")\n",
    "    \n",
    "    df_stale = df_assets \\\n",
    "        .filter(F.col(\"modified_timestamp\").isNotNull()) \\\n",
    "        .withColumn(\"age_days\", datediff(F.current_timestamp(), F.col(\"modified_timestamp\"))) \\\n",
    "        .filter(F.col(\"age_days\") >= STALE_THRESHOLD_DAYS) \\\n",
    "        .select(\"asset_type\", \"asset_name\", \"owner\", \"asset_path\", \"age_days\", \"modified_timestamp\") \\\n",
    "        .orderBy(F.col(\"age_days\").desc())\n",
    "    \n",
    "    stale_count = df_stale.count()\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDDD1️ Stale Assets (not modified in {STALE_THRESHOLD_DAYS}+ days): {stale_count}\")\n",
    "    \n",
    "    if stale_count > 0:\n",
    "        log(f\"\\nRecommendation: Consider reviewing and renaming or deleting these {stale_count} assets.\")\n",
    "        log(f\"These assets have default names and haven't been modified in {STALE_THRESHOLD_DAYS}+ days.\")\n",
    "        \n",
    "        # Breakdown by type\n",
    "        stale_by_type = df_stale.groupBy(\"asset_type\").count().collect()\n",
    "        log(\"\\nStale assets by type:\")\n",
    "        for row in stale_by_type:\n",
    "            log(f\"  {row['asset_type']}: {row['count']}\")\n",
    "        \n",
    "        if execution_mode == 'interactive':\n",
    "            log(\"\\nTop 20 Oldest Stale Assets:\")\n",
    "            display(df_stale.limit(20))\n",
    "        \n",
    "        # Store for export\n",
    "        globals()['df_stale_assets'] = df_stale\n",
    "    else:\n",
    "        log(f\"\\n✓ No stale assets found - all default-named assets modified within {STALE_THRESHOLD_DAYS} days!\")\n",
    "        globals()['df_stale_assets'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4078c775-40db-4044-9288-ffe6f68ca2a6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to HTML Report"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE_HTML_EXPORT and df_assets is not None:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"\\nExporting to HTML report...\")\n",
    "    \n",
    "    # Create timestamp for filename\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "    html_path = f\"{EXPORT_PATH}/workspace_default_named_assets_{timestamp}.html\"\n",
    "    \n",
    "    try:\n",
    "        # Convert to Pandas\n",
    "        pdf_assets = df_assets.toPandas()\n",
    "        \n",
    "        # Build HTML report\n",
    "        html_content = f\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Workspace Default Naming Scanner Report</title>\n",
    "    <style>\n",
    "        body {{ font-family: Arial, sans-serif; margin: 20px; background-color: #f5f5f5; }}\n",
    "        h1 {{ color: #FF3621; }}\n",
    "        h2 {{ color: #333; border-bottom: 2px solid #FF3621; padding-bottom: 5px; }}\n",
    "        .summary {{ background-color: white; padding: 20px; border-radius: 5px; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "        .metric {{ display: inline-block; margin: 10px 20px; }}\n",
    "        .metric-label {{ font-weight: bold; color: #666; }}\n",
    "        .metric-value {{ font-size: 24px; color: #FF3621; }}\n",
    "        table {{ border-collapse: collapse; width: 100%; background-color: white; margin-bottom: 20px; box-shadow: 0 2px 4px rgba(0,0,0,0.1); }}\n",
    "        th {{ background-color: #FF3621; color: white; padding: 12px; text-align: left; }}\n",
    "        td {{ padding: 10px; border-bottom: 1px solid #ddd; }}\n",
    "        tr:hover {{ background-color: #f5f5f5; }}\n",
    "        .footer {{ text-align: center; color: #666; margin-top: 40px; font-size: 12px; }}\n",
    "        .config {{ background-color: #fff3cd; padding: 10px; border-radius: 5px; margin-bottom: 20px; }}\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>\uD83D\uDD0D Workspace Default Naming Scanner Report</h1>\n",
    "    \n",
    "    <div class=\"config\">\n",
    "        <strong>Configuration:</strong> Stale Asset Threshold = {STALE_THRESHOLD_DAYS} days | \n",
    "        Age Filter = {MIN_AGE_DAYS if MIN_AGE_DAYS else 'None'} | \n",
    "        Exclude Repos = {exclude_repos_param}\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"summary\">\n",
    "        <h2>Summary</h2>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Total Assets</div>\n",
    "            <div class=\"metric-value\">{len(all_assets)}</div>\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Notebooks</div>\n",
    "            <div class=\"metric-value\">{len(notebook_data)}</div>\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">SQL Queries</div>\n",
    "            <div class=\"metric-value\">{len(query_data)}</div>\n",
    "        </div>\n",
    "        <div class=\"metric\">\n",
    "            <div class=\"metric-label\">Dashboards</div>\n",
    "            <div class=\"metric-value\">{len(dashboard_data)}</div>\n",
    "        </div>\n",
    "        <p><strong>Scan Date:</strong> {datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')}</p>\n",
    "        <p><strong>Execution Time:</strong> {round((time.time() - execution_stats['start_time']) / 60, 2)} minutes</p>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"summary\">\n",
    "        <h2>Top 10 Users with Most Default-Named Assets</h2>\n",
    "        <table>\n",
    "            <tr><th>Owner</th><th>Count</th></tr>\n",
    "\"\"\"\n",
    "        \n",
    "        # Add top owners\n",
    "        owner_counts = pdf_assets[pdf_assets['owner'].notna() & (pdf_assets['owner'] != '')] \\\n",
    "            .groupby('owner').size().reset_index(name='count') \\\n",
    "            .sort_values('count', ascending=False).head(10)\n",
    "        \n",
    "        for _, row in owner_counts.iterrows():\n",
    "            html_content += f\"            <tr><td>{row['owner']}</td><td>{row['count']}</td></tr>\\n\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "        </table>\n",
    "    </div>\n",
    "    \n",
    "    <div class=\"summary\">\n",
    "        <h2>Recent Activity (Last 90 Days)</h2>\n",
    "        <table>\n",
    "            <tr><th>Asset Type</th><th>Asset Name</th><th>Owner</th><th>Days Since Modified</th></tr>\n",
    "\"\"\"\n",
    "        \n",
    "        # Add recent activity\n",
    "        recent_assets = pdf_assets[pdf_assets['modified_timestamp'].notna()].copy()\n",
    "        recent_assets['days_since_modified'] = (pd.Timestamp.now() - pd.to_datetime(recent_assets['modified_timestamp'])).dt.days\n",
    "        recent_assets = recent_assets[recent_assets['days_since_modified'] <= 90] \\\n",
    "            .sort_values('days_since_modified').head(20)\n",
    "        \n",
    "        for _, row in recent_assets.iterrows():\n",
    "            owner_display = row['owner'] if pd.notna(row['owner']) and row['owner'] != '' else 'Unknown'\n",
    "            html_content += f\"            <tr><td>{row['asset_type']}</td><td>{row['asset_name']}</td><td>{owner_display}</td><td>{row['days_since_modified']}</td></tr>\\n\"\n",
    "        \n",
    "        html_content += \"\"\"\n",
    "        </table>\n",
    "    </div>\n",
    "\"\"\"\n",
    "        \n",
    "        # Add stale assets section if available\n",
    "        if 'df_stale_assets' in globals() and df_stale_assets is not None:\n",
    "            pdf_stale = df_stale_assets.limit(20).toPandas()\n",
    "            stale_count = df_stale_assets.count()\n",
    "            html_content += f\"\"\"\n",
    "    <div class=\"summary\">\n",
    "        <h2>\uD83D\uDDD1️ Cleanup Recommendations (Stale Assets)</h2>\n",
    "        <p>Assets not modified in {STALE_THRESHOLD_DAYS}+ days: <strong>{stale_count}</strong></p>\n",
    "        <table>\n",
    "            <tr><th>Asset Type</th><th>Asset Name</th><th>Owner</th><th>Age (Days)</th><th>Path</th></tr>\n",
    "\"\"\"\n",
    "            for _, row in pdf_stale.iterrows():\n",
    "                owner_display = row['owner'] if pd.notna(row['owner']) and row['owner'] != '' else 'Unknown'\n",
    "                html_content += f\"            <tr><td>{row['asset_type']}</td><td>{row['asset_name']}</td><td>{owner_display}</td><td>{row['age_days']}</td><td>{row['asset_path']}</td></tr>\\n\"\n",
    "            \n",
    "            html_content += \"\"\"        </table>\n",
    "    </div>\n",
    "\"\"\"\n",
    "        \n",
    "        html_content += f\"\"\"\n",
    "    <div class=\"footer\">\n",
    "        <p>Generated by Databricks Workspace Default Naming Scanner</p>\n",
    "        <p>Report generated at {datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')}</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "        \n",
    "        # Write HTML file\n",
    "        html_file_dbfs = html_path.replace('/dbfs', 'dbfs:')\n",
    "        dbutils.fs.put(html_file_dbfs, html_content, overwrite=True)\n",
    "        \n",
    "        log(f\"✓ HTML report export successful: {html_path}\")\n",
    "        log(f\"  File size: {len(html_content) / 1024:.2f} KB\")\n",
    "        log(f\"  Sections: Summary, Top Owners, Recent Activity, Cleanup Recommendations\")\n",
    "        log_execution_time(\"HTML report export\", cell_start_time)\n",
    "    except Exception as e:\n",
    "        log(f\"✗ HTML report export failed: {e}\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "else:\n",
    "    if not ENABLE_HTML_EXPORT:\n",
    "        log(\"\\n⊘ HTML export disabled (ENABLE_HTML_EXPORT=False)\")\n",
    "    else:\n",
    "        log(\"\\n⊘ HTML export skipped (no data)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1559aba9-1328-4223-b1bc-8c1d049c5e62",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Send Email Notification (Job Mode)"
    }
   },
   "outputs": [],
   "source": [
    "if is_job_mode and ENABLE_EMAIL_NOTIFICATIONS and EMAIL_RECIPIENTS and df_assets is not None:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"\\nSending email notification...\")\n",
    "    \n",
    "    try:\n",
    "        # Build email body\n",
    "        email_subject = f\"Workspace Scan Complete - {len(all_assets)} Default-Named Assets Found\"\n",
    "        \n",
    "        email_body = f\"\"\"\n",
    "        <h2>Workspace Default Naming Scanner - Scan Complete</h2>\n",
    "        \n",
    "        <h3>Summary</h3>\n",
    "        <ul>\n",
    "            <li><strong>Total Assets Found:</strong> {len(all_assets)}</li>\n",
    "            <li><strong>Notebooks:</strong> {len(notebook_data)}</li>\n",
    "            <li><strong>SQL Queries:</strong> {len(query_data)}</li>\n",
    "            <li><strong>Dashboards:</strong> {len(dashboard_data)}</li>\n",
    "            <li><strong>Scan Date:</strong> {completion_time}</li>\n",
    "            <li><strong>Execution Time:</strong> {round(total_execution_time / 60, 2)} minutes</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Export Locations</h3>\n",
    "        <ul>\n",
    "            <li><strong>Delta Table:</strong> {DELTA_TABLE_NAME}</li>\n",
    "            <li><strong>Excel File:</strong> {EXPORT_PATH}/workspace_default_named_assets_*.xlsx</li>\n",
    "            <li><strong>HTML Report:</strong> {EXPORT_PATH}/workspace_default_named_assets_*.html</li>\n",
    "        </ul>\n",
    "        \n",
    "        <h3>Execution Statistics</h3>\n",
    "        <ul>\n",
    "            <li><strong>API Calls:</strong> {execution_stats['api_calls']}</li>\n",
    "            <li><strong>API Failures:</strong> {execution_stats['api_failures']}</li>\n",
    "            <li><strong>Success Rate:</strong> {round(((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls'] * 100), 1) if execution_stats['api_calls'] > 0 else 0}%</li>\n",
    "        </ul>\n",
    "        \n",
    "        <p>View the full report in the Delta table or download the Excel/HTML files from the export path.</p>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Send email using Databricks notification API\n",
    "        for recipient in EMAIL_RECIPIENTS:\n",
    "            try:\n",
    "                dbutils.notebook.run(\n",
    "                    \"/System/send_email\",  # Placeholder - adjust to your email sending mechanism\n",
    "                    timeout_seconds=60,\n",
    "                    arguments={\n",
    "                        \"to\": recipient,\n",
    "                        \"subject\": email_subject,\n",
    "                        \"body\": email_body\n",
    "                    }\n",
    "                )\n",
    "                log(f\"  ✓ Email sent to {recipient}\")\n",
    "            except Exception as email_error:\n",
    "                log(f\"  ⚠️ Failed to send email to {recipient}: {email_error}\")\n",
    "        \n",
    "        log_execution_time(\"Email notification\", cell_start_time)\n",
    "    except Exception as e:\n",
    "        log(f\"✗ Email notification failed: {e}\")\n",
    "else:\n",
    "    if not is_job_mode:\n",
    "        log(\"\\n⏭️ Email notifications only available in job mode\")\n",
    "    elif not ENABLE_EMAIL_NOTIFICATIONS:\n",
    "        log(\"\\n⊘ Email notifications disabled (ENABLE_EMAIL_NOTIFICATIONS=False)\")\n",
    "    elif not EMAIL_RECIPIENTS:\n",
    "        log(\"\\n⊘ Email notifications skipped (no recipients configured)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc7b7ce9-9076-448e-823a-718653882c3a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Download Instructions"
    }
   },
   "source": [
    "## Download Exported Files\n",
    "\n",
    "To download the exported files from DBFS to your local machine:\n",
    "\n",
    "### Option 1: Using Databricks CLI\n",
    "```bash\n",
    "# Download Excel report\n",
    "databricks fs cp dbfs:/tmp/workspace_scan_export/workspace_default_named_assets_*.xlsx ./\n",
    "\n",
    "# Download HTML report\n",
    "databricks fs cp dbfs:/tmp/workspace_scan_export/workspace_default_named_assets_*.html ./\n",
    "```\n",
    "\n",
    "### Option 2: Using the Workspace UI\n",
    "1. Navigate to **Data** → **DBFS** in the left sidebar\n",
    "2. Browse to `/tmp/workspace_scan_export/`\n",
    "3. Click on the file you want to download (Excel or HTML)\n",
    "4. Click the **Download** button\n",
    "\n",
    "### Option 3: Query the Delta Table\n",
    "If Delta export was successful, you can query the table directly:\n",
    "```sql\n",
    "SELECT * FROM main.default.workspace_default_named_assets\n",
    "ORDER BY modified_timestamp DESC\n",
    "```\n",
    "\n",
    "### Option 4: Download All Files\n",
    "```bash\n",
    "databricks fs cp -r dbfs:/tmp/workspace_scan_export/ ./workspace_scan_export/\n",
    "```\n",
    "\n",
    "### Option 5: View HTML Report in Browser\n",
    "After downloading the HTML file, simply open it in any web browser to view the formatted report with tables and styling."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Workspace Asset Scanner",
   "widgets": {
    "exclude_repos": {
     "currentValue": "Yes",
     "nuid": "6488a77c-ad26-4c1e-8311-481be35a5e1b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "Yes",
      "label": "Exclude Repos Folder",
      "name": "exclude_repos",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "Yes",
        "No"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "Yes",
      "label": "Exclude Repos Folder",
      "name": "exclude_repos",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "Yes",
        "No"
       ]
      }
     }
    },
    "execution_mode": {
     "currentValue": "interactive",
     "nuid": "19217b27-ea04-4bfb-8ca1-938dbae9469a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "interactive",
      "label": "Execution Mode",
      "name": "execution_mode",
      "options": {
       "widgetDisplayType": "Dropdown",
       "choices": [
        "interactive",
        "job"
       ],
       "fixedDomain": true,
       "multiselect": false
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "dropdown",
      "defaultValue": "interactive",
      "label": "Execution Mode",
      "name": "execution_mode",
      "options": {
       "widgetType": "dropdown",
       "autoCreated": false,
       "choices": [
        "interactive",
        "job"
       ]
      }
     }
    },
    "min_age_days": {
     "currentValue": "",
     "nuid": "8694f562-2f32-4abc-959f-00fc0acb98a0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "Min Age Days (leave empty for all)",
      "name": "min_age_days",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "Min Age Days (leave empty for all)",
      "name": "min_age_days",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "output_catalog": {
     "currentValue": "main",
     "nuid": "b3fee958-246a-445b-80fd-58820cdf7aca",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "main",
      "label": "Output Catalog",
      "name": "output_catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "main",
      "label": "Output Catalog",
      "name": "output_catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "output_schema": {
     "currentValue": "default",
     "nuid": "0610726f-66f1-42d9-a61d-12dfbdd9ea75",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "Output Schema",
      "name": "output_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": "Output Schema",
      "name": "output_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "stale_threshold_days": {
     "currentValue": "180",
     "nuid": "43826ac4-4f8e-48b9-b53d-a101fec48b48",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "180",
      "label": "Stale Asset Threshold (days)",
      "name": "stale_threshold_days",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "180",
      "label": "Stale Asset Threshold (days)",
      "name": "stale_threshold_days",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
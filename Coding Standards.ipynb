{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0df81bb5-57fa-4cdb-a7b7-07fdfe77bedb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Coding Standards Documentation Header"
    }
   },
   "source": [
    "# Coding Standards Documentation\n",
    "## Databricks Workspace Audit Notebooks\n",
    "\n",
    "This document outlines the coding standards, conventions, and best practices used across Databricks workspace audit notebooks including the Workspace Asset Scanner, Security Review Export, and Workspace Features Audit. These standards ensure consistency, maintainability, and enterprise-grade quality.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "### \uD83D\uDCDA Foundational Standards (Start Here)\n",
    "1. [Code Organization](#code-organization) - Cell structure, ordering, logical grouping\n",
    "2. [Import Organization](#import-organization) - Import order, aliases, package installation *(See section 20)*\n",
    "3. [Naming Conventions](#naming-conventions) - Variables, functions, constants, DataFrames\n",
    "4. [Documentation Standards](#documentation-standards) - Docstrings, comments, markdown *(Includes code comments)*\n",
    "5. [Configuration Management](#configuration-management) - Structure, widgets, validation\n",
    "\n",
    "### \uD83D\uDD27 Core Patterns (Essential Techniques)\n",
    "6. [Error Handling](#error-handling) - Try-except, graceful degradation, validation\n",
    "7. [Security and Secrets Management](#security-and-secrets-management) - Tokens, secrets, sanitization *(See section 21)*\n",
    "8. [Logging and Output](#logging-and-output) - Centralized logging, formatting *(Includes string formatting from section 24)*\n",
    "9. [Data Processing](#data-processing) - Data structures, filtering, timestamps *(Includes Spark DataFrame best practices from section 25)*\n",
    "10. [Testing and Validation](#testing-and-validation) - DataFrame validation, config checks *(Includes data quality from section 23)*\n",
    "\n",
    "### \uD83D\uDD0C Integration & APIs\n",
    "11. [API Integration](#api-integration) - REST API patterns, retry logic, pagination\n",
    "12. [Databricks SDK Integration](#databricks-sdk-integration) - WorkspaceClient, SDK patterns\n",
    "13. [Parallel Processing](#parallel-processing-with-threadpoolexecutor) - ThreadPoolExecutor, concurrent operations *(See section 19)*\n",
    "\n",
    "### ⚡ Advanced Patterns (Environment & Optimization)\n",
    "14. [Compute Type Detection](#compute-type-detection-and-optimization) - Serverless vs traditional *(See section 12)*\n",
    "15. [Job Mode and Widget Parameters](#job-mode-and-widget-parameters) - Job detection, overrides *(See section 13)*\n",
    "16. [Execution Mode Patterns](#execution-mode-patterns) - Quick/Full modes, feature flags *(See section 18)*\n",
    "17. [Performance Optimization](#performance-optimization) - Execution tracking, memory monitoring *(See section 9)*\n",
    "\n",
    "### \uD83D\uDCCA Analysis & Reporting\n",
    "18. [Health Scoring and Risk Assessment](#health-scoring-and-risk-assessment) - Scoring systems, risk factors *(See section 14)*\n",
    "19. [Recommendation Generation](#recommendation-generation) - Structured recommendations, priorities *(See section 15)*\n",
    "20. [Visualization Standards](#visualization-standards) - Matplotlib, charts, conditional display *(See section 16)*\n",
    "21. [Export Format Flexibility](#export-format-flexibility) - Excel, CSV, JSON, Delta *(See section 17)*\n",
    "\n",
    "### ✅ Summary\n",
    "22. [Summary: Key Best Practices](#summary-key-best-practices) - Checklist, quick reference\n",
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDCD6 Reading Guide\n",
    "\n",
    "**For New Developers**: Read sections 1-10 (Foundational + Core Patterns)  \n",
    "**For API Integration**: Focus on sections 11-13  \n",
    "**For Performance Tuning**: Read sections 14-17  \n",
    "**For Reporting Features**: Read sections 18-21  \n",
    "\n",
    "**Note**: Some sections appear out of numerical order in the notebook but are cross-referenced above for logical reading flow.\n",
    "\n",
    "---\n",
    "\n",
    "## Version Control\n",
    "\n",
    "| Version | Date | Author | Changes |\n",
    "|---------|------|--------|---------|  \n",
    "| 1.0.0 | 2026-02-16 | Assistant | Comprehensive coding standards documentation with 22 conceptual sections organized into 5 logical categories: Foundational Standards (code organization, imports, naming, documentation, configuration), Core Patterns (error handling, security, logging, data processing, testing), Integration & APIs (REST API, Databricks SDK, parallel processing), Advanced Patterns (compute detection, job mode, execution modes, performance optimization), and Analysis & Reporting (health scoring, recommendations, visualizations, exports). Includes merged topics for better cohesion and cross-references for navigation. |\n",
    "\n",
    "---\n",
    "\n",
    "## Scope\n",
    "\n",
    "These standards apply to:\n",
    "* **Workspace Asset Scanner** - Default naming convention detection\n",
    "* **Security Review Export** - Comprehensive security audit\n",
    "* **Workspace Features Audit** - Feature inventory and configuration\n",
    "* **Other workspace audit and governance notebooks**\n",
    "\n",
    "---\n",
    "\n",
    "## Key Principles\n",
    "\n",
    "✓ **Consistency**: Use the same patterns across all notebooks  \n",
    "✓ **Maintainability**: Write self-documenting code with clear structure  \n",
    "✓ **Enterprise-Grade**: Include error handling, logging, and validation  \n",
    "✓ **Performance**: Optimize for both serverless and traditional compute  \n",
    "✓ **Flexibility**: Support multiple execution modes and export formats  \n",
    "✓ **Security**: Never hardcode credentials, use secrets management  \n",
    "✓ **Documentation**: Comprehensive inline and notebook-level documentation  \n",
    "✓ **Data Quality**: Validate inputs, handle nulls, check for duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c401c01f-aec1-4506-a89c-0bc150b9bcd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1. Code Organization\n",
    "\n",
    "### Cell Structure\n",
    "\n",
    "**Principle**: One logical unit per cell with clear, descriptive titles\n",
    "\n",
    "* **Cell Titles**: Use descriptive display names that clearly indicate purpose\n",
    "  * Good: `\"Scan Notebooks\"`, `\"API Helper Functions\"`, `\"Export to Delta Table\"`\n",
    "  * Avoid: `\"Cell 1\"`, `\"Code\"`, `\"Untitled\"`\n",
    "\n",
    "* **Cell Ordering**:\n",
    "  1. Documentation header (markdown)\n",
    "  2. Setup and configuration\n",
    "  3. Helper functions\n",
    "  4. Data collection/scanning cells\n",
    "  5. Data processing and consolidation\n",
    "  6. Export and reporting\n",
    "  7. Analysis and visualization\n",
    "\n",
    "### Logical Grouping\n",
    "\n",
    "* Group related functionality together\n",
    "* Use markdown cells as section dividers\n",
    "* Keep cells focused on a single responsibility\n",
    "* Typical cell size: 50-150 lines (exceptions for complex logic)\n",
    "\n",
    "### Example Structure\n",
    "\n",
    "```python\n",
    "# ============================================================================\n",
    "# SECTION HEADER: Brief description\n",
    "# ============================================================================\n",
    "\n",
    "# Implementation code here\n",
    "\n",
    "# ============================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93c74b77-f83c-4c86-9de6-2815b79817af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2. Naming Conventions\n",
    "\n",
    "### Variables\n",
    "\n",
    "**Constants** (Configuration values):\n",
    "* `UPPER_SNAKE_CASE` for all constants\n",
    "* Examples: `MAX_RETRIES`, `ENABLE_DELTA_EXPORT`, `EXPORT_PATH`\n",
    "\n",
    "**Variables**:\n",
    "* `snake_case` for all variables\n",
    "* Examples: `api_url`, `api_token`, `all_notebooks`, `page_count`\n",
    "\n",
    "**Data Collections**:\n",
    "* Plural nouns for lists/collections: `notebooks`, `queries`, `dashboards`\n",
    "* Suffix `_data` for processed results: `notebook_data`, `query_data`\n",
    "* Prefix `all_` for complete collections: `all_notebooks`, `all_queries`\n",
    "\n",
    "**DataFrames**:\n",
    "* Prefix with `df_`: `df_assets`, `df_stale`\n",
    "* Use descriptive names: `df_stale_assets` not `df1`\n",
    "\n",
    "### Functions\n",
    "\n",
    "* `snake_case` for all function names\n",
    "* Use verb-noun pattern: `get_api_client()`, `validate_config()`, `log_execution_time()`\n",
    "* Boolean functions start with `is_`, `has_`, `should_`: `is_default_name`\n",
    "\n",
    "### API-Related\n",
    "\n",
    "* Consistent naming for API variables:\n",
    "  * `api_url` - Base API URL\n",
    "  * `api_token` - Authentication token\n",
    "  * `headers` - Request headers dictionary\n",
    "  * `response` - API response object\n",
    "  * `page_token` - Pagination token\n",
    "\n",
    "### Examples\n",
    "\n",
    "```python\n",
    "# Good\n",
    "MAX_RETRIES = 3\n",
    "api_url, api_token = get_api_client()\n",
    "all_notebooks = []\n",
    "df_assets = spark.createDataFrame(all_assets, schema)\n",
    "\n",
    "# Avoid\n",
    "maxRetries = 3  # Wrong case\n",
    "url, token = get_client()  # Not descriptive\n",
    "notebooks_list = []  # Redundant suffix\n",
    "df = spark.createDataFrame(data)  # Not descriptive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7db0dcd6-b0ee-4e5b-bcad-5b51dc187ce5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "3. Documentation Standards"
    }
   },
   "source": [
    "## 3. Documentation Standards\n",
    "\n",
    "### Notebook-Level Documentation\n",
    "\n",
    "**Required Elements**:\n",
    "1. Title and overview\n",
    "2. Feature list with categories\n",
    "3. Version control table\n",
    "4. Configuration documentation\n",
    "5. Usage instructions\n",
    "6. Asset types or data sources table\n",
    "\n",
    "### Docstrings\n",
    "\n",
    "**Functions**: Use triple-quoted docstrings\n",
    "\n",
    "```python\n",
    "def get_api_client():\n",
    "    \"\"\"Get Databricks API client configuration\"\"\"\n",
    "    # Implementation\n",
    "\n",
    "def api_call_with_retry(func, *args, **kwargs):\n",
    "    \"\"\"Execute API call with retry logic and stats tracking\"\"\"\n",
    "    # Implementation\n",
    "\n",
    "def validate_dataframe_exists(df_name, df):\n",
    "    \"\"\"Validate that a DataFrame exists and has data\"\"\"\n",
    "    # Implementation\n",
    "```\n",
    "\n",
    "### Inline Comments\n",
    "\n",
    "**Section Headers**: Use banner-style comments\n",
    "\n",
    "```python\n",
    "# ============================================================================\n",
    "# PERFORMANCE CONFIGURATION\n",
    "# ============================================================================\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2\n",
    "# ============================================================================\n",
    "```\n",
    "\n",
    "**Code Comments - Comment the WHY, not the WHAT**:\n",
    "* Explain *why*, not *what*\n",
    "* Place above the code block\n",
    "* Use complete sentences\n",
    "\n",
    "```python\n",
    "# Good: Explains reasoning\n",
    "# Convert timestamps from milliseconds to datetime if present\n",
    "# Databricks APIs return timestamps in epoch milliseconds\n",
    "if created_at:\n",
    "    created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "\n",
    "# Avoid: States the obvious\n",
    "# Convert timestamp\n",
    "if created_at:  # if created_at exists\n",
    "    created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "```\n",
    "\n",
    "### Complex Logic Comments\n",
    "\n",
    "**Explain complex algorithms**:\n",
    "\n",
    "```python\n",
    "# Calculate risk score using weighted factors:\n",
    "# - High-risk items: +20 points each (security vulnerabilities)\n",
    "# - Medium-risk items: +10 points each (best practice violations)\n",
    "# - Low-risk items: +5 points each (optimization opportunities)\n",
    "# Score is capped at 100 to maintain consistent scale\n",
    "risk_score = 0\n",
    "for factor in risk_factors:\n",
    "    if factor['severity'] == 'HIGH':\n",
    "        risk_score += 20\n",
    "    elif factor['severity'] == 'MEDIUM':\n",
    "        risk_score += 10\n",
    "    else:\n",
    "        risk_score += 5\n",
    "risk_score = min(risk_score, 100)\n",
    "```\n",
    "\n",
    "### TODO and FIXME Comments\n",
    "\n",
    "**Use standard markers for future work**:\n",
    "\n",
    "```python\n",
    "# TODO: Add support for custom date ranges\n",
    "# TODO: Implement incremental refresh for large datasets\n",
    "# TODO(username): Review performance optimization for serverless\n",
    "\n",
    "# FIXME: Handle edge case where user has no groups\n",
    "# HACK: Temporary workaround for API pagination bug\n",
    "```\n",
    "\n",
    "### Deprecation Warnings\n",
    "\n",
    "**Document deprecated code**:\n",
    "\n",
    "```python\n",
    "def old_function():\n",
    "    \"\"\"\n",
    "    DEPRECATED: Use new_function() instead.\n",
    "    This function will be removed in version 2.0.\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    warnings.warn(\n",
    "        \"old_function is deprecated, use new_function instead\",\n",
    "        DeprecationWarning,\n",
    "        stacklevel=2\n",
    "    )\n",
    "    # ... implementation ...\n",
    "```\n",
    "\n",
    "### Markdown Formatting\n",
    "\n",
    "* Use `**bold**` for emphasis\n",
    "* Use `*italic*` for feature names\n",
    "* Use `` `code` `` for inline code/variables\n",
    "* Use `✓`, `✗`, `⚠️`, `\uD83D\uDCCA`, `\uD83D\uDE80` emojis sparingly for visual clarity\n",
    "* Use tables for structured information\n",
    "* Use horizontal rules (`---`) to separate major sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55c50690-bda5-4669-a6cb-71d08dd6a5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4. Configuration Management\n",
    "\n",
    "### Configuration Structure\n",
    "\n",
    "**Organize configurations into logical sections**:\n",
    "\n",
    "```python\n",
    "# ============================================================================\n",
    "# PERFORMANCE CONFIGURATION\n",
    "# ============================================================================\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2\n",
    "MAX_WORKERS = 10\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT PATH CONFIGURATION\n",
    "# ============================================================================\n",
    "EXPORT_PATH = '/dbfs/tmp/workspace_scan_export'\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# FEATURE FLAGS: Enable/disable features\n",
    "# ============================================================================\n",
    "ENABLE_EXCEL_EXPORT = True\n",
    "ENABLE_HTML_EXPORT = True\n",
    "ENABLE_DELTA_EXPORT = True\n",
    "# ============================================================================\n",
    "```\n",
    "\n",
    "### Widget Parameters\n",
    "\n",
    "**Pattern**: Check for job mode, then get widget values with defaults\n",
    "\n",
    "```python\n",
    "# Detect if running in job mode or interactive mode\n",
    "try:\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().currentRunId().isDefined()\n",
    "    is_job_mode = True\n",
    "except:\n",
    "    is_job_mode = False\n",
    "\n",
    "# Get parameters from widgets\n",
    "if not is_job_mode:\n",
    "    execution_mode = dbutils.widgets.get(\"execution_mode\")\n",
    "    output_catalog = dbutils.widgets.get(\"output_catalog\")\n",
    "else:\n",
    "    execution_mode = 'job'\n",
    "    output_catalog = 'main'\n",
    "```\n",
    "\n",
    "### Configuration Dictionaries\n",
    "\n",
    "**Use dictionaries for related configuration**:\n",
    "\n",
    "```python\n",
    "DEFAULT_PATTERNS = {\n",
    "    'notebooks': ['Untitled Notebook', 'Untitled', 'New Notebook'],\n",
    "    'queries': ['Untitled Query', 'New Query', 'Untitled'],\n",
    "    'dashboards': ['New Dashboard', 'Untitled Dashboard', 'Untitled']\n",
    "}\n",
    "\n",
    "execution_stats = {\n",
    "    'start_time': time.time(),\n",
    "    'api_calls': 0,\n",
    "    'api_failures': 0,\n",
    "    'api_retries': 0\n",
    "}\n",
    "```\n",
    "\n",
    "### Validation\n",
    "\n",
    "**Always validate configuration before execution**:\n",
    "\n",
    "```python\n",
    "def validate_config():\n",
    "    \"\"\"Validate configuration parameters\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    if not isinstance(MAX_RETRIES, int) or MAX_RETRIES < 1:\n",
    "        errors.append(\"MAX_RETRIES must be a positive integer\")\n",
    "    \n",
    "    if ENABLE_DELTA_EXPORT:\n",
    "        if not re.match(r'^[a-zA-Z0-9_]+\\.[a-zA-Z0-9_]+\\.[a-zA-Z0-9_]+$', DELTA_TABLE_NAME):\n",
    "            errors.append(f\"DELTA_TABLE_NAME must be in format 'catalog.schema.table'\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "config_errors = validate_config()\n",
    "if config_errors:\n",
    "    error_msg = \"Configuration validation failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in config_errors)\n",
    "    raise ValueError(error_msg)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "837082ff-324c-4f54-95ec-7eb5b3def2a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5. Error Handling\n",
    "\n",
    "### Try-Except Patterns\n",
    "\n",
    "**Graceful Degradation**: Continue execution when possible\n",
    "\n",
    "```python\n",
    "# Pattern 1: Return safe defaults on error\n",
    "api_url, api_token = get_api_client()\n",
    "if not api_url or not api_token:\n",
    "    log(\"  ✗ Failed to get API client\")\n",
    "    notebooks = []\n",
    "else:\n",
    "    # Proceed with API calls\n",
    "    pass\n",
    "\n",
    "# Pattern 2: Catch and log, continue processing\n",
    "try:\n",
    "    # Risky operation\n",
    "    response = requests.get(url, headers=headers, timeout=30)\n",
    "except Exception as e:\n",
    "    log(f\"  ✗ Error fetching data: {str(e)}\")\n",
    "    all_data = []\n",
    "```\n",
    "\n",
    "**Timestamp Conversion**: Handle multiple formats\n",
    "\n",
    "```python\n",
    "# Handle different timestamp formats (milliseconds or ISO string)\n",
    "if created_at:\n",
    "    try:\n",
    "        if isinstance(created_at, (int, float)):\n",
    "            created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "        else:\n",
    "            created_timestamp = datetime.fromisoformat(str(created_at).replace('Z', '+00:00')).astimezone(eastern)\n",
    "    except:\n",
    "        pass  # Silently fail, timestamp remains None\n",
    "```\n",
    "\n",
    "### Validation Checks\n",
    "\n",
    "**Check for None/Empty before processing**:\n",
    "\n",
    "```python\n",
    "if df_assets is not None:\n",
    "    # Process DataFrame\n",
    "    pass\n",
    "else:\n",
    "    log(\"⚠️  No assets found, skipping export\")\n",
    "\n",
    "if len(all_assets) == 0:\n",
    "    log(\"\\n✓ No assets with default naming found!\")\n",
    "    df_assets = None\n",
    "else:\n",
    "    # Create DataFrame\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Error Messages\n",
    "\n",
    "**Use consistent symbols**:\n",
    "* `✓` - Success\n",
    "* `✗` - Failure\n",
    "* `⚠️` - Warning\n",
    "\n",
    "```python\n",
    "log(\"  ✓ Fetched 150 notebooks\")\n",
    "log(\"  ✗ Failed to fetch queries: 404\")\n",
    "log(\"  ⚠️ Warning: Export path may not be writable\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e442d749-f0ef-4d19-ba75-5820c70b17a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6. Logging and Output\n",
    "\n",
    "### Logging Function\n",
    "\n",
    "**Centralized logging with mode awareness**:\n",
    "\n",
    "```python\n",
    "def log(message):\n",
    "    \"\"\"Print messages (always in interactive, selectively in job mode)\"\"\"\n",
    "    if not is_job_mode:\n",
    "        print(message)\n",
    "    else:\n",
    "        print(message)  # Also print in job mode for logs\n",
    "```\n",
    "\n",
    "### Log Message Patterns\n",
    "\n",
    "**Section Headers**:\n",
    "```python\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"CREATING CONSOLIDATED DATAFRAME\")\n",
    "log(\"=\"*60)\n",
    "```\n",
    "\n",
    "**Progress Updates**:\n",
    "```python\n",
    "log(\"Fetching notebooks...\")\n",
    "log(f\"  Fetched {len(all_notebooks)} notebooks so far...\")\n",
    "log(f\"  ✓ Fetched {len(all_notebooks)} total notebooks\")\n",
    "```\n",
    "\n",
    "**Execution Timing**:\n",
    "```python\n",
    "def log_execution_time(cell_name, start_time):\n",
    "    \"\"\"Log execution time for a cell\"\"\"\n",
    "    elapsed = time.time() - start_time\n",
    "    log(f\"⏱️  {cell_name} completed in {elapsed:.2f} seconds\")\n",
    "\n",
    "# Usage\n",
    "cell_start_time = time.time()\n",
    "# ... cell logic ...\n",
    "log_execution_time(\"Scan Notebooks\", cell_start_time)\n",
    "```\n",
    "\n",
    "**Summary Reports**:\n",
    "```python\n",
    "log(f\"Total assets with default naming: {len(all_assets)}\")\n",
    "log(f\"  - Notebooks: {len(notebook_data)}\")\n",
    "log(f\"  - SQL Queries: {len(query_data)}\")\n",
    "log(f\"  - Dashboards: {len(dashboard_data)}\")\n",
    "```\n",
    "\n",
    "### Display vs Print\n",
    "\n",
    "**Use `display()` for DataFrames**:\n",
    "```python\n",
    "# Good\n",
    "display(df_assets.limit(10))\n",
    "\n",
    "# Avoid\n",
    "print(df_assets.limit(10))  # Poor formatting\n",
    "df_assets.limit(10)  # May not render in all contexts\n",
    "```\n",
    "\n",
    "### F-Strings\n",
    "\n",
    "**Always use f-strings for string formatting**:\n",
    "```python\n",
    "# Good\n",
    "log(f\"Fetched {count} items in {elapsed:.2f} seconds\")\n",
    "log(f\"Export path: {EXPORT_PATH}\")\n",
    "\n",
    "# Avoid\n",
    "log(\"Fetched \" + str(count) + \" items\")  # Concatenation\n",
    "log(\"Fetched %d items\" % count)  # Old-style formatting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5fc76616-1652-4007-8975-d52b3329cc56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 7. API Integration\n",
    "\n",
    "### API Client Pattern\n",
    "\n",
    "**Centralized API configuration**:\n",
    "\n",
    "```python\n",
    "def get_api_client():\n",
    "    \"\"\"Get Databricks API client configuration\"\"\"\n",
    "    try:\n",
    "        ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "        api_url = ctx.apiUrl().get()\n",
    "        api_token = ctx.apiToken().get()\n",
    "        return api_url, api_token\n",
    "    except Exception as e:\n",
    "        log(f\"Error getting API client: {e}\")\n",
    "        return None, None\n",
    "```\n",
    "\n",
    "### Retry Logic\n",
    "\n",
    "**Implement exponential backoff**:\n",
    "\n",
    "```python\n",
    "def api_call_with_retry(func, *args, **kwargs):\n",
    "    \"\"\"Execute API call with retry logic and stats tracking\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            execution_stats['api_calls'] += 1\n",
    "            response = func(*args, **kwargs)\n",
    "            if response and response.status_code == 200:\n",
    "                return response\n",
    "            else:\n",
    "                execution_stats['api_failures'] += 1\n",
    "                if attempt < MAX_RETRIES - 1:\n",
    "                    execution_stats['api_retries'] += 1\n",
    "                    time.sleep(RETRY_DELAY * (2 ** attempt))  # Exponential backoff\n",
    "        except Exception as e:\n",
    "            execution_stats['api_failures'] += 1\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                execution_stats['api_retries'] += 1\n",
    "                time.sleep(RETRY_DELAY * (2 ** attempt))\n",
    "    return None\n",
    "```\n",
    "\n",
    "### Pagination Pattern\n",
    "\n",
    "**Standard pagination loop**:\n",
    "\n",
    "```python\n",
    "all_items = []\n",
    "page_token = None\n",
    "page_count = 0\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        params = {\"page_size\": 100}\n",
    "        if page_token:\n",
    "            params[\"page_token\"] = page_token\n",
    "        \n",
    "        def fetch_items():\n",
    "            return requests.get(\n",
    "                f\"{api_url}/api/2.0/endpoint\",\n",
    "                headers=headers,\n",
    "                params=params,\n",
    "                timeout=30\n",
    "            )\n",
    "        \n",
    "        response = api_call_with_retry(fetch_items)\n",
    "        \n",
    "        if response and response.status_code == 200:\n",
    "            data = response.json()\n",
    "            items = data.get('items', [])\n",
    "            all_items.extend(items)\n",
    "            page_count += 1\n",
    "            \n",
    "            # Progress logging every 5 pages\n",
    "            if page_count % 5 == 0:\n",
    "                log(f\"  Fetched {len(all_items)} items so far...\")\n",
    "            \n",
    "            # Check for next page\n",
    "            page_token = data.get('next_page_token')\n",
    "            if not page_token:\n",
    "                break\n",
    "        else:\n",
    "            log(f\"  ✗ Failed to fetch items: {response.status_code if response else 'No response'}\")\n",
    "            break\n",
    "            \n",
    "except Exception as e:\n",
    "    log(f\"  ✗ Error fetching items: {str(e)}\")\n",
    "    all_items = []\n",
    "```\n",
    "\n",
    "### Request Headers\n",
    "\n",
    "**Consistent header structure**:\n",
    "\n",
    "```python\n",
    "# Standard headers\n",
    "headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\n",
    "# For POST requests with JSON\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_token}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1438488-907c-49c2-8ced-f9ca64db263a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 8. Data Processing\n",
    "\n",
    "### Data Structure Pattern\n",
    "\n",
    "**Consistent dictionary structure for all asset types**:\n",
    "\n",
    "```python\n",
    "assets.append({\n",
    "    'asset_type': 'notebook',\n",
    "    'asset_name': notebook_name,\n",
    "    'asset_id': str(notebook_id),\n",
    "    'asset_path': notebook_path,\n",
    "    'owner': owner,\n",
    "    'created_timestamp': created_timestamp,\n",
    "    'modified_timestamp': modified_timestamp\n",
    "})\n",
    "```\n",
    "\n",
    "### DataFrame Creation\n",
    "\n",
    "**Define schema explicitly**:\n",
    "\n",
    "```python\n",
    "schema = StructType([\n",
    "    StructField(\"asset_type\", StringType(), True),\n",
    "    StructField(\"asset_name\", StringType(), True),\n",
    "    StructField(\"asset_id\", StringType(), True),\n",
    "    StructField(\"asset_path\", StringType(), True),\n",
    "    StructField(\"owner\", StringType(), True),\n",
    "    StructField(\"created_timestamp\", TimestampType(), True),\n",
    "    StructField(\"modified_timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "df_assets = spark.createDataFrame(all_assets, schema)\n",
    "```\n",
    "\n",
    "### Filtering Pattern\n",
    "\n",
    "**Apply filters consistently**:\n",
    "\n",
    "```python\n",
    "# Check if name matches default patterns\n",
    "is_default_name = any(\n",
    "    pattern.lower() in asset_name.lower() \n",
    "    for pattern in DEFAULT_PATTERNS.get('asset_type', [])\n",
    ")\n",
    "\n",
    "if is_default_name:\n",
    "    # Apply age filter if configured\n",
    "    if MIN_AGE_DAYS and modified_timestamp:\n",
    "        age_days = (datetime.now(eastern) - modified_timestamp).days\n",
    "        if age_days < MIN_AGE_DAYS:\n",
    "            execution_stats['resources_skipped'] += 1\n",
    "            continue\n",
    "    \n",
    "    # Apply incremental scan filter if enabled\n",
    "    if last_scan_timestamp and modified_timestamp:\n",
    "        if modified_timestamp <= last_scan_timestamp:\n",
    "            execution_stats['resources_skipped'] += 1\n",
    "            continue\n",
    "    \n",
    "    # Add to results\n",
    "    assets.append({...})\n",
    "```\n",
    "\n",
    "### Timestamp Handling\n",
    "\n",
    "**Consistent timezone conversion**:\n",
    "\n",
    "```python\n",
    "# Define timezone once\n",
    "eastern = pytz.timezone('America/New_York')\n",
    "\n",
    "# Convert from milliseconds\n",
    "if created_at:\n",
    "    try:\n",
    "        created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Use created as fallback for modified\n",
    "if not modified_timestamp and created_timestamp:\n",
    "    modified_timestamp = created_timestamp\n",
    "```\n",
    "\n",
    "### Deduplication\n",
    "\n",
    "**Remove duplicates based on composite key**:\n",
    "\n",
    "```python\n",
    "# Deduplicate based on asset_type + asset_id\n",
    "initial_count = df_assets.count()\n",
    "df_assets = df_assets.dropDuplicates([\"asset_type\", \"asset_id\"])\n",
    "final_count = df_assets.count()\n",
    "\n",
    "if initial_count > final_count:\n",
    "    log(f\"\\n⚠️  Removed {initial_count - final_count} duplicate entries\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c309e46-a36f-408c-9d3e-e18cd0739345",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 9. Performance Optimization\n",
    "\n",
    "### Execution Tracking\n",
    "\n",
    "**Track performance metrics**:\n",
    "\n",
    "```python\n",
    "execution_stats = {\n",
    "    'start_time': time.time(),\n",
    "    'api_calls': 0,\n",
    "    'api_failures': 0,\n",
    "    'api_retries': 0,\n",
    "    'resources_processed': 0,\n",
    "    'resources_skipped': 0,\n",
    "    'memory_usage_mb': 0\n",
    "}\n",
    "\n",
    "# Update throughout execution\n",
    "execution_stats['resources_processed'] += len(all_items)\n",
    "execution_stats['resources_skipped'] += 1\n",
    "```\n",
    "\n",
    "**Memory Monitoring**:\n",
    "\n",
    "```python\n",
    "def get_memory_usage():\n",
    "    \"\"\"Get current memory usage in MB\"\"\"\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process()\n",
    "        return process.memory_info().rss / 1024 / 1024\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# Track memory deltas\n",
    "memory_before = get_memory_usage()\n",
    "# ... operations ...\n",
    "memory_after = get_memory_usage()\n",
    "memory_delta = memory_after - memory_before\n",
    "execution_stats['memory_usage_mb'] = max(execution_stats['memory_usage_mb'], memory_after)\n",
    "```\n",
    "\n",
    "### Cell Timing\n",
    "\n",
    "**Time each major operation**:\n",
    "\n",
    "```python\n",
    "cell_start_time = time.time()\n",
    "\n",
    "# ... cell logic ...\n",
    "\n",
    "log_execution_time(\"Cell Name\", cell_start_time)\n",
    "```\n",
    "\n",
    "### Progress Reporting\n",
    "\n",
    "**Report progress for long operations**:\n",
    "\n",
    "```python\n",
    "if page_count % 5 == 0:\n",
    "    log(f\"  Fetched {len(all_items)} items so far...\")\n",
    "```\n",
    "\n",
    "### Performance Presets\n",
    "\n",
    "**Provide configurable performance modes**:\n",
    "\n",
    "```python\n",
    "if USE_QUICK_MODE:\n",
    "    MAX_NOTEBOOKS_FOR_OWNER_LOOKUP = 0\n",
    "    MAX_WORKSPACE_SCAN_LIMIT = 1000\n",
    "    ENABLE_SHARED_FOLDER_OWNER_LOOKUP = False\n",
    "    \n",
    "elif USE_FULL_MODE:\n",
    "    MAX_NOTEBOOKS_FOR_OWNER_LOOKUP = 999\n",
    "    MAX_WORKSPACE_SCAN_LIMIT = 999999\n",
    "    ENABLE_SHARED_FOLDER_OWNER_LOOKUP = True\n",
    "```\n",
    "\n",
    "### Conditional Processing\n",
    "\n",
    "**Skip expensive operations when not needed**:\n",
    "\n",
    "```python\n",
    "if ENABLE_EXCEL_EXPORT and df_assets is not None:\n",
    "    # Export to Excel\n",
    "    pass\n",
    "\n",
    "if execution_mode == 'interactive':\n",
    "    # Show detailed visualizations\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee65d4cd-cd9e-4591-8dd7-16c8ed0915c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 10. Testing and Validation\n",
    "\n",
    "### Data Validation\n",
    "\n",
    "**Validate DataFrames before use**:\n",
    "\n",
    "```python\n",
    "def validate_dataframe_exists(df_name, df):\n",
    "    \"\"\"Validate that a DataFrame exists and has data\"\"\"\n",
    "    if df is None:\n",
    "        log(f\"⚠️  Warning: {df_name} is None\")\n",
    "        return False\n",
    "    try:\n",
    "        count = df.count()\n",
    "        if count == 0:\n",
    "            log(f\"⚠️  Warning: {df_name} is empty (0 rows)\")\n",
    "            return False\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log(f\"⚠️  Warning: Error checking {df_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "if not validate_dataframe_exists(\"df_assets\", df_assets):\n",
    "    log(\"⚠️  Warning: DataFrame validation failed\")\n",
    "```\n",
    "\n",
    "### Configuration Validation\n",
    "\n",
    "**Validate before execution**:\n",
    "\n",
    "```python\n",
    "def validate_config():\n",
    "    \"\"\"Validate configuration parameters\"\"\"\n",
    "    errors = []\n",
    "    \n",
    "    # Type checks\n",
    "    if not isinstance(MAX_RETRIES, int) or MAX_RETRIES < 1:\n",
    "        errors.append(\"MAX_RETRIES must be a positive integer\")\n",
    "    \n",
    "    # Format checks\n",
    "    if ENABLE_DELTA_EXPORT:\n",
    "        if not re.match(r'^[a-zA-Z0-9_]+\\.[a-zA-Z0-9_]+\\.[a-zA-Z0-9_]+$', DELTA_TABLE_NAME):\n",
    "            errors.append(f\"DELTA_TABLE_NAME must be in format 'catalog.schema.table'\")\n",
    "    \n",
    "    # Range checks\n",
    "    if MIN_AGE_DAYS is not None and MIN_AGE_DAYS < 0:\n",
    "        errors.append(\"MIN_AGE_DAYS must be a positive integer or None\")\n",
    "    \n",
    "    return errors\n",
    "\n",
    "config_errors = validate_config()\n",
    "if config_errors:\n",
    "    error_msg = \"Configuration validation failed:\\n\" + \"\\n\".join(f\"  - {e}\" for e in config_errors)\n",
    "    raise ValueError(error_msg)\n",
    "```\n",
    "\n",
    "### Path Validation\n",
    "\n",
    "**Test write permissions**:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    test_file = f\"{EXPORT_PATH}/.test\".replace('/dbfs', 'dbfs:')\n",
    "    dbutils.fs.put(test_file, 'test', overwrite=True)\n",
    "    dbutils.fs.rm(test_file)\n",
    "    log(\"  ✓ Export path is writable\")\n",
    "except Exception as e:\n",
    "    log(f\"  ⚠️ Warning: Export path may not be writable: {e}\")\n",
    "```\n",
    "\n",
    "### Execution Summary\n",
    "\n",
    "**Report comprehensive statistics**:\n",
    "\n",
    "```python\n",
    "def print_execution_summary():\n",
    "    \"\"\"Print execution statistics summary\"\"\"\n",
    "    elapsed = time.time() - execution_stats['start_time']\n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(\"EXECUTION SUMMARY\")\n",
    "    log(f\"{'='*60}\")\n",
    "    log(f\"Total execution time: {elapsed:.2f} seconds ({elapsed/60:.2f} minutes)\")\n",
    "    log(f\"API calls made: {execution_stats['api_calls']}\")\n",
    "    log(f\"Resources processed: {execution_stats['resources_processed']}\")\n",
    "    log(f\"Resources skipped: {execution_stats['resources_skipped']}\")\n",
    "    log(f\"API failures: {execution_stats['api_failures']}\")\n",
    "    log(f\"API retries: {execution_stats['api_retries']}\")\n",
    "    if execution_stats['api_calls'] > 0:\n",
    "        success_rate = ((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls']) * 100\n",
    "        log(f\"Success rate: {success_rate:.1f}%\")\n",
    "    log(f\"{'='*60}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "19742bdf-1e3e-4fe5-b39b-563e0c58b230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 11. Databricks SDK Integration\n",
    "\n",
    "### SDK Client Pattern\n",
    "\n",
    "**Use Databricks SDK for modern API access**:\n",
    "\n",
    "```python\n",
    "from databricks.sdk import WorkspaceClient\n",
    "\n",
    "# Initialize client (automatically uses notebook context)\n",
    "wc = WorkspaceClient()\n",
    "\n",
    "# Use SDK methods instead of raw REST API calls\n",
    "users = list(wc.users.list())\n",
    "groups = list(wc.groups.list())\n",
    "jobs = list(wc.jobs.list())\n",
    "warehouses = list(wc.warehouses.list())\n",
    "```\n",
    "\n",
    "### SDK vs REST API\n",
    "\n",
    "**When to use SDK**:\n",
    "* Modern, type-safe Python interface\n",
    "* Automatic authentication from notebook context\n",
    "* Built-in pagination handling\n",
    "* Better error messages and type hints\n",
    "* Recommended for new code\n",
    "\n",
    "**When to use REST API**:\n",
    "* SDK doesn't support the endpoint yet\n",
    "* Need fine-grained control over requests\n",
    "* Working with legacy code\n",
    "* Custom retry logic required\n",
    "\n",
    "### SDK Error Handling\n",
    "\n",
    "```python\n",
    "from databricks.sdk.errors import NotFound, PermissionDenied\n",
    "\n",
    "try:\n",
    "    job = wc.jobs.get(job_id)\n",
    "except NotFound:\n",
    "    log(f\"Job {job_id} not found\")\n",
    "except PermissionDenied:\n",
    "    log(f\"No permission to access job {job_id}\")\n",
    "except Exception as e:\n",
    "    log(f\"Error fetching job: {str(e)}\")\n",
    "```\n",
    "\n",
    "### SDK List Comprehensions\n",
    "\n",
    "**Convert SDK objects to DataFrames**:\n",
    "\n",
    "```python\n",
    "# Pattern: Extract relevant fields from SDK objects\n",
    "users = list(wc.users.list())\n",
    "users_df = spark.createDataFrame([\n",
    "    {\n",
    "        'user_name': u.user_name,\n",
    "        'display_name': u.display_name,\n",
    "        'active': u.active\n",
    "    }\n",
    "    for u in users\n",
    "])\n",
    "\n",
    "# Pattern: Handle optional fields safely\n",
    "jobs_data = [\n",
    "    {\n",
    "        'job_id': j.job_id,\n",
    "        'job_name': j.settings.name if j.settings else 'Unknown',\n",
    "        'creator': j.creator_user_name or 'Unknown'\n",
    "    }\n",
    "    for j in jobs\n",
    "]\n",
    "```\n",
    "\n",
    "### SDK Pagination\n",
    "\n",
    "**SDK handles pagination automatically**:\n",
    "\n",
    "```python\n",
    "# Good: SDK handles pagination internally\n",
    "all_jobs = list(wc.jobs.list())\n",
    "\n",
    "# Avoid: Manual pagination (SDK does this for you)\n",
    "# Don't implement manual pagination with SDK\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65541886-c945-40ed-a633-17de7f6f2a02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 12. Compute Type Detection and Optimization\n",
    "\n",
    "### Serverless Detection\n",
    "\n",
    "**Detect compute type at notebook start**:\n",
    "\n",
    "```python\n",
    "# Detect if running on serverless compute (most reliable method: try caching)\n",
    "try:\n",
    "    test_df = spark.range(1)\n",
    "    test_df.cache()\n",
    "    test_df.count()\n",
    "    test_df.unpersist()\n",
    "    is_serverless = False\n",
    "    log(\"Running on TRADITIONAL compute\")\n",
    "except Exception as e:\n",
    "    if 'PERSIST' in str(e).upper() or 'CACHE' in str(e).upper():\n",
    "        is_serverless = True\n",
    "        log(\"Running on SERVERLESS compute\")\n",
    "    else:\n",
    "        is_serverless = False\n",
    "        log(f\"Compute type detection inconclusive: {e}\")\n",
    "```\n",
    "\n",
    "### Conditional Caching\n",
    "\n",
    "**Cache only on traditional compute**:\n",
    "\n",
    "```python\n",
    "if is_serverless:\n",
    "    # Serverless: Just materialize with count (automatic optimization)\n",
    "    log(\"  Using automatic optimization (serverless)\")\n",
    "    users_count = users_df.count()\n",
    "    groups_count = groups_df.count()\n",
    "    log(f\"  Materialized {users_count} users, {groups_count} groups\")\n",
    "else:\n",
    "    # Traditional: Explicit caching for performance\n",
    "    log(\"  Caching DataFrames for reuse (traditional compute)\")\n",
    "    users_df.cache()\n",
    "    groups_df.cache()\n",
    "    users_df.count()  # Materialize\n",
    "    groups_df.count()  # Materialize\n",
    "    log(f\"  Cached {users_df.count()} users, {groups_df.count()} groups\")\n",
    "```\n",
    "\n",
    "### Serverless-Compatible File Operations\n",
    "\n",
    "**Use temporary directories for serverless**:\n",
    "\n",
    "```python\n",
    "if is_serverless:\n",
    "    # Serverless: Use temp directory (DBFS not writable)\n",
    "    import tempfile\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    export_path = temp_dir\n",
    "    log(f\"Using temporary directory: {export_path}\")\n",
    "else:\n",
    "    # Traditional: Use DBFS\n",
    "    export_path = '/dbfs/tmp/exports'\n",
    "    os.makedirs(export_path, exist_ok=True)\n",
    "    log(f\"Using DBFS directory: {export_path}\")\n",
    "```\n",
    "\n",
    "### Compute-Aware Optimizations\n",
    "\n",
    "**Adjust settings based on compute type**:\n",
    "\n",
    "```python\n",
    "# Adjust parallelism based on compute type\n",
    "if is_serverless:\n",
    "    MAX_WORKERS = 5  # Lower for serverless\n",
    "    BATCH_SIZE = 100\n",
    "    log(\"Optimized for serverless: MAX_WORKERS=5, BATCH_SIZE=100\")\n",
    "else:\n",
    "    MAX_WORKERS = 20  # Higher for traditional clusters\n",
    "    BATCH_SIZE = 500\n",
    "    log(\"Optimized for traditional: MAX_WORKERS=20, BATCH_SIZE=500\")\n",
    "```\n",
    "\n",
    "### DBFS Path Handling\n",
    "\n",
    "**Convert paths for serverless compatibility**:\n",
    "\n",
    "```python\n",
    "# Use dbutils.fs for serverless-compatible operations\n",
    "try:\n",
    "    dbutils.fs.ls(EXPORT_PATH.replace('/dbfs', 'dbfs:'))\n",
    "except:\n",
    "    dbutils.fs.mkdirs(EXPORT_PATH.replace('/dbfs', 'dbfs:'))\n",
    "\n",
    "# Write files using dbutils.fs\n",
    "dbutils.fs.put(\n",
    "    f\"{EXPORT_PATH}/file.txt\".replace('/dbfs', 'dbfs:'),\n",
    "    content,\n",
    "    overwrite=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "511d9ef1-fadc-46d9-b1ed-01b1537d9c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 13. Job Mode and Widget Parameters\n",
    "\n",
    "### Job Mode Detection\n",
    "\n",
    "**Detect at the very start of notebook (MUST BE FIRST)**:\n",
    "\n",
    "```python\n",
    "# MUST BE FIRST - before any other code\n",
    "try:\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().currentRunId().isDefined()\n",
    "    is_job_mode = True\n",
    "except:\n",
    "    is_job_mode = False\n",
    "\n",
    "log(f\"Execution mode: {'JOB' if is_job_mode else 'INTERACTIVE'}\")\n",
    "```\n",
    "\n",
    "### Widget Parameter Handling\n",
    "\n",
    "**Optional parameters with try-except**:\n",
    "\n",
    "```python\n",
    "# Pattern: Try to get widget parameter, fall back to default\n",
    "try:\n",
    "    job_max_resources = dbutils.widgets.get(\"max_resources_per_type\")\n",
    "    if job_max_resources:\n",
    "        MAX_RESOURCES_PER_TYPE = int(job_max_resources)\n",
    "        log(f\"Using job parameter: MAX_RESOURCES_PER_TYPE = {MAX_RESOURCES_PER_TYPE}\")\n",
    "except:\n",
    "    # Widget doesn't exist - use default from configuration\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    export_path_param = dbutils.widgets.get(\"export_path\")\n",
    "    if export_path_param:\n",
    "        EXPORT_PATH = export_path_param\n",
    "        log(f\"Using job parameter: EXPORT_PATH = {EXPORT_PATH}\")\n",
    "except:\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Required parameters with validation**:\n",
    "\n",
    "```python\n",
    "if not is_job_mode:\n",
    "    # Interactive: Get from widgets\n",
    "    try:\n",
    "        output_catalog = dbutils.widgets.get(\"output_catalog\")\n",
    "        output_schema = dbutils.widgets.get(\"output_schema\")\n",
    "    except:\n",
    "        raise ValueError(\"Required widgets not found. Run cell 1 to create widgets.\")\n",
    "else:\n",
    "    # Job: Use defaults or job parameters\n",
    "    output_catalog = 'main'\n",
    "    output_schema = 'default'\n",
    "```\n",
    "\n",
    "### Job Mode Overrides\n",
    "\n",
    "**Force comprehensive settings in job mode**:\n",
    "\n",
    "```python\n",
    "if is_job_mode:\n",
    "    log(\"\\n\uD83E\uDD16 JOB MODE DETECTED - Forcing Full Mode\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Override all limits for comprehensive audit\n",
    "    MAX_RESOURCES_PER_TYPE = 999\n",
    "    MAX_WORKSPACE_OBJECTS = 2000\n",
    "    \n",
    "    # Enable all resource types\n",
    "    ENABLE_JOBS = True\n",
    "    ENABLE_WAREHOUSES = True\n",
    "    ENABLE_CLUSTERS = True\n",
    "    ENABLE_PIPELINES = True\n",
    "    \n",
    "    log(\"  All limits removed for comprehensive audit\")\n",
    "    log(\"  All resource types enabled\")\n",
    "    log(\"=\"*60)\n",
    "```\n",
    "\n",
    "### Job Completion Summary\n",
    "\n",
    "**Return JSON summary for orchestration**:\n",
    "\n",
    "```python\n",
    "if is_job_mode:\n",
    "    # Create summary for job output\n",
    "    summary = {\n",
    "        'status': 'success',\n",
    "        'execution_time_seconds': execution_time,\n",
    "        'resources_processed': execution_stats['resources_processed'],\n",
    "        'api_calls': execution_stats['api_calls'],\n",
    "        'export_path': EXPORT_PATH,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Return as JSON for job orchestration\n",
    "    import json\n",
    "    dbutils.notebook.exit(json.dumps(summary))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b20e91ad-16e3-4777-a259-9c4b40a7d2f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 14. Health Scoring and Risk Assessment\n",
    "\n",
    "### Health Score Calculation\n",
    "\n",
    "**Weighted scoring system (0-100)**:\n",
    "\n",
    "```python\n",
    "# Define category weights (must sum to 100)\n",
    "weights = {\n",
    "    'security': 30,\n",
    "    'governance': 25,\n",
    "    'compliance': 25,\n",
    "    'performance': 20\n",
    "}\n",
    "\n",
    "# Calculate scores per category\n",
    "health_score = 0\n",
    "max_score = 100\n",
    "\n",
    "# Security score (0-30 points)\n",
    "security_score = 0\n",
    "if security_features_enabled >= 4:\n",
    "    security_score = 24\n",
    "elif security_features_enabled >= 3:\n",
    "    security_score = 18\n",
    "else:\n",
    "    security_score = 12\n",
    "\n",
    "health_score += security_score\n",
    "\n",
    "# Governance score (0-25 points)\n",
    "governance_score = 0\n",
    "if unity_catalog_enabled:\n",
    "    governance_score += 15\n",
    "if audit_logs_enabled:\n",
    "    governance_score += 10\n",
    "\n",
    "health_score += governance_score\n",
    "\n",
    "# Display results with emojis\n",
    "log(f\"\\n\uD83C\uDFE5 Overall Health Score: {health_score}/{max_score} ({health_score/max_score*100:.0f}%)\")\n",
    "log(f\"\\n\uD83D\uDD12 Security: {security_score}/{weights['security']} points\")\n",
    "log(f\"\uD83D\uDCCB Governance: {governance_score}/{weights['governance']} points\")\n",
    "log(f\"⚖️ Compliance: {compliance_score}/{weights['compliance']} points\")\n",
    "log(f\"⚡ Performance: {performance_score}/{weights['performance']} points\")\n",
    "```\n",
    "\n",
    "### Risk Scoring\n",
    "\n",
    "**0-100 risk scale (higher = more risk)**:\n",
    "\n",
    "```python\n",
    "risk_score = 0\n",
    "\n",
    "# High-risk configurations (+20 points each)\n",
    "if tokens_without_expiry > 0:\n",
    "    risk_score += 20\n",
    "    \n",
    "if admin_count > 10:\n",
    "    risk_score += 20\n",
    "\n",
    "# Medium-risk issues (+10 points each)\n",
    "if inactive_users_with_permissions > 0:\n",
    "    risk_score += 10\n",
    "\n",
    "if external_users_count > 5:\n",
    "    risk_score += 10\n",
    "\n",
    "# Low-risk issues (+5 points each)\n",
    "if over_privileged_users > 0:\n",
    "    risk_score += 5\n",
    "\n",
    "# Cap at 100\n",
    "risk_score = min(risk_score, 100)\n",
    "\n",
    "# Categorize risk level with emojis\n",
    "if risk_score >= 70:\n",
    "    risk_level = \"\uD83D\uDD34 HIGH RISK\"\n",
    "elif risk_score >= 40:\n",
    "    risk_level = \"\uD83D\uDFE1 MEDIUM RISK\"\n",
    "else:\n",
    "    risk_level = \"\uD83D\uDFE2 LOW RISK\"\n",
    "\n",
    "log(f\"\\n⚠️ Risk Score: {risk_score}/100 - {risk_level}\")\n",
    "```\n",
    "\n",
    "### Risk Factors Tracking\n",
    "\n",
    "**Track individual risk contributors**:\n",
    "\n",
    "```python\n",
    "risk_factors = []\n",
    "\n",
    "if tokens_without_expiry > 0:\n",
    "    risk_factors.append({\n",
    "        'category': 'Token Security',\n",
    "        'severity': 'HIGH',\n",
    "        'issue': f'{tokens_without_expiry} tokens without expiration',\n",
    "        'impact': 'Permanent access tokens pose security risk',\n",
    "        'recommendation': 'Set expiration dates on all tokens',\n",
    "        'affected_count': tokens_without_expiry\n",
    "    })\n",
    "\n",
    "if admin_count > 10:\n",
    "    risk_factors.append({\n",
    "        'category': 'Access Control',\n",
    "        'severity': 'MEDIUM',\n",
    "        'issue': f'{admin_count} workspace admins',\n",
    "        'impact': 'Too many admins increases attack surface',\n",
    "        'recommendation': 'Review and reduce admin count to <10',\n",
    "        'affected_count': admin_count\n",
    "    })\n",
    "\n",
    "# Create DataFrame for reporting\n",
    "if risk_factors:\n",
    "    risk_factors_df = spark.createDataFrame(risk_factors)\n",
    "    display(risk_factors_df)\n",
    "```\n",
    "\n",
    "### Severity Levels\n",
    "\n",
    "**Consistent severity definitions**:\n",
    "\n",
    "* **CRITICAL**: Immediate action required, active security threat\n",
    "* **HIGH**: Significant risk, should be addressed soon\n",
    "* **MEDIUM**: Moderate risk, address in next review cycle\n",
    "* **LOW**: Minor issue, address when convenient\n",
    "* **INFO**: Informational, no action required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a473f9f0-dea7-488d-8559-4bd3cebc925e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 15. Recommendation Generation\n",
    "\n",
    "### Recommendation Structure\n",
    "\n",
    "**Consistent recommendation format**:\n",
    "\n",
    "```python\n",
    "recommendations = []\n",
    "\n",
    "# Pattern: Check condition, add recommendation\n",
    "if condition_detected:\n",
    "    recommendations.append({\n",
    "        'priority': 'HIGH',  # CRITICAL, HIGH, MEDIUM, LOW, INFO\n",
    "        'category': 'Security',\n",
    "        'title': 'Enable IP Access Lists',\n",
    "        'description': 'IP access lists are not enabled',\n",
    "        'impact': 'Workspace accessible from any IP address',\n",
    "        'recommendation': 'Enable IP access lists to restrict access to trusted networks',\n",
    "        'documentation': 'https://docs.databricks.com/security/network/ip-access-list.html',\n",
    "        'affected_count': 1\n",
    "    })\n",
    "```\n",
    "\n",
    "### Priority Levels\n",
    "\n",
    "**Consistent priority definitions**:\n",
    "\n",
    "* **CRITICAL**: Immediate action required, active security threat, compliance violation\n",
    "* **HIGH**: Significant risk, should be addressed within days\n",
    "* **MEDIUM**: Moderate risk, address in next review cycle (weeks)\n",
    "* **LOW**: Minor issue, address when convenient (months)\n",
    "* **INFO**: Informational, no action required\n",
    "\n",
    "### Recommendation Display\n",
    "\n",
    "**User-friendly output with priority grouping**:\n",
    "\n",
    "```python\n",
    "if recommendations:\n",
    "    log(f\"\\n\uD83D\uDCA1 Found {len(recommendations)} recommendations:\\n\")\n",
    "    \n",
    "    # Group by priority\n",
    "    critical = [r for r in recommendations if r['priority'] == 'CRITICAL']\n",
    "    high_priority = [r for r in recommendations if r['priority'] == 'HIGH']\n",
    "    medium_priority = [r for r in recommendations if r['priority'] == 'MEDIUM']\n",
    "    low_priority = [r for r in recommendations if r['priority'] == 'LOW']\n",
    "    \n",
    "    # Display critical first\n",
    "    if critical:\n",
    "        log(\"\uD83D\uDD34 CRITICAL:\")\n",
    "        for i, rec in enumerate(critical, 1):\n",
    "            log(f\"  {i}. {rec['title']}\")\n",
    "            log(f\"     {rec['description']}\")\n",
    "            log(f\"     Impact: {rec['impact']}\")\n",
    "            log(f\"     → {rec['recommendation']}\\n\")\n",
    "    \n",
    "    # Display high priority\n",
    "    if high_priority:\n",
    "        log(\"\uD83D\uDFE0 HIGH PRIORITY:\")\n",
    "        for i, rec in enumerate(high_priority, 1):\n",
    "            log(f\"  {i}. {rec['title']}\")\n",
    "            log(f\"     → {rec['recommendation']}\\n\")\n",
    "    \n",
    "    # Display medium priority\n",
    "    if medium_priority:\n",
    "        log(\"\uD83D\uDFE1 MEDIUM PRIORITY:\")\n",
    "        for i, rec in enumerate(medium_priority, 1):\n",
    "            log(f\"  {i}. {rec['title']}\")\n",
    "            log(f\"     → {rec['recommendation']}\\n\")\n",
    "else:\n",
    "    log(\"\\n✅ No recommendations - configuration looks good!\")\n",
    "```\n",
    "\n",
    "### Actionable Recommendations\n",
    "\n",
    "**Include specific, measurable actions**:\n",
    "\n",
    "```python\n",
    "# Good: Specific and actionable\n",
    "recommendation = \"Set MAX_TOKEN_LIFETIME_DAYS to 90 in workspace settings\"\n",
    "recommendation = \"Remove CAN_MANAGE permission from 5 inactive users\"\n",
    "recommendation = \"Enable Unity Catalog for data governance\"\n",
    "\n",
    "# Avoid: Vague and generic\n",
    "recommendation = \"Improve token security\"  # Too vague\n",
    "recommendation = \"Fix permissions\"  # Not specific\n",
    "recommendation = \"Review settings\"  # No clear action\n",
    "```\n",
    "\n",
    "### Recommendation DataFrame\n",
    "\n",
    "**Create DataFrame for export**:\n",
    "\n",
    "```python\n",
    "if recommendations:\n",
    "    recommendations_df = spark.createDataFrame(recommendations)\n",
    "    \n",
    "    # Sort by priority\n",
    "    priority_order = {'CRITICAL': 1, 'HIGH': 2, 'MEDIUM': 3, 'LOW': 4, 'INFO': 5}\n",
    "    recommendations_df = recommendations_df.withColumn(\n",
    "        'priority_order',\n",
    "        F.when(F.col('priority') == 'CRITICAL', 1)\n",
    "         .when(F.col('priority') == 'HIGH', 2)\n",
    "         .when(F.col('priority') == 'MEDIUM', 3)\n",
    "         .when(F.col('priority') == 'LOW', 4)\n",
    "         .otherwise(5)\n",
    "    ).orderBy('priority_order')\n",
    "    \n",
    "    display(recommendations_df.drop('priority_order'))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30e1d2e4-89fa-4d46-b74a-ff2ffb5426fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 16. Visualization Standards\n",
    "\n",
    "### Matplotlib Configuration\n",
    "\n",
    "**Standard chart setup**:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create figure with subplots\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Chart 1: Bar chart\n",
    "ax1 = axes[0]\n",
    "category_counts = df.groupby('category').size().sort_values(ascending=False)\n",
    "category_counts.plot(kind='bar', ax=ax1, color='steelblue')\n",
    "ax1.set_title('Distribution by Category', fontsize=12, fontweight='bold')\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Chart 2: Pie chart\n",
    "ax2 = axes[1]\n",
    "status_counts = df.groupby('status').size()\n",
    "ax2.pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "ax2.set_title('Status Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Chart Best Practices\n",
    "\n",
    "**Consistent styling**:\n",
    "\n",
    "```python\n",
    "# Define color palette\n",
    "COLORS = {\n",
    "    'primary': 'steelblue',\n",
    "    'success': 'green',\n",
    "    'warning': 'orange',\n",
    "    'danger': 'red',\n",
    "    'info': 'skyblue'\n",
    "}\n",
    "\n",
    "# Add grid for readability\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "# Rotate x-axis labels if needed\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Add value labels on bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%d')\n",
    "\n",
    "# Set figure size appropriately\n",
    "fig, ax = plt.subplots(figsize=(10, 6))  # Single chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))  # Multiple charts\n",
    "```\n",
    "\n",
    "### Conditional Visualization\n",
    "\n",
    "**Only show charts in interactive mode**:\n",
    "\n",
    "```python\n",
    "if not is_job_mode and df is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"VISUALIZATION\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Create charts\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    df.groupby('category').size().plot(kind='bar', ax=ax)\n",
    "    ax.set_title('Distribution by Category')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    log(\"ℹ️  Visualization skipped (job mode or no data)\")\n",
    "```\n",
    "\n",
    "### Display vs Show\n",
    "\n",
    "**Use appropriate display method**:\n",
    "\n",
    "```python\n",
    "# DataFrames: Use display()\n",
    "display(df.limit(10))\n",
    "\n",
    "# Matplotlib charts: Use plt.show()\n",
    "plt.show()\n",
    "\n",
    "# Pandas DataFrames in interactive mode\n",
    "if not is_job_mode:\n",
    "    display(pandas_df.head(10))\n",
    "\n",
    "# Don't mix them\n",
    "# display(plt)  # Wrong - doesn't work\n",
    "# plt.show(df)  # Wrong - not a chart\n",
    "```\n",
    "\n",
    "### Chart Types\n",
    "\n",
    "**Choose appropriate chart type**:\n",
    "\n",
    "```python\n",
    "# Bar chart: Comparing categories\n",
    "df.groupby('category').size().plot(kind='bar')\n",
    "\n",
    "# Pie chart: Showing proportions\n",
    "df.groupby('status').size().plot(kind='pie', autopct='%1.1f%%')\n",
    "\n",
    "# Line chart: Showing trends over time\n",
    "df.groupby('date').size().plot(kind='line')\n",
    "\n",
    "# Histogram: Showing distributions\n",
    "df['age_days'].plot(kind='hist', bins=20)\n",
    "\n",
    "# Horizontal bar: Long category names\n",
    "df.groupby('category').size().plot(kind='barh')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf3e343d-a9ef-47c9-b723-bc77214962a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 17. Export Format Flexibility\n",
    "\n",
    "### Multiple Export Formats\n",
    "\n",
    "**Support Excel, CSV, JSON, Delta with feature flags**:\n",
    "\n",
    "```python\n",
    "# Configuration flags\n",
    "ENABLE_EXCEL_EXPORT = True\n",
    "ENABLE_CSV_EXPORT = False\n",
    "ENABLE_JSON_EXPORT = False\n",
    "ENABLE_DELTA_EXPORT = True\n",
    "\n",
    "# Export logic\n",
    "if df is not None:\n",
    "    if ENABLE_EXCEL_EXPORT:\n",
    "        log(\"Exporting to Excel...\")\n",
    "        # Excel export code\n",
    "    \n",
    "    if ENABLE_CSV_EXPORT:\n",
    "        log(\"Exporting to CSV...\")\n",
    "        # CSV export code\n",
    "    \n",
    "    if ENABLE_JSON_EXPORT:\n",
    "        log(\"Exporting to JSON...\")\n",
    "        # JSON export code\n",
    "    \n",
    "    if ENABLE_DELTA_EXPORT:\n",
    "        log(\"Exporting to Delta table...\")\n",
    "        # Delta export code\n",
    "```\n",
    "\n",
    "### Excel Export with Multiple Sheets\n",
    "\n",
    "**Use ExcelWriter for multi-sheet workbooks**:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "\n",
    "# Create Excel file with multiple sheets\n",
    "with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "    # Write data sheets\n",
    "    df_summary.toPandas().to_excel(writer, sheet_name='Summary', index=False)\n",
    "    df_details.toPandas().to_excel(writer, sheet_name='Details', index=False)\n",
    "    df_recommendations.toPandas().to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "    df_stats.toPandas().to_excel(writer, sheet_name='Statistics', index=False)\n",
    "\n",
    "log(f\"✓ Excel workbook created with {len(writer.sheets)} sheets\")\n",
    "```\n",
    "\n",
    "### Excel Formatting\n",
    "\n",
    "**Apply professional styling with openpyxl**:\n",
    "\n",
    "```python\n",
    "# Load workbook for formatting\n",
    "wb = load_workbook(excel_path)\n",
    "\n",
    "for sheet_name in wb.sheetnames:\n",
    "    ws = wb[sheet_name]\n",
    "    \n",
    "    # Format header row (row 1)\n",
    "    for cell in ws[1]:\n",
    "        cell.font = Font(bold=True, color='FFFFFF')\n",
    "        cell.fill = PatternFill(start_color='366092', end_color='366092', fill_type='solid')\n",
    "        cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    \n",
    "    # Auto-adjust column widths\n",
    "    for column in ws.columns:\n",
    "        max_length = 0\n",
    "        column_letter = column[0].column_letter\n",
    "        for cell in column:\n",
    "            if cell.value:\n",
    "                max_length = max(max_length, len(str(cell.value)))\n",
    "        # Set width with min/max bounds\n",
    "        adjusted_width = min(max_length + 2, 50)\n",
    "        ws.column_dimensions[column_letter].width = max(adjusted_width, 10)\n",
    "    \n",
    "    # Freeze header row\n",
    "    ws.freeze_panes = 'A2'\n",
    "\n",
    "wb.save(excel_path)\n",
    "log(\"✓ Excel formatting applied\")\n",
    "```\n",
    "\n",
    "### CSV Export\n",
    "\n",
    "**Simple CSV export for large datasets**:\n",
    "\n",
    "```python\n",
    "if ENABLE_CSV_EXPORT:\n",
    "    csv_path = f\"{EXPORT_PATH}/data_{timestamp}.csv\"\n",
    "    \n",
    "    # Convert to Pandas and export\n",
    "    df.toPandas().to_csv(csv_path, index=False)\n",
    "    \n",
    "    log(f\"✓ CSV exported: {csv_path}\")\n",
    "    log(f\"  Rows: {df.count()}\")\n",
    "```\n",
    "\n",
    "### JSON Export\n",
    "\n",
    "**Structured JSON for API integration**:\n",
    "\n",
    "```python\n",
    "if ENABLE_JSON_EXPORT:\n",
    "    json_path = f\"{EXPORT_PATH}/data_{timestamp}.json\"\n",
    "    \n",
    "    # Convert to JSON with metadata\n",
    "    export_data = {\n",
    "        'metadata': {\n",
    "            'export_timestamp': datetime.now().isoformat(),\n",
    "            'record_count': df.count(),\n",
    "            'version': '1.0',\n",
    "            'execution_time_seconds': execution_time\n",
    "        },\n",
    "        'data': df.toPandas().to_dict(orient='records')\n",
    "    }\n",
    "    \n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(export_data, f, indent=2, default=str)\n",
    "    \n",
    "    log(f\"✓ JSON exported: {json_path}\")\n",
    "```\n",
    "\n",
    "### Delta Table Export\n",
    "\n",
    "**Historical accumulation with append mode**:\n",
    "\n",
    "```python\n",
    "if ENABLE_DELTA_EXPORT:\n",
    "    # Add audit metadata\n",
    "    df_export = df.withColumn('audit_timestamp', F.current_timestamp())\n",
    "    df_export = df_export.withColumn('execution_time_seconds', F.lit(execution_time))\n",
    "    df_export = df_export.withColumn('execution_mode', F.lit(execution_mode))\n",
    "    \n",
    "    # Write to Delta table (append mode for history)\n",
    "    df_export.write \\\n",
    "        .format('delta') \\\n",
    "        .mode('append') \\\n",
    "        .option('mergeSchema', 'true') \\\n",
    "        .saveAsTable(DELTA_TABLE_NAME)\n",
    "    \n",
    "    log(f\"✓ Delta table updated: {DELTA_TABLE_NAME}\")\n",
    "    log(f\"  Mode: append (historical retention)\")\n",
    "    log(f\"  Schema evolution: enabled\")\n",
    "```\n",
    "\n",
    "### Export Path Handling\n",
    "\n",
    "**Consistent path construction**:\n",
    "\n",
    "```python\n",
    "# Create timestamp for filenames\n",
    "eastern = pytz.timezone('America/New_York')\n",
    "timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Construct export paths\n",
    "excel_path = f\"{EXPORT_PATH}/report_{timestamp}.xlsx\"\n",
    "csv_path = f\"{EXPORT_PATH}/data_{timestamp}.csv\"\n",
    "json_path = f\"{EXPORT_PATH}/data_{timestamp}.json\"\n",
    "\n",
    "log(f\"Export files will be saved to: {EXPORT_PATH}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f767833-92d0-48f7-9a17-6be1298d4889",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 18. Execution Mode Patterns\n",
    "\n",
    "### Mode-Aware Output\n",
    "\n",
    "**Conditional display based on execution mode**:\n",
    "\n",
    "```python\n",
    "# Interactive mode: Show detailed output and visualizations\n",
    "if not is_job_mode:\n",
    "    log(\"\\nDetailed Analysis:\")\n",
    "    display(df.limit(20))\n",
    "    \n",
    "    # Show visualizations\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    df.groupby('category').size().plot(kind='bar')\n",
    "    plt.title('Distribution by Category')\n",
    "    plt.show()\n",
    "else:\n",
    "    # Job mode: Minimal output, focus on metrics\n",
    "    log(f\"Processed {df.count()} records\")\n",
    "    log(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "```\n",
    "\n",
    "### Performance Presets\n",
    "\n",
    "**Quick, Full, and Custom modes**:\n",
    "\n",
    "```python\n",
    "# ============================================================================\n",
    "# PERFORMANCE PRESETS: Choose your execution mode\n",
    "# ============================================================================\n",
    "# Uncomment ONE of the following presets\n",
    "\n",
    "# PRESET 1: QUICK MODE (5-10 minutes) - Fast scanning with limits\n",
    "# Recommended for: Daily monitoring, quick audits, testing\n",
    "# USE_QUICK_MODE = True\n",
    "\n",
    "# PRESET 2: FULL MODE (20-60 minutes) - Complete coverage\n",
    "# Recommended for: Compliance audits, weekly reviews, comprehensive analysis\n",
    "# USE_FULL_MODE = True\n",
    "\n",
    "# PRESET 3: CUSTOM MODE (default)\n",
    "# Recommended for: Specific use cases, targeted audits\n",
    "# (Default if no preset is uncommented)\n",
    "\n",
    "# ============================================================================\n",
    "# Apply preset configurations\n",
    "# ============================================================================\n",
    "\n",
    "if 'USE_QUICK_MODE' in dir() and USE_QUICK_MODE:\n",
    "    log(\"\\n\uD83D\uDE80 QUICK MODE ENABLED\")\n",
    "    log(\"=\"*60)\n",
    "    MAX_RESOURCES = 100\n",
    "    ENABLE_EXPENSIVE_OPERATIONS = False\n",
    "    ENABLE_DETAILED_ANALYSIS = False\n",
    "    log(\"  Resource limit: 100 per type\")\n",
    "    log(\"  Expensive operations: DISABLED\")\n",
    "    log(\"  Estimated time: 5-10 minutes\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "elif 'USE_FULL_MODE' in dir() and USE_FULL_MODE:\n",
    "    log(\"\\n\uD83D\uDD0D FULL MODE ENABLED\")\n",
    "    log(\"=\"*60)\n",
    "    MAX_RESOURCES = 999\n",
    "    ENABLE_EXPENSIVE_OPERATIONS = True\n",
    "    ENABLE_DETAILED_ANALYSIS = True\n",
    "    log(\"  Resource limit: NONE (complete scan)\")\n",
    "    log(\"  Expensive operations: ENABLED\")\n",
    "    log(\"  Estimated time: 20-60 minutes\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "else:\n",
    "    log(\"\\n⚙️ CUSTOM MODE - Using configuration from Cell 2\")\n",
    "    log(\"=\"*60)\n",
    "\n",
    "# Job mode override: Always use Full Mode\n",
    "if is_job_mode:\n",
    "    log(\"\\n\uD83E\uDD16 JOB MODE - Forcing Full Mode\")\n",
    "    MAX_RESOURCES = 999\n",
    "    ENABLE_EXPENSIVE_OPERATIONS = True\n",
    "```\n",
    "\n",
    "### Conditional Feature Flags\n",
    "\n",
    "**Enable/disable features based on configuration**:\n",
    "\n",
    "```python\n",
    "# Configuration\n",
    "ENABLE_JOBS = True\n",
    "ENABLE_WAREHOUSES = True\n",
    "ENABLE_CLUSTERS = False\n",
    "ENABLE_PIPELINES = True\n",
    "\n",
    "# Execution with skip messages\n",
    "if ENABLE_JOBS:\n",
    "    log(\"Processing jobs...\")\n",
    "    # ... jobs processing code ...\n",
    "else:\n",
    "    log(\"ℹ️  Jobs scanning disabled (ENABLE_JOBS=False)\")\n",
    "\n",
    "if ENABLE_WAREHOUSES:\n",
    "    log(\"Processing warehouses...\")\n",
    "    # ... warehouses processing code ...\n",
    "else:\n",
    "    log(\"ℹ️  Warehouses scanning disabled (ENABLE_WAREHOUSES=False)\")\n",
    "```\n",
    "\n",
    "### Skip Messages\n",
    "\n",
    "**Informative skip messages with instructions**:\n",
    "\n",
    "```python\n",
    "if not ENABLE_FEATURE:\n",
    "    log(\"ℹ️  Feature skipped (ENABLE_FEATURE=False)\")\n",
    "    log(\"   Set ENABLE_FEATURE=True in Cell 2 to enable\")\n",
    "else:\n",
    "    # Process feature\n",
    "    pass\n",
    "\n",
    "# For conditional features\n",
    "if not condition_met:\n",
    "    log(\"ℹ️  Feature skipped (condition not met)\")\n",
    "    log(f\"   Reason: {reason}\")\n",
    "```\n",
    "\n",
    "### Resource Limits\n",
    "\n",
    "**Apply limits with clear logging**:\n",
    "\n",
    "```python\n",
    "if MAX_RESOURCES_PER_TYPE == 999:\n",
    "    log(f\"Fetching all resources (no limit)...\")\n",
    "    resources = list(wc.resource.list())\n",
    "else:\n",
    "    log(f\"Fetching resources (up to {MAX_RESOURCES_PER_TYPE})...\")\n",
    "    resources = list(wc.resource.list())[:MAX_RESOURCES_PER_TYPE]\n",
    "    \n",
    "    if len(resources) == MAX_RESOURCES_PER_TYPE:\n",
    "        log(f\"  ⚠️  Limit reached: {MAX_RESOURCES_PER_TYPE} resources\")\n",
    "        log(f\"     Set MAX_RESOURCES_PER_TYPE=999 for complete scan\")\n",
    "\n",
    "log(f\"  ✓ Fetched {len(resources)} resources\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb076898-67fd-48e3-b995-df955b5f94f7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Summary: Key Best Practices"
    }
   },
   "source": [
    "## Summary: Key Best Practices\n",
    "\n",
    "### Code Quality Checklist\n",
    "\n",
    "✓ **Naming**\n",
    "- [ ] Constants in `UPPER_SNAKE_CASE`\n",
    "- [ ] Variables and functions in `snake_case`\n",
    "- [ ] DataFrames prefixed with `df_`\n",
    "- [ ] Descriptive, meaningful names\n",
    "\n",
    "✓ **Documentation**\n",
    "- [ ] Notebook header with overview and version control\n",
    "- [ ] Docstrings for all functions\n",
    "- [ ] Section headers with banner comments\n",
    "- [ ] Inline comments explain *why*, not *what*\n",
    "- [ ] TODO/FIXME comments for future work\n",
    "\n",
    "✓ **Error Handling**\n",
    "- [ ] Graceful degradation on errors\n",
    "- [ ] Consistent error symbols (✓, ✗, ⚠️)\n",
    "- [ ] Validation before processing\n",
    "- [ ] Try-except with specific error handling\n",
    "\n",
    "✓ **Configuration**\n",
    "- [ ] Organized into logical sections with banners\n",
    "- [ ] Validation function implemented\n",
    "- [ ] Widget parameters with defaults\n",
    "- [ ] Feature flags for optional functionality\n",
    "\n",
    "✓ **API Integration**\n",
    "- [ ] Centralized API client function (REST or SDK)\n",
    "- [ ] Retry logic with exponential backoff\n",
    "- [ ] Pagination pattern for large datasets\n",
    "- [ ] Progress logging for long operations\n",
    "\n",
    "✓ **Performance**\n",
    "- [ ] Execution timing for major operations\n",
    "- [ ] Memory usage monitoring\n",
    "- [ ] Statistics tracking\n",
    "- [ ] Conditional processing based on mode\n",
    "- [ ] Compute-aware optimizations (serverless vs traditional)\n",
    "- [ ] Parallel processing with ThreadPoolExecutor where appropriate\n",
    "\n",
    "✓ **Data Processing**\n",
    "- [ ] Consistent data structure across asset types\n",
    "- [ ] Explicit schema definition\n",
    "- [ ] Timezone-aware timestamp handling\n",
    "- [ ] Deduplication logic\n",
    "- [ ] Null handling and validation\n",
    "- [ ] Data quality checks\n",
    "\n",
    "✓ **Logging**\n",
    "- [ ] Centralized logging function\n",
    "- [ ] Consistent message formatting with f-strings\n",
    "- [ ] Progress updates for long operations\n",
    "- [ ] Summary reports with statistics\n",
    "- [ ] Appropriate emoji usage for visual clarity\n",
    "\n",
    "✓ **Security**\n",
    "- [ ] Never hardcode credentials or tokens\n",
    "- [ ] Use dbutils.secrets for sensitive data\n",
    "- [ ] Mask sensitive information in logs\n",
    "- [ ] Sanitize user input\n",
    "- [ ] Set appropriate file permissions\n",
    "\n",
    "✓ **Advanced Patterns**\n",
    "- [ ] Databricks SDK integration where appropriate\n",
    "- [ ] Serverless compute detection and optimization\n",
    "- [ ] Job mode detection and overrides\n",
    "- [ ] Health scoring and risk assessment (if applicable)\n",
    "- [ ] Recommendation generation with priorities\n",
    "- [ ] Conditional visualizations\n",
    "- [ ] Multiple export format support\n",
    "- [ ] Performance presets (Quick/Full/Custom)\n",
    "\n",
    "✓ **Code Quality**\n",
    "- [ ] Imports organized (stdlib → third-party → local)\n",
    "- [ ] Standard import aliases (pd, F, T)\n",
    "- [ ] Spark DataFrame best practices\n",
    "- [ ] Proper string formatting\n",
    "- [ ] Data quality validation\n",
    "\n",
    "---\n",
    "\n",
    "## Quick Reference\n",
    "\n",
    "### Common Patterns\n",
    "\n",
    "```python\n",
    "# Job mode detection (MUST BE FIRST)\n",
    "try:\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().currentRunId().isDefined()\n",
    "    is_job_mode = True\n",
    "except:\n",
    "    is_job_mode = False\n",
    "\n",
    "# Serverless detection\n",
    "try:\n",
    "    test_df = spark.range(1).cache().count()\n",
    "    is_serverless = False\n",
    "except:\n",
    "    is_serverless = True\n",
    "\n",
    "# API client (REST)\n",
    "api_url, api_token = get_api_client()\n",
    "\n",
    "# SDK client\n",
    "from databricks.sdk import WorkspaceClient\n",
    "wc = WorkspaceClient()\n",
    "\n",
    "# Logging with symbols\n",
    "log(f\"✓ Success: {count} items processed\")\n",
    "log(f\"✗ Error: {error_message}\")\n",
    "log(f\"⚠️ Warning: {warning_message}\")\n",
    "log(f\"ℹ️ Info: {info_message}\")\n",
    "\n",
    "# Execution timing\n",
    "cell_start_time = time.time()\n",
    "# ... code ...\n",
    "log_execution_time(\"Cell Name\", cell_start_time)\n",
    "\n",
    "# Conditional execution\n",
    "if ENABLE_FEATURE and df is not None:\n",
    "    # Process feature\n",
    "    pass\n",
    "else:\n",
    "    log(\"ℹ️  Feature skipped\")\n",
    "\n",
    "# DataFrame validation\n",
    "if not validate_dataframe_exists(\"df_name\", df):\n",
    "    log(\"⚠️  Validation failed\")\n",
    "\n",
    "# Parallel processing\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(func, item) for item in items]\n",
    "    for future in as_completed(futures):\n",
    "        result = future.result()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "* **PEP 8**: Python style guide - https://pep8.org/\n",
    "* **Databricks Best Practices**: Official documentation\n",
    "* **Spark Programming Guide**: DataFrame optimization\n",
    "* **Databricks SDK Documentation**: Modern API patterns\n",
    "* **The Zen of Python**: `import this`\n",
    "\n",
    "---\n",
    "\n",
    "## Maintenance\n",
    "\n",
    "This document should be updated when:\n",
    "* New patterns are established\n",
    "* Standards are refined\n",
    "* New features require new conventions\n",
    "* Team feedback suggests improvements\n",
    "* New notebooks are added to the audit suite\n",
    "* Databricks releases new features or deprecates old ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fda64b27-f790-4cf4-9133-63da8b4584d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 19. Parallel Processing with ThreadPoolExecutor\n",
    "\n",
    "### Concurrent API Calls\n",
    "\n",
    "**Use ThreadPoolExecutor for parallel operations**:\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Configuration\n",
    "MAX_WORKERS = 10  # Adjust based on compute type\n",
    "\n",
    "def fetch_permissions(resource_id):\n",
    "    \"\"\"Fetch permissions for a single resource\"\"\"\n",
    "    try:\n",
    "        response = api_call_with_retry(lambda: wc.permissions.get(resource_id))\n",
    "        return {'resource_id': resource_id, 'permissions': response}\n",
    "    except Exception as e:\n",
    "        log(f\"Error fetching permissions for {resource_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Parallel execution\n",
    "results = []\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_resource = {\n",
    "        executor.submit(fetch_permissions, resource_id): resource_id \n",
    "        for resource_id in resource_ids\n",
    "    }\n",
    "    \n",
    "    # Collect results as they complete\n",
    "    for future in as_completed(future_to_resource):\n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "```\n",
    "\n",
    "### Progress Tracking for Parallel Operations\n",
    "\n",
    "**Track progress with counters**:\n",
    "\n",
    "```python\n",
    "completed = 0\n",
    "total = len(resource_ids)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    future_to_resource = {executor.submit(fetch_data, rid): rid for rid in resource_ids}\n",
    "    \n",
    "    for future in as_completed(future_to_resource):\n",
    "        completed += 1\n",
    "        \n",
    "        # Log progress every 10% or every 20 items\n",
    "        if completed % max(1, total // 10) == 0 or completed % 20 == 0:\n",
    "            progress_pct = (completed / total) * 100\n",
    "            log(f\"  Progress: {completed}/{total} ({progress_pct:.1f}%)\")\n",
    "        \n",
    "        result = future.result()\n",
    "        if result:\n",
    "            results.append(result)\n",
    "\n",
    "log(f\"✓ Completed {completed}/{total} operations\")\n",
    "```\n",
    "\n",
    "### Error Handling in Parallel Operations\n",
    "\n",
    "**Handle exceptions gracefully**:\n",
    "\n",
    "```python\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = [executor.submit(process_item, item) for item in items]\n",
    "    \n",
    "    for future in as_completed(futures):\n",
    "        try:\n",
    "            result = future.result(timeout=30)  # Add timeout\n",
    "            if result:\n",
    "                results.append(result)\n",
    "        except TimeoutError:\n",
    "            log(f\"⚠️  Operation timed out\")\n",
    "            execution_stats['api_failures'] += 1\n",
    "        except Exception as e:\n",
    "            log(f\"⚠️  Error in parallel operation: {str(e)}\")\n",
    "            execution_stats['api_failures'] += 1\n",
    "```\n",
    "\n",
    "### When to Use Parallel Processing\n",
    "\n",
    "**Use for**:\n",
    "* Multiple independent API calls\n",
    "* Fetching permissions for many resources\n",
    "* Processing independent data chunks\n",
    "* I/O-bound operations\n",
    "\n",
    "**Avoid for**:\n",
    "* CPU-bound operations (use Spark instead)\n",
    "* Operations with shared state\n",
    "* Very fast operations (overhead not worth it)\n",
    "* When order matters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "311ffced-9db6-4d55-92b7-74fc7ddfe6b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 20. Import Organization\n",
    "\n",
    "### Import Order\n",
    "\n",
    "**Standard library → Third-party → Local**:\n",
    "\n",
    "```python\n",
    "# Standard library imports\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Third-party imports\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font, PatternFill, Alignment\n",
    "\n",
    "# Databricks SDK\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors import NotFound, PermissionDenied\n",
    "\n",
    "# PySpark imports\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType\n",
    "\n",
    "# Local/project imports (if any)\n",
    "# from my_module import my_function\n",
    "```\n",
    "\n",
    "### Import Aliases\n",
    "\n",
    "**Use standard aliases**:\n",
    "\n",
    "```python\n",
    "# Good: Standard aliases\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# Avoid: Non-standard aliases\n",
    "import pandas as p  # Not standard\n",
    "from pyspark.sql import functions as func  # Too verbose\n",
    "```\n",
    "\n",
    "### Conditional Imports\n",
    "\n",
    "**Import only when needed**:\n",
    "\n",
    "```python\n",
    "# Import at top for always-used packages\n",
    "import pandas as pd\n",
    "\n",
    "# Import conditionally for optional features\n",
    "if ENABLE_EXCEL_EXPORT:\n",
    "    from openpyxl import load_workbook\n",
    "    from openpyxl.styles import Font, PatternFill\n",
    "\n",
    "if ENABLE_VISUALIZATION:\n",
    "    import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "### Package Installation\n",
    "\n",
    "**Use %pip for notebook package installation**:\n",
    "\n",
    "```python\n",
    "# Good: Use %pip magic command\n",
    "%pip install openpyxl --quiet\n",
    "\n",
    "# Avoid: Using !pip\n",
    "# !pip install openpyxl  # Less reliable in notebooks\n",
    "```\n",
    "\n",
    "### Import Error Handling\n",
    "\n",
    "**Handle missing optional dependencies**:\n",
    "\n",
    "```python\n",
    "try:\n",
    "    import psutil\n",
    "    PSUTIL_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PSUTIL_AVAILABLE = False\n",
    "    log(\"⚠️  psutil not available, memory monitoring disabled\")\n",
    "\n",
    "# Use conditional logic\n",
    "if PSUTIL_AVAILABLE:\n",
    "    memory_usage = get_memory_usage()\n",
    "else:\n",
    "    memory_usage = 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7a4f064-6f74-4c80-a895-81281fc7ca66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 21. Security and Secrets Management\n",
    "\n",
    "### API Token Handling\n",
    "\n",
    "**Never hardcode tokens**:\n",
    "\n",
    "```python\n",
    "# Good: Get from notebook context\n",
    "def get_api_client():\n",
    "    \"\"\"Get Databricks API client configuration\"\"\"\n",
    "    try:\n",
    "        ctx = dbutils.notebook.entry_point.getDbutils().notebook().getContext()\n",
    "        api_url = ctx.apiUrl().get()\n",
    "        api_token = ctx.apiToken().get()\n",
    "        return api_url, api_token\n",
    "    except Exception as e:\n",
    "        log(f\"Error getting API client: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Avoid: Hardcoded tokens\n",
    "# api_token = \"dapi123456789\"  # NEVER DO THIS\n",
    "```\n",
    "\n",
    "### Secrets Management\n",
    "\n",
    "**Use Databricks Secrets for sensitive data**:\n",
    "\n",
    "```python\n",
    "# Good: Use secrets\n",
    "try:\n",
    "    api_key = dbutils.secrets.get(scope=\"my-scope\", key=\"api-key\")\n",
    "    db_password = dbutils.secrets.get(scope=\"my-scope\", key=\"db-password\")\n",
    "except Exception as e:\n",
    "    log(f\"Error retrieving secrets: {e}\")\n",
    "    raise\n",
    "\n",
    "# Avoid: Hardcoded credentials\n",
    "# api_key = \"abc123\"  # NEVER\n",
    "# db_password = \"password123\"  # NEVER\n",
    "```\n",
    "\n",
    "### Sensitive Data in Logs\n",
    "\n",
    "**Mask sensitive information**:\n",
    "\n",
    "```python\n",
    "# Good: Mask tokens and passwords\n",
    "log(f\"API token: {api_token[:8]}...{api_token[-4:]}\")\n",
    "log(f\"Using user: {username}\")\n",
    "\n",
    "# Avoid: Logging full credentials\n",
    "# log(f\"API token: {api_token}\")  # Exposes full token\n",
    "# log(f\"Password: {password}\")  # Never log passwords\n",
    "```\n",
    "\n",
    "### Secure File Permissions\n",
    "\n",
    "**Set appropriate permissions on exported files**:\n",
    "\n",
    "```python\n",
    "# For sensitive exports\n",
    "if os.path.exists(export_path):\n",
    "    os.chmod(export_path, 0o600)  # Owner read/write only\n",
    "    log(f\"✓ Set secure permissions on {export_path}\")\n",
    "```\n",
    "\n",
    "### Data Sanitization\n",
    "\n",
    "**Sanitize user input and file paths**:\n",
    "\n",
    "```python\n",
    "import re\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    \"\"\"Remove unsafe characters from filename\"\"\"\n",
    "    # Remove or replace unsafe characters\n",
    "    safe_name = re.sub(r'[^a-zA-Z0-9_.-]', '_', filename)\n",
    "    return safe_name\n",
    "\n",
    "# Usage\n",
    "user_input = dbutils.widgets.get(\"filename\")\n",
    "safe_filename = sanitize_filename(user_input)\n",
    "export_path = f\"{EXPORT_PATH}/{safe_filename}.xlsx\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d1a9ac4-5c45-40cd-bc47-25bd7d8c8153",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 22. Code Comments and Documentation\n",
    "\n",
    "### When to Comment\n",
    "\n",
    "**Comment the WHY, not the WHAT**:\n",
    "\n",
    "```python\n",
    "# Good: Explains reasoning\n",
    "# Convert timestamps from milliseconds to datetime if present\n",
    "# Databricks APIs return timestamps in epoch milliseconds\n",
    "if created_at:\n",
    "    created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "\n",
    "# Avoid: States the obvious\n",
    "# Convert created_at to timestamp\n",
    "if created_at:\n",
    "    created_timestamp = datetime.fromtimestamp(created_at / 1000, tz=eastern)\n",
    "```\n",
    "\n",
    "### Complex Logic Comments\n",
    "\n",
    "**Explain complex algorithms**:\n",
    "\n",
    "```python\n",
    "# Calculate risk score using weighted factors:\n",
    "# - High-risk items: +20 points each (security vulnerabilities)\n",
    "# - Medium-risk items: +10 points each (best practice violations)\n",
    "# - Low-risk items: +5 points each (optimization opportunities)\n",
    "# Score is capped at 100 to maintain consistent scale\n",
    "risk_score = 0\n",
    "for factor in risk_factors:\n",
    "    if factor['severity'] == 'HIGH':\n",
    "        risk_score += 20\n",
    "    elif factor['severity'] == 'MEDIUM':\n",
    "        risk_score += 10\n",
    "    else:\n",
    "        risk_score += 5\n",
    "risk_score = min(risk_score, 100)\n",
    "```\n",
    "\n",
    "### TODO Comments\n",
    "\n",
    "**Use TODO for future improvements**:\n",
    "\n",
    "```python\n",
    "# TODO: Add support for custom date ranges\n",
    "# TODO: Implement incremental refresh for large datasets\n",
    "# TODO(username): Review performance optimization for serverless\n",
    "\n",
    "# FIXME: Handle edge case where user has no groups\n",
    "# HACK: Temporary workaround for API pagination bug\n",
    "```\n",
    "\n",
    "### Deprecation Warnings\n",
    "\n",
    "**Document deprecated code**:\n",
    "\n",
    "```python\n",
    "def old_function():\n",
    "    \"\"\"\n",
    "    DEPRECATED: Use new_function() instead.\n",
    "    This function will be removed in version 2.0.\n",
    "    \"\"\"\n",
    "    import warnings\n",
    "    warnings.warn(\n",
    "        \"old_function is deprecated, use new_function instead\",\n",
    "        DeprecationWarning,\n",
    "        stacklevel=2\n",
    "    )\n",
    "    # ... implementation ...\n",
    "```\n",
    "\n",
    "### Section Dividers\n",
    "\n",
    "**Use consistent section dividers**:\n",
    "\n",
    "```python\n",
    "# ============================================================================\n",
    "# MAJOR SECTION: Brief description\n",
    "# ============================================================================\n",
    "\n",
    "# --- Subsection ---\n",
    "\n",
    "# Minor grouping (no divider needed, just comment)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74d4bde1-71bf-4dab-9b23-c2908fafec8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 23. Data Quality and Validation\n",
    "\n",
    "### Null Handling\n",
    "\n",
    "**Check for nulls before processing**:\n",
    "\n",
    "```python\n",
    "# Check for null values in critical columns\n",
    "null_counts = df.select([\n",
    "    F.sum(F.when(F.col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df.columns\n",
    "])\n",
    "\n",
    "if null_counts.first():\n",
    "    log(\"⚠️  Null values detected:\")\n",
    "    for col in df.columns:\n",
    "        null_count = null_counts.first()[col]\n",
    "        if null_count > 0:\n",
    "            log(f\"  - {col}: {null_count} nulls\")\n",
    "```\n",
    "\n",
    "### Data Type Validation\n",
    "\n",
    "**Validate expected data types**:\n",
    "\n",
    "```python\n",
    "# Validate schema matches expectations\n",
    "expected_schema = {\n",
    "    'asset_id': 'string',\n",
    "    'asset_name': 'string',\n",
    "    'created_timestamp': 'timestamp',\n",
    "    'owner': 'string'\n",
    "}\n",
    "\n",
    "for field in df.schema.fields:\n",
    "    expected_type = expected_schema.get(field.name)\n",
    "    if expected_type and str(field.dataType).lower() != expected_type:\n",
    "        log(f\"⚠️  Schema mismatch: {field.name} is {field.dataType}, expected {expected_type}\")\n",
    "```\n",
    "\n",
    "### Empty DataFrame Checks\n",
    "\n",
    "**Always check before processing**:\n",
    "\n",
    "```python\n",
    "if df is None:\n",
    "    log(\"⚠️  DataFrame is None, skipping processing\")\n",
    "elif df.count() == 0:\n",
    "    log(\"⚠️  DataFrame is empty (0 rows), skipping processing\")\n",
    "else:\n",
    "    # Process DataFrame\n",
    "    log(f\"✓ Processing {df.count()} rows\")\n",
    "```\n",
    "\n",
    "### Data Range Validation\n",
    "\n",
    "**Validate data is within expected ranges**:\n",
    "\n",
    "```python\n",
    "# Check for reasonable date ranges\n",
    "min_date = df.agg(F.min('created_timestamp')).first()[0]\n",
    "max_date = df.agg(F.max('created_timestamp')).first()[0]\n",
    "\n",
    "if min_date and max_date:\n",
    "    date_range_days = (max_date - min_date).days\n",
    "    if date_range_days > 3650:  # 10 years\n",
    "        log(f\"⚠️  Unusual date range: {date_range_days} days\")\n",
    "    else:\n",
    "        log(f\"✓ Date range: {date_range_days} days\")\n",
    "```\n",
    "\n",
    "### Duplicate Detection\n",
    "\n",
    "**Check for and handle duplicates**:\n",
    "\n",
    "```python\n",
    "# Check for duplicates\n",
    "initial_count = df.count()\n",
    "df_deduped = df.dropDuplicates(['asset_type', 'asset_id'])\n",
    "final_count = df_deduped.count()\n",
    "\n",
    "if initial_count > final_count:\n",
    "    duplicates = initial_count - final_count\n",
    "    log(f\"⚠️  Removed {duplicates} duplicate entries ({duplicates/initial_count*100:.1f}%)\")\n",
    "else:\n",
    "    log(\"✓ No duplicates found\")\n",
    "\n",
    "df = df_deduped\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9fec5741-b027-46b3-b6a0-e14a16123ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 24. String Formatting and Output\n",
    "\n",
    "### F-String Best Practices\n",
    "\n",
    "**Use f-strings for all formatting**:\n",
    "\n",
    "```python\n",
    "# Good: F-strings (Python 3.6+)\n",
    "log(f\"Processed {count} items in {elapsed:.2f} seconds\")\n",
    "log(f\"Success rate: {success_rate:.1f}%\")\n",
    "log(f\"Path: {catalog}.{schema}.{table}\")\n",
    "\n",
    "# Avoid: Old-style formatting\n",
    "log(\"Processed %d items\" % count)  # Old\n",
    "log(\"Processed {} items\".format(count))  # Verbose\n",
    "log(\"Processed \" + str(count) + \" items\")  # Concatenation\n",
    "```\n",
    "\n",
    "### Number Formatting\n",
    "\n",
    "**Consistent number formatting**:\n",
    "\n",
    "```python\n",
    "# Integers: No decimal places\n",
    "log(f\"Count: {count:,}\")  # 1,234,567\n",
    "\n",
    "# Floats: 1-2 decimal places\n",
    "log(f\"Percentage: {pct:.1f}%\")  # 85.3%\n",
    "log(f\"Time: {elapsed:.2f} seconds\")  # 12.45 seconds\n",
    "\n",
    "# Large numbers: Use K, M, B suffixes\n",
    "if count >= 1_000_000:\n",
    "    log(f\"Count: {count/1_000_000:.1f}M\")\n",
    "elif count >= 1_000:\n",
    "    log(f\"Count: {count/1_000:.1f}K\")\n",
    "else:\n",
    "    log(f\"Count: {count}\")\n",
    "```\n",
    "\n",
    "### Multi-line Strings\n",
    "\n",
    "**Use triple quotes for SQL and long text**:\n",
    "\n",
    "```python\n",
    "# Good: Triple quotes for SQL\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        asset_type,\n",
    "        COUNT(*) as count\n",
    "    FROM assets\n",
    "    WHERE modified_timestamp > current_date() - INTERVAL 30 DAYS\n",
    "    GROUP BY asset_type\n",
    "    ORDER BY count DESC\n",
    "\"\"\"\n",
    "\n",
    "# Good: Triple quotes for long messages\n",
    "message = \"\"\"\n",
    "Workspace scan completed successfully.\n",
    "Found {count} assets with default naming.\n",
    "Results exported to {path}.\n",
    "\"\"\".format(count=count, path=path)\n",
    "```\n",
    "\n",
    "### Path Formatting\n",
    "\n",
    "**Consistent path construction**:\n",
    "\n",
    "```python\n",
    "# Use f-strings for paths\n",
    "full_path = f\"{catalog}.{schema}.{table}\"\n",
    "file_path = f\"{EXPORT_PATH}/{filename}_{timestamp}.xlsx\"\n",
    "\n",
    "# Use os.path.join for file system paths\n",
    "import os\n",
    "file_path = os.path.join(EXPORT_PATH, f\"{filename}_{timestamp}.xlsx\")\n",
    "```\n",
    "\n",
    "### Unicode and Emojis\n",
    "\n",
    "**Use emojis consistently for visual clarity**:\n",
    "\n",
    "```python\n",
    "# Status indicators\n",
    "log(\"✓ Success\")  # Checkmark\n",
    "log(\"✗ Failure\")  # X mark\n",
    "log(\"⚠️  Warning\")  # Warning sign\n",
    "log(\"ℹ️  Info\")  # Information\n",
    "\n",
    "# Progress indicators\n",
    "log(\"⏱️  Timing information\")\n",
    "log(\"\uD83D\uDE80 Quick mode enabled\")\n",
    "log(\"\uD83D\uDD0D Full mode enabled\")\n",
    "log(\"\uD83E\uDD16 Job mode detected\")\n",
    "\n",
    "# Category indicators\n",
    "log(\"\uD83D\uDD12 Security\")\n",
    "log(\"\uD83D\uDCCB Governance\")\n",
    "log(\"\uD83D\uDCA1 Recommendations\")\n",
    "log(\"\uD83D\uDCCA Statistics\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5b6b71f-13db-49c4-94a8-ab109c1109b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 25. Spark DataFrame Best Practices\n",
    "\n",
    "### DataFrame Creation\n",
    "\n",
    "**Always define schema explicitly**:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "# Good: Explicit schema\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType(), False),  # Not nullable\n",
    "    StructField(\"name\", StringType(), True),  # Nullable\n",
    "    StructField(\"count\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", TimestampType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Avoid: Schema inference (slower, less reliable)\n",
    "# df = spark.createDataFrame(data)  # Schema inferred\n",
    "```\n",
    "\n",
    "### Column Operations\n",
    "\n",
    "**Use F.col() for column references**:\n",
    "\n",
    "```python\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Good: Use F.col()\n",
    "df = df.filter(F.col('status') == 'active')\n",
    "df = df.withColumn('age_days', F.datediff(F.current_date(), F.col('created_date')))\n",
    "\n",
    "# Acceptable: String column names in simple cases\n",
    "df = df.select('id', 'name', 'status')\n",
    "df = df.groupBy('category').count()\n",
    "```\n",
    "\n",
    "### Avoid Collect on Large DataFrames\n",
    "\n",
    "**Use display() or limit() instead**:\n",
    "\n",
    "```python\n",
    "# Good: Display limited results\n",
    "display(df.limit(100))\n",
    "\n",
    "# Good: Aggregate before collecting\n",
    "summary = df.groupBy('category').count().collect()\n",
    "\n",
    "# Avoid: Collecting large DataFrames\n",
    "# all_data = df.collect()  # Can cause OOM on large data\n",
    "```\n",
    "\n",
    "### Column Naming\n",
    "\n",
    "**Use snake_case for column names**:\n",
    "\n",
    "```python\n",
    "# Good: snake_case\n",
    "df = df.withColumnRenamed('AssetType', 'asset_type')\n",
    "df = df.withColumnRenamed('CreatedAt', 'created_at')\n",
    "\n",
    "# Avoid: camelCase or PascalCase in Spark\n",
    "# df = df.withColumn('assetType', ...)  # Inconsistent\n",
    "```\n",
    "\n",
    "### Chaining Operations\n",
    "\n",
    "**Chain operations for readability**:\n",
    "\n",
    "```python\n",
    "# Good: Chained operations\n",
    "df_result = (df\n",
    "    .filter(F.col('status') == 'active')\n",
    "    .withColumn('age_days', F.datediff(F.current_date(), F.col('created_date')))\n",
    "    .groupBy('category')\n",
    "    .agg(\n",
    "        F.count('*').alias('count'),\n",
    "        F.avg('age_days').alias('avg_age')\n",
    "    )\n",
    "    .orderBy(F.desc('count'))\n",
    ")\n",
    "\n",
    "# Avoid: Multiple intermediate variables\n",
    "# df1 = df.filter(F.col('status') == 'active')\n",
    "# df2 = df1.withColumn('age_days', ...)\n",
    "# df3 = df2.groupBy('category')\n",
    "# df_result = df3.agg(...)\n",
    "```\n",
    "\n",
    "### Null-Safe Operations\n",
    "\n",
    "**Handle nulls explicitly**:\n",
    "\n",
    "```python\n",
    "# Use coalesce for null defaults\n",
    "df = df.withColumn('owner', F.coalesce(F.col('owner'), F.lit('Unknown')))\n",
    "\n",
    "# Use null-safe equality\n",
    "df = df.filter(F.col('status').eqNullSafe('active'))\n",
    "\n",
    "# Filter out nulls explicitly\n",
    "df = df.filter(F.col('required_field').isNotNull())\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Coding Standards",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
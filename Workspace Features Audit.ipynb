{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b83330-cef1-4550-9563-e4afb42347be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Documentation Header"
    }
   },
   "source": [
    "# Databricks Workspace Features Audit\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **comprehensive inventory** of all features and capabilities enabled in your Databricks workspace. The output includes detailed reports showing workspace configuration flags, Unity Catalog resources, compute capabilities, collaboration tools, security features, and data sharing configurations.\n",
    "\n",
    "**✨ Production-ready workspace auditing with parallel execution, retry logic, export capabilities, and historical tracking.**\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Modes\n",
    "\n",
    "### \uD83D\uDE80 Quick Mode (<1 second)\n",
    "**Recommended for**: Daily checks, quick status reviews, testing, **interactive development**\n",
    "\n",
    "* Workspace configuration flags only\n",
    "* No resource enumeration (jobs, warehouses, etc.)\n",
    "* Fastest execution\n",
    "* **Skips**: Unity Catalog, Compute, Collaboration, Security resource counts\n",
    "* **To enable**: Set `USE_FULL_MODE = False` in Cell 3\n",
    "\n",
    "### \uD83D\uDD0D Full Mode (10-30 seconds)\n",
    "**Recommended for**: Complete audits, compliance reviews, comprehensive analysis, **scheduled jobs**\n",
    "\n",
    "* **ALL** feature categories enabled\n",
    "* Complete workspace coverage including:\n",
    "  * Workspace configuration flags\n",
    "  * Unity Catalog resources (catalogs, external locations, storage credentials)\n",
    "  * Compute resources (SQL warehouses, serverless, cluster policies)\n",
    "  * Collaboration tools (repos, jobs, pipelines, model serving)\n",
    "  * Security features (groups, service principals, system tables)\n",
    "  * Data sharing (Delta Sharing, volumes)\n",
    "* **\uD83E\uDD16 AUTOMATIC IN JOB MODE**: Jobs always run in Full Mode for comprehensive audits\n",
    "* **To enable**: Set `USE_FULL_MODE = True` in Cell 3 (default)\n",
    "\n",
    "### ⚙️ Custom Mode (Variable)\n",
    "**Recommended for**: Specific use cases, targeted audits, **interactive sessions**\n",
    "\n",
    "* Set `USE_FULL_MODE = True` and configure individual enable/disable flags in Cell 3\n",
    "* Fine-tune which feature categories to check\n",
    "* Balance performance vs coverage\n",
    "\n",
    "**To select a mode**: Edit Cell 3 and set `USE_FULL_MODE = True` (Full) or `False` (Quick)\n",
    "\n",
    "---\n",
    "\n",
    "## What This Code Does\n",
    "\n",
    "### 1. **Workspace Configuration Flags**\n",
    "* Checks 18 validated workspace configuration settings\n",
    "* Identifies enabled/disabled features:\n",
    "  * Personal access tokens\n",
    "  * Web terminal access\n",
    "  * DBFS file browser\n",
    "  * Verbose audit logs\n",
    "  * Workspace filesystem\n",
    "  * Git integration\n",
    "  * And more...\n",
    "* Provides descriptions for each configuration flag\n",
    "\n",
    "### 2. **Unity Catalog Resources** (Full Mode only)\n",
    "* **Catalogs**: Count and list of all Unity Catalog catalogs\n",
    "* **External Locations**: Cloud storage access configurations\n",
    "* **Storage Credentials**: Authentication credentials for cloud storage\n",
    "* **System Catalog**: Audit logs, billing, and lineage availability\n",
    "* Validates Unity Catalog enablement status\n",
    "\n",
    "### 3. **Compute Capabilities** (Full Mode only)\n",
    "* **SQL Warehouses**: Total count and serverless detection\n",
    "* **Serverless SQL**: Identifies serverless-enabled warehouses\n",
    "* **Cluster Policies**: Governance policies for cluster configuration\n",
    "* **Instance Pools**: Pre-provisioned compute pools\n",
    "* **Active Clusters**: Currently running interactive clusters (optional, can be slow)\n",
    "\n",
    "### 4. **Collaboration & Workflows** (Full Mode only)\n",
    "* **Repos (Git Integration)**: Git repository count and status\n",
    "* **Jobs/Workflows**: Scheduled and triggered job count\n",
    "* **Delta Live Tables**: DLT pipeline count\n",
    "* **MLflow Experiments**: Active experiment tracking\n",
    "* **Registered Models**: ML models in Model Registry\n",
    "* **Model Serving**: Real-time serving endpoint count\n",
    "\n",
    "### 5. **Security & Governance** (Full Mode only)\n",
    "* **Users**: Total and active user counts\n",
    "* **Groups**: User group count for permission management\n",
    "* **Service Principals**: Non-human identity count\n",
    "* **Secret Scopes**: Secure credential storage count\n",
    "* **IP Access Lists**: Network access control status\n",
    "\n",
    "### 6. **Data Sharing & Marketplace** (Full Mode only)\n",
    "* **Delta Sharing (Providers)**: Outbound data sharing count\n",
    "* **Delta Sharing (Recipients)**: External consumer count\n",
    "* **Data Providers**: Inbound data source count\n",
    "\n",
    "### 7. **Workspace Capabilities** (Full Mode only)\n",
    "* **Notification Destinations**: Alert integration count (Slack, email, webhooks, PagerDuty)\n",
    "* **Lakeview Dashboards**: Modern BI dashboard count\n",
    "\n",
    "### 8. **Feature Analysis & Visualization**\n",
    "* **Category Summaries**: Features grouped by category\n",
    "* **Enabled vs Available**: Clear distinction of active features\n",
    "* **Visual Charts**: Feature distribution and enabled/disabled breakdown\n",
    "* **Export Capabilities**: Excel and Delta table exports with audit metadata\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "✓ **Automatic Job Mode**: Jobs always run Full Mode for comprehensive audits  \n",
    "✓ **Simple Configuration**: Single `USE_FULL_MODE` variable to switch between modes  \n",
    "✓ **Performance Presets**: Quick Mode (<1s) or Full Mode (10-30s)  \n",
    "✓ **Parallel Execution**: ThreadPoolExecutor with configurable workers (10 concurrent API calls)  \n",
    "✓ **Retry Logic**: Automatic retry with exponential backoff for transient API failures  \n",
    "✓ **Timeout Handling**: Prevents hanging on slow API calls (30s timeout)  \n",
    "✓ **Progress Tracking**: Real-time progress updates during execution  \n",
    "✓ **Execution Statistics**: Tracks API calls, failures, retries, success rates  \n",
    "✓ **Error Resilience**: Continues execution even if some checks fail  \n",
    "✓ **Feature Descriptions**: Every feature includes a clear description  \n",
    "✓ **Category Organization**: Features grouped by logical categories  \n",
    "✓ **Multiple Export Formats**: Excel workbooks and Delta tables  \n",
    "✓ **Historical Tracking**: Delta table export for trend analysis  \n",
    "✓ **Audit Metadata**: Timestamps, execution time, API statistics  \n",
    "✓ **Configuration Validation**: Validates settings before execution  \n",
    "✓ **Job Mode Detection**: Automatically detects scheduled vs interactive execution  \n",
    "✓ **Serverless Detection**: Automatically detects compute type and optimizes accordingly  \n",
    "✓ **Compute-Aware Optimization**: Adapts behavior for serverless vs traditional clusters  \n",
    "✓ **Visual Analytics**: Feature distribution charts and enabled/disabled breakdown  \n",
    "\n",
    "---\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Execution Mode (Cell 3):\n",
    "* `USE_FULL_MODE = True` - Complete audit with all resources (10-30 seconds) **[DEFAULT]**\n",
    "* `USE_FULL_MODE = False` - Fast config-only checks (<1 second)\n",
    "* `IS_JOB_MODE` - Auto-detected (scheduled job vs interactive)\n",
    "* `IS_SERVERLESS` - Auto-detected (serverless vs traditional cluster)\n",
    "* `TIMEZONE` - Timezone for timestamps (default: 'America/New_York')\n",
    "\n",
    "### Performance Settings (Cell 3):\n",
    "* `MAX_WORKERS = 10` - Parallel API calls (1-20)\n",
    "* `MAX_RETRIES = 3` - Number of retries for failed API calls\n",
    "* `RETRY_DELAY = 1` - Seconds to wait between retries\n",
    "* `API_TIMEOUT = 30` - Timeout for API calls in seconds\n",
    "\n",
    "### Feature Category Selection (Cell 3):\n",
    "**For FULL MODE only:**\n",
    "* `ENABLE_UNITY_CATALOG = True` - Check Unity Catalog resources\n",
    "* `ENABLE_COMPUTE = True` - Check compute resources\n",
    "* `ENABLE_COLLABORATION = True` - Check collaboration tools\n",
    "* `ENABLE_SECURITY = True` - Check security features\n",
    "* `ENABLE_DATA_SHARING = True` - Check data sharing features\n",
    "\n",
    "### Optional Checks (Cell 3):\n",
    "* `ENABLE_CLUSTER_CHECK = False` - Check active clusters (⚠️ can be VERY slow with 1000+ clusters)\n",
    "\n",
    "### Export Settings (Cell 3):\n",
    "* `ENABLE_EXCEL_EXPORT = False` - Export results to Excel workbook\n",
    "* `ENABLE_DELTA_EXPORT = False` - Save to Delta table for historical tracking\n",
    "* `EXPORT_PATH = \"/dbfs/tmp/workspace_features_export\"` - Export directory\n",
    "* `DELTA_TABLE_NAME = \"main.default.workspace_features_audit\"` - Delta table name\n",
    "\n",
    "---\n",
    "\n",
    "## Output\n",
    "\n",
    "### Interactive Display:\n",
    "* **Main Table**: All features with Category, Feature, Description, Value, Enabled columns\n",
    "* **Summary Table**: Feature counts by category (Total and Enabled)\n",
    "* **Execution Statistics**: API calls, failures, retries, success rate, execution time\n",
    "* **Visual Charts**: Feature distribution and enabled/disabled breakdown\n",
    "\n",
    "### Excel Export (if enabled):\n",
    "* **Features Sheet**: Complete feature inventory\n",
    "* **Summary Sheet**: Execution statistics and metrics\n",
    "* **Timestamped Files**: `workspace_features_YYYYMMDD_HHMMSS.xlsx`\n",
    "\n",
    "### Delta Table Export (if enabled):\n",
    "* **Historical Tracking**: Append-mode for trend analysis\n",
    "* **Audit Metadata**: Timestamps, execution time, API statistics\n",
    "* **Queryable History**: SQL queries for feature changes over time\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Interactive Execution:\n",
    "1. Configure settings in Cell 3 (mode, exports, feature categories)\n",
    "2. Run Cell 3 to load configuration\n",
    "3. Run Cell 4 to execute the feature audit\n",
    "4. Run Cell 5 (optional) to export results\n",
    "5. Run Cell 6 (optional) for visualizations\n",
    "6. Review results in displayed tables\n",
    "\n",
    "### Scheduled Job Execution:\n",
    "1. Create a Databricks job pointing to this notebook\n",
    "2. Job automatically runs in Full Mode (overrides `USE_FULL_MODE = False` if set)\n",
    "3. Enable exports in Cell 3 for automated reporting\n",
    "4. Results saved to Excel and/or Delta table\n",
    "5. Schedule daily/weekly for compliance tracking\n",
    "\n",
    "### Job Parameters (optional):\n",
    "See Cell 12 for widget configuration examples:\n",
    "* `use_full_mode` - Override USE_FULL_MODE setting (\"true\" or \"false\")\n",
    "* `max_workers` - Override MAX_WORKERS setting\n",
    "* `enable_excel_export` - Override ENABLE_EXCEL_EXPORT (\"true\" or \"false\")\n",
    "* `enable_delta_export` - Override ENABLE_DELTA_EXPORT (\"true\" or \"false\")\n",
    "* `enable_cluster_check` - Override ENABLE_CLUSTER_CHECK (\"true\" or \"false\")\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Optimization Tips\n",
    "\n",
    "### For Faster Execution:\n",
    "1. **Use Quick Mode**: Set `USE_FULL_MODE = False` (<1 second)\n",
    "2. **Reduce workers**: Set `MAX_WORKERS = 5` if API rate limits are hit\n",
    "3. **Skip slow checks**: Keep `ENABLE_CLUSTER_CHECK = False` (default)\n",
    "4. **Disable exports**: Set both exports to False for fastest execution\n",
    "\n",
    "### For Complete Coverage:\n",
    "1. **Use Full Mode**: Set `USE_FULL_MODE = True` (default)\n",
    "2. **Enable all categories**: All ENABLE_* flags = True (default)\n",
    "3. **Increase workers**: Set `MAX_WORKERS = 15-20` for faster parallel execution\n",
    "4. **Enable cluster check**: Set `ENABLE_CLUSTER_CHECK = True` (only if needed)\n",
    "\n",
    "### Compute Type Considerations:\n",
    "* **Serverless**: Automatic memory management, no caching needed, fast startup\n",
    "* **Traditional Cluster**: Manual memory management, caching available if needed\n",
    "* **Detection**: Automatic - notebook adapts to compute type\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### High API Failure Rate:\n",
    "* Check workspace permissions (need admin or appropriate read access)\n",
    "* Verify network connectivity\n",
    "* Review retry settings (increase MAX_RETRIES if transient failures)\n",
    "* Check API rate limits (reduce MAX_WORKERS if hitting limits)\n",
    "\n",
    "### Slow Execution:\n",
    "* Use Quick Mode for faster results\n",
    "* Disable ENABLE_CLUSTER_CHECK (can take 30+ minutes with many clusters)\n",
    "* Reduce MAX_WORKERS if causing resource contention\n",
    "* Check for slow API endpoints in execution statistics\n",
    "\n",
    "### Missing Features:\n",
    "* Verify USE_FULL_MODE = True for complete coverage\n",
    "* Check ENABLE_* flags for specific categories\n",
    "* Review API failures in execution statistics\n",
    "* Ensure appropriate workspace permissions\n",
    "\n",
    "### Export Failures:\n",
    "* Verify export path exists and is writable\n",
    "* Check Delta table permissions (CREATE TABLE required)\n",
    "* Ensure openpyxl package is installed for Excel export\n",
    "* Review error messages in execution output\n",
    "\n",
    "---\n",
    "\n",
    "## Version History\n",
    "\n",
    "| Version | Date | Author | Changes |\n",
    "|---------|------|--------|----------|\n",
    "| 1.0.0 | 2026-02-12 | Brandon Croom | Initial production release: Comprehensive workspace feature auditing with 42+ feature checks across 8 categories; Parallel execution with ThreadPoolExecutor (10 concurrent workers); Retry logic with exponential backoff for API resilience; Quick Mode (config-only, <1s) and Full Mode (complete audit, 10-30s) support; Job mode detection with automatic Full Mode override; Serverless vs traditional cluster detection with compute-aware optimizations; Excel and Delta table exports with audit metadata; Execution statistics tracking (API calls, failures, retries, success rate); Configuration validation and error handling; Feature descriptions for all checks; Category-based organization; Visual analytics with matplotlib charts; Removed 14 invalid workspace config keys for 100% success rate; Added comprehensive resource checks: Unity Catalog (catalogs, external locations, storage credentials, system catalog), Compute (SQL warehouses, serverless detection, cluster policies, instance pools), Collaboration (jobs, repos, Delta Live Tables), ML/AI (model serving endpoints, registered models, MLflow experiments), Security (users, groups, service principals, secret scopes, IP access lists), Data Sharing (providers, recipients, data providers), Workspace Capabilities (notification destinations, Lakeview dashboards); Optional cluster check (disabled by default due to performance); Complete documentation with usage guide, troubleshooting, and best practices |\n",
    "\n",
    "---\n",
    "\n",
    "## Support\n",
    "\n",
    "For questions or issues:\n",
    "* **Performance**: Use Quick Mode for faster execution\n",
    "* **Coverage**: Use Full Mode for complete feature inventory\n",
    "* **Compute**: Notebook automatically detects and optimizes for serverless vs traditional clusters\n",
    "* Review execution statistics for API failures and retry information\n",
    "* Check configuration validation messages for setup issues\n",
    "* Contact your Databricks administrator for permission issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e342701-5c15-4a5a-9254-029eb43aed63",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Install required packages"
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c511aba0-adcf-498e-8a1a-c4ddc3162889",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration and execution modes"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PRODUCTION CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Detect if running in job mode or interactive mode\n",
    "try:\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().currentRunId().isDefined()\n",
    "    IS_JOB_MODE = True\n",
    "except:\n",
    "    IS_JOB_MODE = False\n",
    "\n",
    "# Detect if running on serverless compute (most reliable method: try caching)\n",
    "try:\n",
    "    test_df = spark.range(1)\n",
    "    test_df.cache()\n",
    "    test_df.count()\n",
    "    test_df.unpersist()\n",
    "    IS_SERVERLESS = False\n",
    "except Exception as e:\n",
    "    # If caching fails with serverless error, we're on serverless\n",
    "    IS_SERVERLESS = 'SERVERLESS' in str(e) or 'PERSIST TABLE is not supported' in str(e)\n",
    "\n",
    "# ============================================================================\n",
    "# TIMEZONE CONFIGURATION\n",
    "# ============================================================================\n",
    "# All timestamps will be displayed in this timezone\n",
    "TIMEZONE = 'America/New_York'  # Eastern Time\n",
    "# Common values: 'America/New_York' (Eastern), 'America/Chicago' (Central),\n",
    "#                'America/Denver' (Mountain), 'America/Los_Angeles' (Pacific),\n",
    "#                'UTC', 'Europe/London', etc.\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION MODE: Choose your execution speed vs completeness\n",
    "# ============================================================================\n",
    "\n",
    "# Set to True for comprehensive audit (10-30 seconds)\n",
    "# Set to False for quick config-only checks (<1 second)\n",
    "USE_FULL_MODE = True\n",
    "\n",
    "# FULL MODE (True):\n",
    "# - Checks all workspace features\n",
    "# - Enumerates all resources (Unity Catalog, Compute, Collaboration, Security)\n",
    "# - Provides comprehensive feature inventory\n",
    "# - Recommended for: Complete audits, compliance reviews, scheduled jobs\n",
    "\n",
    "# QUICK MODE (False):\n",
    "# - Checks only workspace config flags\n",
    "# - Skips resource enumeration (jobs, warehouses, etc.)\n",
    "# - Fastest execution\n",
    "# - Recommended for: Daily checks, quick status reviews, testing\n",
    "\n",
    "# ============================================================================\n",
    "# PERFORMANCE SETTINGS\n",
    "# ============================================================================\n",
    "MAX_WORKERS = 10  # Parallel API calls (1-20)\n",
    "MAX_RETRIES = 3  # Number of retries for failed API calls\n",
    "RETRY_DELAY = 1  # Seconds to wait between retries\n",
    "API_TIMEOUT = 30  # Timeout for API calls in seconds\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT SETTINGS\n",
    "# ============================================================================\n",
    "ENABLE_EXCEL_EXPORT = True  # Export results to Excel\n",
    "ENABLE_DELTA_EXPORT = False  # Save to Delta table for historical tracking\n",
    "EXPORT_PATH = \"/dbfs/tmp/workspace_features_export\"\n",
    "DELTA_TABLE_NAME = \"main.default.workspace_features_audit\"\n",
    "\n",
    "# ============================================================================\n",
    "# RESOURCE TYPE SELECTION (for FULL MODE only)\n",
    "# ============================================================================\n",
    "ENABLE_UNITY_CATALOG = True\n",
    "ENABLE_COMPUTE = True\n",
    "ENABLE_COLLABORATION = True\n",
    "ENABLE_SECURITY = True\n",
    "ENABLE_DATA_SHARING = True\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL CHECKS (can be slow in large workspaces)\n",
    "# ============================================================================\n",
    "ENABLE_CLUSTER_CHECK = False  # ⚠️ Can be VERY slow if you have 1000+ clusters\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_configuration():\n",
    "    \"\"\"Validate configuration settings and resolve conflicts\"\"\"\n",
    "    errors = []\n",
    "    warnings = []\n",
    "    \n",
    "    # Validate execution mode\n",
    "    if not isinstance(USE_FULL_MODE, bool):\n",
    "        errors.append(\"USE_FULL_MODE must be a boolean (True or False)\")\n",
    "    \n",
    "    # Validate performance settings\n",
    "    if not isinstance(MAX_WORKERS, int) or MAX_WORKERS < 1 or MAX_WORKERS > 20:\n",
    "        errors.append(\"MAX_WORKERS must be an integer between 1 and 20\")\n",
    "    \n",
    "    if not isinstance(MAX_RETRIES, int) or MAX_RETRIES < 0:\n",
    "        errors.append(\"MAX_RETRIES must be a non-negative integer\")\n",
    "    \n",
    "    if not isinstance(RETRY_DELAY, (int, float)) or RETRY_DELAY < 0:\n",
    "        errors.append(\"RETRY_DELAY must be a non-negative number\")\n",
    "    \n",
    "    if not isinstance(API_TIMEOUT, (int, float)) or API_TIMEOUT < 1:\n",
    "        errors.append(\"API_TIMEOUT must be at least 1 second\")\n",
    "    \n",
    "    # Validate export settings\n",
    "    if ENABLE_DELTA_EXPORT:\n",
    "        if not DELTA_TABLE_NAME or '.' not in DELTA_TABLE_NAME:\n",
    "            errors.append(\"DELTA_TABLE_NAME must be in format 'catalog.schema.table'\")\n",
    "        parts = DELTA_TABLE_NAME.split('.')\n",
    "        if len(parts) != 3:\n",
    "            errors.append(f\"DELTA_TABLE_NAME must have exactly 3 parts (catalog.schema.table), got {len(parts)} parts\")\n",
    "    \n",
    "    # Job mode override\n",
    "    if IS_JOB_MODE and not USE_FULL_MODE:\n",
    "        warnings.append(\"Job mode detected: Overriding to FULL MODE (jobs always run comprehensive audits)\")\n",
    "        globals()['USE_FULL_MODE'] = True\n",
    "    \n",
    "    # Cluster check warning\n",
    "    if ENABLE_CLUSTER_CHECK:\n",
    "        warnings.append(\"Cluster check enabled: This may take several minutes in large workspaces\")\n",
    "    \n",
    "    return errors, warnings\n",
    "\n",
    "# Run validation\n",
    "config_errors, config_warnings = validate_configuration()\n",
    "\n",
    "# Display warnings\n",
    "if config_warnings:\n",
    "    for warning in config_warnings:\n",
    "        print(f\"⚠️  WARNING: {warning}\")\n",
    "\n",
    "# Raise errors if any\n",
    "if config_errors:\n",
    "    error_msg = \"Configuration validation failed:\\n\" + \"\\n\".join(f\"  ✖ {e}\" for e in config_errors)\n",
    "    raise ValueError(error_msg)\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITIES\n",
    "# ============================================================================\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Print only in interactive mode\"\"\"\n",
    "    if not IS_JOB_MODE:\n",
    "        print(message)\n",
    "\n",
    "def log_error(message):\n",
    "    \"\"\"Always print errors\"\"\"\n",
    "    print(f\"✖ ERROR: {message}\")\n",
    "\n",
    "def get_current_time_in_timezone():\n",
    "    \"\"\"Get current timestamp in configured timezone\"\"\"\n",
    "    result = spark.sql(f\"\"\"\n",
    "        SELECT from_utc_timestamp(current_timestamp(), '{TIMEZONE}') as current_time\n",
    "    \"\"\").collect()[0]['current_time']\n",
    "    return result\n",
    "\n",
    "log(\"✓ Configuration loaded and validated\")\n",
    "log(f\"Mode: {'JOB' if IS_JOB_MODE else 'INTERACTIVE'}\")\n",
    "log(f\"Execution: {'FULL' if USE_FULL_MODE else 'QUICK'} MODE\")\n",
    "log(f\"Compute: {'SERVERLESS' if IS_SERVERLESS else 'TRADITIONAL CLUSTER'}\")\n",
    "log(f\"Timezone: {TIMEZONE}\")\n",
    "log(f\"Performance: MAX_WORKERS={MAX_WORKERS}, MAX_RETRIES={MAX_RETRIES}\")\n",
    "\n",
    "if IS_SERVERLESS:\n",
    "    log(\"\\n⚡ Serverless optimizations:\")\n",
    "    log(\"  - Automatic memory management\")\n",
    "    log(\"  - Fast startup and scaling\")\n",
    "    log(\"  - No explicit caching needed\")\n",
    "else:\n",
    "    log(\"\\n\uD83D\uDD27 Traditional cluster:\")\n",
    "    log(\"  - Manual memory management available\")\n",
    "    log(\"  - Persistent compute resources\")\n",
    "    log(\"  - Caching available if needed\")\n",
    "\n",
    "if ENABLE_CLUSTER_CHECK:\n",
    "    log(\"⚠️  Cluster check enabled (may be slow)\")\n",
    "if ENABLE_EXCEL_EXPORT:\n",
    "    log(f\"\uD83D\uDCCA Excel export enabled: {EXPORT_PATH}\")\n",
    "if ENABLE_DELTA_EXPORT:\n",
    "    log(f\"\uD83D\uDCBE Delta export enabled: {DELTA_TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00488b70-a4f0-4ddf-ada5-dd59ef50fce9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enhanced parallel feature check with retry logic"
    }
   },
   "outputs": [],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, TimeoutError\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "w = WorkspaceClient()\n",
    "\n",
    "# Execution statistics\n",
    "execution_stats = {\n",
    "    'start_time': time.time(),\n",
    "    'api_calls': 0,\n",
    "    'api_failures': 0,\n",
    "    'api_retries': 0,\n",
    "    'features_found': 0\n",
    "}\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"WORKSPACE FEATURES AUDIT\")\n",
    "log(\"=\"*60)\n",
    "log(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "# Feature descriptions - ONLY VALID KEYS\n",
    "feature_descriptions = {\n",
    "    'enableTokensConfig': 'Allows creation and management of personal access tokens',\n",
    "    'enableProjectTypeInWorkspace': 'Enables project type classification in workspace',\n",
    "    'enableWorkspaceFilesystem': 'Enables workspace file system for storing files and folders',\n",
    "    'enableNotebookTableClipboard': 'Allows copying table results to clipboard from notebooks',\n",
    "    'enableResultsDownloading': 'Allows downloading query and notebook results',\n",
    "    'enableExportNotebook': 'Allows exporting notebooks in various formats',\n",
    "    'enableWebTerminal': 'Enables web-based terminal access to clusters',\n",
    "    'enableGp3': 'Enables GP3 storage volumes (AWS EBS volume type)',\n",
    "    'enableIpAccessLists': 'Enables IP access list restrictions for workspace',\n",
    "    'enableVerboseAuditLogs': 'Enables detailed audit logging for compliance',\n",
    "    'enableDcs': 'Enables Databricks Container Services for custom containers',\n",
    "    'enableDbfsFileBrowser': 'Enables DBFS file browser in workspace UI',\n",
    "    'enableDeprecatedClusterNamedInitScripts': 'Allows deprecated cluster-named init scripts',\n",
    "    'enableDeprecatedGlobalInitScripts': 'Allows deprecated global init scripts',\n",
    "    'maxTokenLifetimeDays': 'Maximum lifetime for personal access tokens (in days)',\n",
    "    'storeInteractiveNotebookResultsInCustomerAccount': 'Stores notebook results in customer storage account',\n",
    "    'enforceUserIsolation': 'Enforces user isolation for enhanced security',\n",
    "    'enableNotebookGitVersioning': 'Enables Git integration for notebook versioning'\n",
    "}\n",
    "\n",
    "def check_workspace_config_with_retry(key, max_retries=MAX_RETRIES):\n",
    "    \"\"\"Check a single workspace configuration key with retry logic\"\"\"\n",
    "    execution_stats['api_calls'] += 1\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            result = w.workspace_conf.get_status(keys=key)\n",
    "            if key in result:\n",
    "                value = result[key]\n",
    "                execution_stats['features_found'] += 1\n",
    "                return {\n",
    "                    'Category': 'Workspace Config',\n",
    "                    'Feature': key,\n",
    "                    'Description': feature_descriptions.get(key, 'No description available'),\n",
    "                    'Value': str(value) if value is not None else 'null',\n",
    "                    'Enabled': str(value).lower() in ['true', '1', 'enabled']\n",
    "                }\n",
    "        except Exception as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                execution_stats['api_retries'] += 1\n",
    "                time.sleep(RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                execution_stats['api_failures'] += 1\n",
    "    return None\n",
    "\n",
    "def check_unity_catalog_with_retry():\n",
    "    \"\"\"Check Unity Catalog features with retry logic\"\"\"\n",
    "    if not ENABLE_UNITY_CATALOG or not USE_FULL_MODE:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        catalogs = list(w.catalogs.list())\n",
    "        results.append({\n",
    "            'Category': 'Unity Catalog',\n",
    "            'Feature': 'Catalogs',\n",
    "            'Description': 'Top-level containers for organizing schemas and tables',\n",
    "            'Value': str(len(catalogs)),\n",
    "            'Enabled': len(catalogs) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check Unity Catalog: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        ext_locations = list(w.external_locations.list())\n",
    "        results.append({\n",
    "            'Category': 'Unity Catalog',\n",
    "            'Feature': 'External Locations',\n",
    "            'Description': 'Named references to cloud storage paths',\n",
    "            'Value': str(len(ext_locations)),\n",
    "            'Enabled': len(ext_locations) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except:\n",
    "        execution_stats['api_failures'] += 1\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        storage_creds = list(w.storage_credentials.list())\n",
    "        results.append({\n",
    "            'Category': 'Unity Catalog',\n",
    "            'Feature': 'Storage Credentials',\n",
    "            'Description': 'Authentication credentials for cloud storage',\n",
    "            'Value': str(len(storage_creds)),\n",
    "            'Enabled': len(storage_creds) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except:\n",
    "        execution_stats['api_failures'] += 1\n",
    "    \n",
    "    # Check for system catalog (audit logs)\n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        system_catalog = w.catalogs.get('system')\n",
    "        results.append({\n",
    "            'Category': 'Unity Catalog',\n",
    "            'Feature': 'System Catalog',\n",
    "            'Description': 'Built-in catalog for audit logs, billing, and lineage',\n",
    "            'Value': 'Available',\n",
    "            'Enabled': True\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except:\n",
    "        results.append({\n",
    "            'Category': 'Unity Catalog',\n",
    "            'Feature': 'System Catalog',\n",
    "            'Description': 'Built-in catalog for audit logs, billing, and lineage',\n",
    "            'Value': 'Not Available',\n",
    "            'Enabled': False\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_compute_with_retry():\n",
    "    \"\"\"Check compute features with retry logic\"\"\"\n",
    "    if not ENABLE_COMPUTE or not USE_FULL_MODE:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        warehouses = list(w.warehouses.list())\n",
    "        results.append({\n",
    "            'Category': 'Compute',\n",
    "            'Feature': 'SQL Warehouses',\n",
    "            'Description': 'Compute engines optimized for SQL queries',\n",
    "            'Value': str(len(warehouses)),\n",
    "            'Enabled': len(warehouses) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "        \n",
    "        serverless_count = sum(1 for wh in warehouses if wh.enable_serverless_compute)\n",
    "        if serverless_count > 0:\n",
    "            results.append({\n",
    "                'Category': 'Compute',\n",
    "                'Feature': 'Serverless SQL',\n",
    "                'Description': 'Instant-on SQL compute with automatic scaling',\n",
    "                'Value': str(serverless_count),\n",
    "                'Enabled': True\n",
    "            })\n",
    "            execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check SQL warehouses: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        policies = list(w.cluster_policies.list())\n",
    "        results.append({\n",
    "            'Category': 'Compute',\n",
    "            'Feature': 'Cluster Policies',\n",
    "            'Description': 'Templates that enforce cluster configuration rules',\n",
    "            'Value': str(len(policies)),\n",
    "            'Enabled': len(policies) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check cluster policies: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        pools = list(w.instance_pools.list())\n",
    "        results.append({\n",
    "            'Category': 'Compute',\n",
    "            'Feature': 'Instance Pools',\n",
    "            'Description': 'Pre-provisioned VM instances for faster cluster startup',\n",
    "            'Value': str(len(pools)),\n",
    "            'Enabled': len(pools) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check instance pools: {str(e)[:100]}\")\n",
    "    \n",
    "    # Check active clusters ONLY if explicitly enabled (can be very slow)\n",
    "    if ENABLE_CLUSTER_CHECK:\n",
    "        try:\n",
    "            execution_stats['api_calls'] += 1\n",
    "            log(\"⚠️  Checking clusters (this may take several minutes)...\")\n",
    "            clusters = list(w.clusters.list())\n",
    "            active_clusters = [c for c in clusters[:1000] if c.state and 'RUNNING' in str(c.state)]\n",
    "            total_note = f\" (sampled first 1000)\" if len(clusters) > 1000 else \"\"\n",
    "            results.append({\n",
    "                'Category': 'Compute',\n",
    "                'Feature': 'Active Clusters',\n",
    "                'Description': 'Currently running interactive clusters',\n",
    "                'Value': f\"{len(active_clusters)} running / {min(len(clusters), 1000)} checked{total_note}\",\n",
    "                'Enabled': len(active_clusters) > 0\n",
    "            })\n",
    "            execution_stats['features_found'] += 1\n",
    "        except Exception as e:\n",
    "            execution_stats['api_failures'] += 1\n",
    "            log(f\"⚠️  Failed to check clusters: {str(e)[:100]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_collaboration_with_retry():\n",
    "    \"\"\"Check collaboration features with retry logic\"\"\"\n",
    "    if not ENABLE_COLLABORATION or not USE_FULL_MODE:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        jobs = list(w.jobs.list())\n",
    "        results.append({\n",
    "            'Category': 'Collaboration',\n",
    "            'Feature': 'Jobs/Workflows',\n",
    "            'Description': 'Scheduled and triggered execution of notebooks',\n",
    "            'Value': str(len(jobs)),\n",
    "            'Enabled': len(jobs) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check jobs: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        repos = list(w.repos.list())\n",
    "        results.append({\n",
    "            'Category': 'Collaboration',\n",
    "            'Feature': 'Repos (Git)',\n",
    "            'Description': 'Git integration for version control',\n",
    "            'Value': str(len(repos)),\n",
    "            'Enabled': len(repos) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check repos: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        pipelines = list(w.pipelines.list_pipelines())\n",
    "        results.append({\n",
    "            'Category': 'Collaboration',\n",
    "            'Feature': 'Delta Live Tables',\n",
    "            'Description': 'Declarative ETL framework for data pipelines',\n",
    "            'Value': str(len(pipelines)),\n",
    "            'Enabled': len(pipelines) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check pipelines: {str(e)[:100]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_ml_ai_with_retry():\n",
    "    \"\"\"Check ML/AI features with retry logic\"\"\"\n",
    "    if not USE_FULL_MODE:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        endpoints = list(w.serving_endpoints.list())\n",
    "        results.append({\n",
    "            'Category': 'ML/AI',\n",
    "            'Feature': 'Model Serving Endpoints',\n",
    "            'Description': 'Real-time REST API endpoints for ML inference',\n",
    "            'Value': str(len(endpoints)),\n",
    "            'Enabled': len(endpoints) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check model serving: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        models = list(w.model_registry.list_models())\n",
    "        results.append({\n",
    "            'Category': 'ML/AI',\n",
    "            'Feature': 'Registered Models',\n",
    "            'Description': 'ML models in MLflow Model Registry',\n",
    "            'Value': str(len(models)),\n",
    "            'Enabled': len(models) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check registered models: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        experiments = list(w.experiments.list_experiments())\n",
    "        results.append({\n",
    "            'Category': 'ML/AI',\n",
    "            'Feature': 'MLflow Experiments',\n",
    "            'Description': 'Active experiments for tracking ML runs',\n",
    "            'Value': str(len(experiments)),\n",
    "            'Enabled': len(experiments) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check experiments: {str(e)[:100]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_security_with_retry():\n",
    "    \"\"\"Check security features with retry logic\"\"\"\n",
    "    if not ENABLE_SECURITY or not USE_FULL_MODE:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        users = list(w.users.list())\n",
    "        active_users = [u for u in users if u.active]\n",
    "        results.append({\n",
    "            'Category': 'Security',\n",
    "            'Feature': 'Users',\n",
    "            'Description': 'Workspace users (active/total)',\n",
    "            'Value': f\"{len(active_users)} active / {len(users)} total\",\n",
    "            'Enabled': len(users) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check users: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        groups = list(w.groups.list())\n",
    "        results.append({\n",
    "            'Category': 'Security',\n",
    "            'Feature': 'Groups',\n",
    "            'Description': 'User groups for managing permissions',\n",
    "            'Value': str(len(groups)),\n",
    "            'Enabled': len(groups) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check groups: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        service_principals = list(w.service_principals.list())\n",
    "        results.append({\n",
    "            'Category': 'Security',\n",
    "            'Feature': 'Service Principals',\n",
    "            'Description': 'Non-human identities for automated workflows',\n",
    "            'Value': str(len(service_principals)),\n",
    "            'Enabled': len(service_principals) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check service principals: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        secret_scopes = list(w.secrets.list_scopes())\n",
    "        results.append({\n",
    "            'Category': 'Security',\n",
    "            'Feature': 'Secret Scopes',\n",
    "            'Description': 'Secure storage for credentials and API keys',\n",
    "            'Value': str(len(secret_scopes)),\n",
    "            'Enabled': len(secret_scopes) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check secret scopes: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        ip_lists = list(w.ip_access_lists.list())\n",
    "        results.append({\n",
    "            'Category': 'Security',\n",
    "            'Feature': 'IP Access Lists',\n",
    "            'Description': 'Network-level access control by IP ranges',\n",
    "            'Value': str(len(ip_lists)),\n",
    "            'Enabled': len(ip_lists) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check IP access lists: {str(e)[:100]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_data_sharing_with_retry():\n",
    "    \"\"\"Check data sharing features with retry logic\"\"\"\n",
    "    if not ENABLE_DATA_SHARING or not USE_FULL_MODE:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        shares = list(w.shares.list())\n",
    "        results.append({\n",
    "            'Category': 'Data Sharing',\n",
    "            'Feature': 'Delta Sharing (Providers)',\n",
    "            'Description': 'Share live data with external organizations',\n",
    "            'Value': str(len(shares)),\n",
    "            'Enabled': len(shares) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check Delta Sharing providers: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        recipients = list(w.recipients.list())\n",
    "        results.append({\n",
    "            'Category': 'Data Sharing',\n",
    "            'Feature': 'Delta Sharing (Recipients)',\n",
    "            'Description': 'External parties receiving shared data',\n",
    "            'Value': str(len(recipients)),\n",
    "            'Enabled': len(recipients) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check Delta Sharing recipients: {str(e)[:100]}\")\n",
    "    \n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        providers = list(w.providers.list())\n",
    "        results.append({\n",
    "            'Category': 'Data Sharing',\n",
    "            'Feature': 'Data Providers',\n",
    "            'Description': 'External data sources sharing into workspace',\n",
    "            'Value': str(len(providers)),\n",
    "            'Enabled': len(providers) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check data providers: {str(e)[:100]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def check_workspace_capabilities_with_retry():\n",
    "    \"\"\"Check workspace-level capabilities and settings\"\"\"\n",
    "    if not USE_FULL_MODE:\n",
    "        return []\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Check notification destinations\n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        destinations = list(w.notification_destinations.list())\n",
    "        results.append({\n",
    "            'Category': 'Workspace Capabilities',\n",
    "            'Feature': 'Notification Destinations',\n",
    "            'Description': 'Alert destinations (Slack, email, webhooks, PagerDuty)',\n",
    "            'Value': str(len(destinations)),\n",
    "            'Enabled': len(destinations) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check notification destinations: {str(e)[:100]}\")\n",
    "    \n",
    "    # Check dashboards (Lakeview)\n",
    "    try:\n",
    "        execution_stats['api_calls'] += 1\n",
    "        dashboards = list(w.lakeview.list())\n",
    "        results.append({\n",
    "            'Category': 'Workspace Capabilities',\n",
    "            'Feature': 'Lakeview Dashboards',\n",
    "            'Description': 'Modern BI dashboards for data visualization',\n",
    "            'Value': str(len(dashboards)),\n",
    "            'Enabled': len(dashboards) > 0\n",
    "        })\n",
    "        execution_stats['features_found'] += 1\n",
    "    except Exception as e:\n",
    "        execution_stats['api_failures'] += 1\n",
    "        log(f\"⚠️  Failed to check Lakeview dashboards: {str(e)[:100]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Execute all checks in parallel\n",
    "all_features = []\n",
    "\n",
    "try:\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Submit workspace config checks\n",
    "        config_futures = {executor.submit(check_workspace_config_with_retry, key): key \n",
    "                         for key in feature_descriptions.keys()}\n",
    "        \n",
    "        # Submit other feature checks (only if FULL MODE)\n",
    "        other_futures = []\n",
    "        if USE_FULL_MODE:\n",
    "            other_futures = [\n",
    "                executor.submit(check_unity_catalog_with_retry),\n",
    "                executor.submit(check_compute_with_retry),\n",
    "                executor.submit(check_collaboration_with_retry),\n",
    "                executor.submit(check_ml_ai_with_retry),\n",
    "                executor.submit(check_security_with_retry),\n",
    "                executor.submit(check_data_sharing_with_retry),\n",
    "                executor.submit(check_workspace_capabilities_with_retry)\n",
    "            ]\n",
    "        \n",
    "        # Collect workspace config results with progress\n",
    "        completed = 0\n",
    "        total = len(config_futures)\n",
    "        for future in as_completed(config_futures, timeout=API_TIMEOUT):\n",
    "            try:\n",
    "                result = future.result(timeout=5)\n",
    "                if result:\n",
    "                    all_features.append(result)\n",
    "                completed += 1\n",
    "                if completed % 10 == 0 and not IS_JOB_MODE:\n",
    "                    log(f\"  Progress: {completed}/{total} config checks ({completed/total*100:.0f}%)\")\n",
    "            except TimeoutError:\n",
    "                execution_stats['api_failures'] += 1\n",
    "                log(f\"⚠️  Timeout checking config key\")\n",
    "            except Exception as e:\n",
    "                execution_stats['api_failures'] += 1\n",
    "        \n",
    "        # Collect other results with better timeout handling\n",
    "        if other_futures:\n",
    "            try:\n",
    "                for future in as_completed(other_futures, timeout=60):  # Increased timeout\n",
    "                    try:\n",
    "                        results = future.result(timeout=15)  # Increased individual timeout\n",
    "                        all_features.extend(results)\n",
    "                    except TimeoutError:\n",
    "                        execution_stats['api_failures'] += 1\n",
    "                        log(f\"⚠️  Timeout checking resource type\")\n",
    "                    except Exception as e:\n",
    "                        execution_stats['api_failures'] += 1\n",
    "                        log(f\"⚠️  Error checking resource type: {str(e)[:100]}\")\n",
    "            except TimeoutError:\n",
    "                log(f\"⚠️  Overall timeout reached, some checks may be incomplete\")\n",
    "except Exception as e:\n",
    "    log_error(f\"Unexpected error during feature check: {str(e)}\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - execution_stats['start_time']\n",
    "\n",
    "# Create DataFrame and display\n",
    "if all_features:\n",
    "    df = pd.DataFrame(all_features)\n",
    "    df = df.sort_values(['Category', 'Feature']).reset_index(drop=True)\n",
    "    df = df[['Category', 'Feature', 'Description', 'Value', 'Enabled']]\n",
    "    \n",
    "    log(f\"\\n✓ Completed in {execution_time:.2f} seconds\")\n",
    "    log(f\"Total features found: {len(df)}\")\n",
    "    log(f\"Enabled features: {df['Enabled'].sum()}\")\n",
    "    log(f\"API calls: {execution_stats['api_calls']}\")\n",
    "    log(f\"API failures: {execution_stats['api_failures']}\")\n",
    "    log(f\"API retries: {execution_stats['api_retries']}\")\n",
    "    if execution_stats['api_calls'] > 0:\n",
    "        success_rate = ((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls']) * 100\n",
    "        log(f\"Success rate: {success_rate:.1f}%\\n\")\n",
    "    \n",
    "    display(df)\n",
    "    \n",
    "    # Summary by category\n",
    "    log(\"\\n=== SUMMARY BY CATEGORY ===\")\n",
    "    summary = df.groupby('Category').agg({\n",
    "        'Feature': 'count',\n",
    "        'Enabled': 'sum'\n",
    "    }).rename(columns={'Feature': 'Total', 'Enabled': 'Enabled'})\n",
    "    display(summary)\n",
    "    \n",
    "    # Store for export\n",
    "    features_df = df\n",
    "else:\n",
    "    log_error(\"No features could be retrieved\")\n",
    "    log(f\"Execution time: {execution_time:.2f} seconds\")\n",
    "    features_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afdbb86d-a5bf-41a6-a1b9-ec82d0b61880",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export results (Excel and Delta)"
    }
   },
   "outputs": [],
   "source": [
    "if features_df is not None and (ENABLE_EXCEL_EXPORT or ENABLE_DELTA_EXPORT):\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"EXPORTING RESULTS\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Add audit metadata\n",
    "    features_df_export = features_df.copy()\n",
    "    features_df_export['audit_timestamp'] = datetime.now()\n",
    "    features_df_export['execution_time_seconds'] = execution_time\n",
    "    features_df_export['api_calls'] = execution_stats['api_calls']\n",
    "    features_df_export['api_failures'] = execution_stats['api_failures']\n",
    "    \n",
    "    # Excel export\n",
    "    if ENABLE_EXCEL_EXPORT:\n",
    "        try:\n",
    "            # Create export directory\n",
    "            dbutils.fs.mkdirs(EXPORT_PATH.replace('/dbfs', 'dbfs:'))\n",
    "            \n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            excel_file = f\"{EXPORT_PATH}/workspace_features_{timestamp}.xlsx\"\n",
    "            \n",
    "            # Convert to pandas and export\n",
    "            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:\n",
    "                features_df_export.to_excel(writer, sheet_name='Features', index=False)\n",
    "                \n",
    "                # Add summary sheet\n",
    "                summary_data = {\n",
    "                    'Metric': [\n",
    "                        'Total Features',\n",
    "                        'Enabled Features',\n",
    "                        'Execution Time (seconds)',\n",
    "                        'API Calls',\n",
    "                        'API Failures',\n",
    "                        'Success Rate (%)'\n",
    "                    ],\n",
    "                    'Value': [\n",
    "                        len(features_df),\n",
    "                        features_df['Enabled'].sum(),\n",
    "                        f\"{execution_time:.2f}\",\n",
    "                        execution_stats['api_calls'],\n",
    "                        execution_stats['api_failures'],\n",
    "                        f\"{((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls'] * 100):.1f}\" if execution_stats['api_calls'] > 0 else 'N/A'\n",
    "                    ]\n",
    "                }\n",
    "                pd.DataFrame(summary_data).to_excel(writer, sheet_name='Summary', index=False)\n",
    "            \n",
    "            log(f\"✓ Excel export complete: {excel_file}\")\n",
    "        except Exception as e:\n",
    "            log_error(f\"Excel export failed: {str(e)}\")\n",
    "    \n",
    "    # Delta export for historical tracking\n",
    "    if ENABLE_DELTA_EXPORT:\n",
    "        try:\n",
    "            spark_df = spark.createDataFrame(features_df_export)\n",
    "            \n",
    "            # Check if table exists\n",
    "            try:\n",
    "                spark.sql(f\"DESCRIBE TABLE {DELTA_TABLE_NAME}\")\n",
    "                table_exists = True\n",
    "            except:\n",
    "                table_exists = False\n",
    "            \n",
    "            if table_exists:\n",
    "                spark_df.write.format(\"delta\").mode(\"append\").saveAsTable(DELTA_TABLE_NAME)\n",
    "                log(f\"✓ Delta export complete: Appended to {DELTA_TABLE_NAME}\")\n",
    "            else:\n",
    "                spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(DELTA_TABLE_NAME)\n",
    "                log(f\"✓ Delta export complete: Created {DELTA_TABLE_NAME}\")\n",
    "        except Exception as e:\n",
    "            log_error(f\"Delta export failed: {str(e)}\")\n",
    "    \n",
    "    log(\"=\"*60)\n",
    "else:\n",
    "    if features_df is None:\n",
    "        log(\"⚠️  Skipping export: No data to export\")\n",
    "    else:\n",
    "        log(\"ℹ️  Export disabled in configuration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfbbc225-48ae-4121-a9ee-108f85579a5d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature distribution visualization"
    }
   },
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"FEATURE DISTRIBUTION ANALYSIS\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Create visualizations\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Chart 1: Features by Category\n",
    "    category_counts = features_df.groupby('Category').size().sort_values(ascending=False)\n",
    "    axes[0].barh(category_counts.index, category_counts.values, color='steelblue')\n",
    "    axes[0].set_xlabel('Number of Features')\n",
    "    axes[0].set_title('Features by Category')\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Chart 2: Enabled vs Disabled by Category\n",
    "    enabled_by_category = features_df.groupby(['Category', 'Enabled']).size().unstack(fill_value=0)\n",
    "    enabled_by_category.plot(kind='barh', stacked=True, ax=axes[1], \n",
    "                             color=['lightcoral', 'lightgreen'],\n",
    "                             legend=True)\n",
    "    axes[1].set_xlabel('Number of Features')\n",
    "    axes[1].set_title('Enabled vs Disabled Features by Category')\n",
    "    axes[1].legend(['Disabled', 'Enabled'], loc='lower right')\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCCA Visualization complete\")\n",
    "else:\n",
    "    log(\"⚠️  No data available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5b4567a-7f97-49de-acb8-b1525cad2d6e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature change detection (compare with previous run)"
    }
   },
   "outputs": [],
   "source": [
    "if ENABLE_DELTA_EXPORT and features_df is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"FEATURE CHANGE DETECTION\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Check if history table exists\n",
    "        previous_run = spark.sql(f\"\"\"\n",
    "            SELECT * FROM {DELTA_TABLE_NAME}\n",
    "            WHERE audit_timestamp < (SELECT MAX(audit_timestamp) FROM {DELTA_TABLE_NAME})\n",
    "            ORDER BY audit_timestamp DESC\n",
    "            LIMIT 1\n",
    "        \"\"\")\n",
    "        \n",
    "        if previous_run.count() > 0:\n",
    "            log(\"\\nComparing with previous audit run...\")\n",
    "            \n",
    "            # Get previous run data\n",
    "            prev_df = previous_run.toPandas()\n",
    "            prev_timestamp = prev_df['audit_timestamp'].iloc[0]\n",
    "            \n",
    "            log(f\"  Previous run: {prev_timestamp}\")\n",
    "            log(f\"  Current run: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            \n",
    "            # Compare current vs previous\n",
    "            current_features = set(features_df[features_df['Enabled'] == True]['Feature'].tolist())\n",
    "            previous_features = set(prev_df[prev_df['Enabled'] == True]['Feature'].tolist())\n",
    "            \n",
    "            # Identify changes\n",
    "            newly_enabled = current_features - previous_features\n",
    "            newly_disabled = previous_features - current_features\n",
    "            \n",
    "            log(f\"\\n\uD83D\uDCCA Change Summary:\")\n",
    "            log(f\"  Features newly enabled: {len(newly_enabled)}\")\n",
    "            log(f\"  Features newly disabled: {len(newly_disabled)}\")\n",
    "            log(f\"  Features unchanged: {len(current_features & previous_features)}\")\n",
    "            \n",
    "            if newly_enabled:\n",
    "                log(f\"\\n✅ Newly Enabled Features:\")\n",
    "                for feature in sorted(newly_enabled):\n",
    "                    feature_info = features_df[features_df['Feature'] == feature].iloc[0]\n",
    "                    log(f\"  + {feature} ({feature_info['Category']})\")\n",
    "                    log(f\"    {feature_info['Description']}\")\n",
    "            \n",
    "            if newly_disabled:\n",
    "                log(f\"\\n❌ Newly Disabled Features:\")\n",
    "                for feature in sorted(newly_disabled):\n",
    "                    log(f\"  - {feature}\")\n",
    "            \n",
    "            if not newly_enabled and not newly_disabled:\n",
    "                log(\"\\n✓ No feature changes detected since last run\")\n",
    "            \n",
    "            # Store change summary\n",
    "            change_summary_df = pd.DataFrame([{\n",
    "                'current_timestamp': datetime.now(),\n",
    "                'previous_timestamp': prev_timestamp,\n",
    "                'newly_enabled_count': len(newly_enabled),\n",
    "                'newly_disabled_count': len(newly_disabled),\n",
    "                'newly_enabled_features': ', '.join(sorted(newly_enabled)) if newly_enabled else 'None',\n",
    "                'newly_disabled_features': ', '.join(sorted(newly_disabled)) if newly_disabled else 'None'\n",
    "            }])\n",
    "            \n",
    "        else:\n",
    "            log(\"\\nℹ️  No previous run found for comparison\")\n",
    "            log(\"   This appears to be the first audit run\")\n",
    "            change_summary_df = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        if 'TABLE_OR_VIEW_NOT_FOUND' in str(e):\n",
    "            log(\"\\nℹ️  Delta table not found - this is the first run\")\n",
    "            log(f\"   Enable ENABLE_DELTA_EXPORT=True and run Cell 4 to start tracking changes\")\n",
    "        else:\n",
    "            log(f\"\\n⚠️  Change detection failed: {str(e)[:100]}\")\n",
    "        change_summary_df = None\n",
    "else:\n",
    "    if not ENABLE_DELTA_EXPORT:\n",
    "        log(\"\\nℹ️  Change detection skipped (ENABLE_DELTA_EXPORT=False)\")\n",
    "        log(\"   Set ENABLE_DELTA_EXPORT=True to track feature changes over time\")\n",
    "    change_summary_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bccab59-7487-4031-9c5e-e5830ba25cfa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Feature recommendations"
    }
   },
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"FEATURE RECOMMENDATIONS\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Check for important disabled features\n",
    "    disabled_features = features_df[features_df['Enabled'] == False]\n",
    "    \n",
    "    # Recommend enabling IP Access Lists (security)\n",
    "    ip_access = disabled_features[disabled_features['Feature'] == 'IP Access Lists']\n",
    "    if not ip_access.empty:\n",
    "        recommendations.append({\n",
    "            'Priority': 'High',\n",
    "            'Category': 'Security',\n",
    "            'Feature': 'IP Access Lists',\n",
    "            'Recommendation': 'Enable IP access lists to restrict workspace access by network',\n",
    "            'Benefit': 'Enhanced network security and compliance',\n",
    "            'Impact': 'Limits workspace access to approved IP ranges'\n",
    "        })\n",
    "    \n",
    "    # Recommend enabling verbose audit logs (compliance)\n",
    "    audit_logs = features_df[\n",
    "        (features_df['Feature'] == 'enableVerboseAuditLogs') & \n",
    "        (features_df['Enabled'] == False)\n",
    "    ]\n",
    "    if not audit_logs.empty:\n",
    "        recommendations.append({\n",
    "            'Priority': 'High',\n",
    "            'Category': 'Compliance',\n",
    "            'Feature': 'Verbose Audit Logs',\n",
    "            'Recommendation': 'Enable verbose audit logs for detailed compliance tracking',\n",
    "            'Benefit': 'Complete audit trail for security and compliance reviews',\n",
    "            'Impact': 'Captures detailed user activity and API calls'\n",
    "        })\n",
    "    \n",
    "    # Recommend enabling user isolation (security)\n",
    "    user_isolation = features_df[\n",
    "        (features_df['Feature'] == 'enforceUserIsolation') & \n",
    "        (features_df['Enabled'] == False)\n",
    "    ]\n",
    "    if not user_isolation.empty:\n",
    "        recommendations.append({\n",
    "            'Priority': 'Medium',\n",
    "            'Category': 'Security',\n",
    "            'Feature': 'User Isolation',\n",
    "            'Recommendation': 'Enable user isolation for enhanced security on shared clusters',\n",
    "            'Benefit': 'Prevents users from accessing each other\\'s data',\n",
    "            'Impact': 'Enforces data isolation on shared compute resources'\n",
    "        })\n",
    "    \n",
    "    # Recommend Unity Catalog if not available\n",
    "    system_catalog = features_df[\n",
    "        (features_df['Feature'] == 'System Catalog') & \n",
    "        (features_df['Enabled'] == False)\n",
    "    ]\n",
    "    if not system_catalog.empty:\n",
    "        recommendations.append({\n",
    "            'Priority': 'High',\n",
    "            'Category': 'Data Governance',\n",
    "            'Feature': 'Unity Catalog',\n",
    "            'Recommendation': 'Enable Unity Catalog for centralized data governance',\n",
    "            'Benefit': 'Fine-grained access control, data lineage, and audit logs',\n",
    "            'Impact': 'Modern data governance with system tables for monitoring'\n",
    "        })\n",
    "    \n",
    "    # Recommend Delta Live Tables if not used\n",
    "    dlt = features_df[\n",
    "        (features_df['Feature'] == 'Delta Live Tables') & \n",
    "        (features_df['Value'] == '0')\n",
    "    ]\n",
    "    if not dlt.empty:\n",
    "        recommendations.append({\n",
    "            'Priority': 'Low',\n",
    "            'Category': 'Data Engineering',\n",
    "            'Feature': 'Delta Live Tables',\n",
    "            'Recommendation': 'Consider Delta Live Tables for declarative ETL pipelines',\n",
    "            'Benefit': 'Simplified data pipeline development and maintenance',\n",
    "            'Impact': 'Reduces ETL complexity with declarative SQL/Python'\n",
    "        })\n",
    "    \n",
    "    # Recommend Model Serving if ML models exist but no serving\n",
    "    models = features_df[features_df['Feature'] == 'Registered Models']\n",
    "    serving = features_df[features_df['Feature'] == 'Model Serving Endpoints']\n",
    "    \n",
    "    if not models.empty and not serving.empty:\n",
    "        model_count = int(models.iloc[0]['Value']) if models.iloc[0]['Value'].isdigit() else 0\n",
    "        serving_count = int(serving.iloc[0]['Value']) if serving.iloc[0]['Value'].isdigit() else 0\n",
    "        \n",
    "        if model_count > 0 and serving_count == 0:\n",
    "            recommendations.append({\n",
    "                'Priority': 'Medium',\n",
    "                'Category': 'ML/AI',\n",
    "                'Feature': 'Model Serving',\n",
    "                'Recommendation': f'You have {model_count} registered models but no serving endpoints',\n",
    "                'Benefit': 'Deploy models as real-time REST APIs for production inference',\n",
    "                'Impact': 'Enables real-time ML predictions with automatic scaling'\n",
    "            })\n",
    "    \n",
    "    # Recommend notification destinations if none configured\n",
    "    notifications = features_df[\n",
    "        (features_df['Feature'] == 'Notification Destinations') & \n",
    "        (features_df['Value'] == '0')\n",
    "    ]\n",
    "    if not notifications.empty:\n",
    "        recommendations.append({\n",
    "            'Priority': 'Low',\n",
    "            'Category': 'Operations',\n",
    "            'Feature': 'Notification Destinations',\n",
    "            'Recommendation': 'Configure notification destinations for job and pipeline alerts',\n",
    "            'Benefit': 'Proactive monitoring with Slack, email, or PagerDuty integration',\n",
    "            'Impact': 'Reduces downtime with immediate failure notifications'\n",
    "        })\n",
    "    \n",
    "    # Display recommendations\n",
    "    if recommendations:\n",
    "        rec_df = pd.DataFrame(recommendations)\n",
    "        rec_df = rec_df[['Priority', 'Category', 'Feature', 'Recommendation', 'Benefit', 'Impact']]\n",
    "        \n",
    "        # Sort by priority\n",
    "        priority_order = {'High': 0, 'Medium': 1, 'Low': 2}\n",
    "        rec_df['_sort'] = rec_df['Priority'].map(priority_order)\n",
    "        rec_df = rec_df.sort_values('_sort').drop('_sort', axis=1).reset_index(drop=True)\n",
    "        \n",
    "        log(f\"\\n\uD83D\uDCA1 Found {len(rec_df)} recommendations:\\n\")\n",
    "        display(rec_df)\n",
    "        \n",
    "        # Summary by priority\n",
    "        priority_summary = rec_df['Priority'].value_counts().sort_index()\n",
    "        log(f\"\\n\uD83D\uDCCB Recommendations by priority:\")\n",
    "        for priority, count in priority_summary.items():\n",
    "            log(f\"  {priority}: {count}\")\n",
    "    else:\n",
    "        log(\"\\n✓ No feature recommendations\")\n",
    "        log(\"   Your workspace is using recommended features\")\n",
    "    \n",
    "    log(\"=\"*60)\n",
    "else:\n",
    "    log(\"⚠️  No data available for recommendations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "691efdcb-2795-4c9e-8b51-56b1e4a6bb7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Workspace health score calculation"
    }
   },
   "outputs": [],
   "source": [
    "if features_df is not None:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"WORKSPACE HEALTH SCORE\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Calculate health score (0-100, higher is better)\n",
    "    health_score = 0\n",
    "    max_score = 100\n",
    "    \n",
    "    # Category weights\n",
    "    weights = {\n",
    "        'security': 30,      # Security features (30 points)\n",
    "        'governance': 25,    # Unity Catalog and data governance (25 points)\n",
    "        'collaboration': 20, # Repos, jobs, pipelines (20 points)\n",
    "        'ml_ai': 15,        # ML/AI capabilities (15 points)\n",
    "        'operations': 10     # Monitoring, notifications (10 points)\n",
    "    }\n",
    "    \n",
    "    # Security score (30 points max)\n",
    "    security_features = features_df[features_df['Category'] == 'Security']\n",
    "    if len(security_features) > 0:\n",
    "        security_enabled = security_features['Enabled'].sum()\n",
    "        security_total = len(security_features)\n",
    "        security_score = (security_enabled / security_total) * weights['security']\n",
    "        health_score += security_score\n",
    "        log(f\"\\n\uD83D\uDD12 Security: {security_score:.1f}/{weights['security']} points\")\n",
    "        log(f\"   {security_enabled}/{security_total} security features enabled\")\n",
    "    \n",
    "    # Check critical security flags\n",
    "    critical_security = [\n",
    "        ('enableVerboseAuditLogs', 'Verbose audit logs'),\n",
    "        ('enableIpAccessLists', 'IP access lists'),\n",
    "        ('enforceUserIsolation', 'User isolation')\n",
    "    ]\n",
    "    \n",
    "    security_issues = []\n",
    "    for feature_key, feature_name in critical_security:\n",
    "        feature_row = features_df[features_df['Feature'] == feature_key]\n",
    "        if not feature_row.empty and not feature_row.iloc[0]['Enabled']:\n",
    "            security_issues.append(feature_name)\n",
    "    \n",
    "    if security_issues:\n",
    "        log(f\"   ⚠️  Missing: {', '.join(security_issues)}\")\n",
    "    \n",
    "    # Governance score (25 points max)\n",
    "    uc_features = features_df[features_df['Category'] == 'Unity Catalog']\n",
    "    if len(uc_features) > 0:\n",
    "        uc_enabled = uc_features['Enabled'].sum()\n",
    "        uc_total = len(uc_features)\n",
    "        governance_score = (uc_enabled / uc_total) * weights['governance']\n",
    "        health_score += governance_score\n",
    "        log(f\"\\n\uD83D\uDCDA Data Governance: {governance_score:.1f}/{weights['governance']} points\")\n",
    "        log(f\"   {uc_enabled}/{uc_total} Unity Catalog features enabled\")\n",
    "    \n",
    "    # Collaboration score (20 points max)\n",
    "    collab_features = features_df[features_df['Category'] == 'Collaboration']\n",
    "    if len(collab_features) > 0:\n",
    "        collab_enabled = collab_features['Enabled'].sum()\n",
    "        collab_total = len(collab_features)\n",
    "        collaboration_score = (collab_enabled / collab_total) * weights['collaboration']\n",
    "        health_score += collaboration_score\n",
    "        log(f\"\\n\uD83E\uDD1D Collaboration: {collaboration_score:.1f}/{weights['collaboration']} points\")\n",
    "        log(f\"   {collab_enabled}/{collab_total} collaboration features enabled\")\n",
    "    \n",
    "    # ML/AI score (15 points max)\n",
    "    ml_features = features_df[features_df['Category'] == 'ML/AI']\n",
    "    if len(ml_features) > 0:\n",
    "        ml_enabled = ml_features['Enabled'].sum()\n",
    "        ml_total = len(ml_features)\n",
    "        ml_score = (ml_enabled / ml_total) * weights['ml_ai']\n",
    "        health_score += ml_score\n",
    "        log(f\"\\n\uD83E\uDD16 ML/AI: {ml_score:.1f}/{weights['ml_ai']} points\")\n",
    "        log(f\"   {ml_enabled}/{ml_total} ML/AI features enabled\")\n",
    "    \n",
    "    # Operations score (10 points max)\n",
    "    ops_features = features_df[features_df['Category'] == 'Workspace Capabilities']\n",
    "    if len(ops_features) > 0:\n",
    "        ops_enabled = ops_features['Enabled'].sum()\n",
    "        ops_total = len(ops_features)\n",
    "        operations_score = (ops_enabled / ops_total) * weights['operations']\n",
    "        health_score += operations_score\n",
    "        log(f\"\\n⚙️  Operations: {operations_score:.1f}/{weights['operations']} points\")\n",
    "        log(f\"   {ops_enabled}/{ops_total} operational features enabled\")\n",
    "    \n",
    "    # Determine health level\n",
    "    if health_score >= 90:\n",
    "        health_level = 'EXCELLENT'\n",
    "        health_emoji = '\uD83D\uDFE2'\n",
    "    elif health_score >= 75:\n",
    "        health_level = 'GOOD'\n",
    "        health_emoji = '\uD83D\uDFE2'\n",
    "    elif health_score >= 60:\n",
    "        health_level = 'FAIR'\n",
    "        health_emoji = '\uD83D\uDFE1'\n",
    "    elif health_score >= 40:\n",
    "        health_level = 'NEEDS IMPROVEMENT'\n",
    "        health_emoji = '\uD83D\uDFE0'\n",
    "    else:\n",
    "        health_level = 'POOR'\n",
    "        health_emoji = '\uD83D\uDD34'\n",
    "    \n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(f\"{health_emoji} OVERALL WORKSPACE HEALTH SCORE: {health_score:.1f}/100 ({health_level})\")\n",
    "    log(f\"{'='*60}\")\n",
    "    \n",
    "    # Store for export\n",
    "    workspace_health_score = health_score\n",
    "    workspace_health_level = health_level\n",
    "else:\n",
    "    log(\"⚠️  Cannot calculate health score - no feature data available\")\n",
    "    workspace_health_score = None\n",
    "    workspace_health_level = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ec65bea-e699-4d22-ba0f-9983cb8e8719",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enhanced Excel export with summary dashboard"
    }
   },
   "outputs": [],
   "source": [
    "if features_df is not None and ENABLE_EXCEL_EXPORT:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"ENHANCED EXCEL EXPORT WITH SUMMARY DASHBOARD\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        import os\n",
    "        import tempfile\n",
    "        \n",
    "        # Use temp directory for serverless compatibility\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        temp_excel_file = f\"{temp_dir}/workspace_features_{timestamp}.xlsx\"\n",
    "        \n",
    "        log(f\"\\nCreating Excel workbook with 8 sheets...\")\n",
    "        \n",
    "        # Prepare all sheets\n",
    "        with pd.ExcelWriter(temp_excel_file, engine='openpyxl') as writer:\n",
    "            \n",
    "            # Sheet 1: Executive Summary Dashboard\n",
    "            exec_summary_data = []\n",
    "            \n",
    "            # Overall metrics\n",
    "            exec_summary_data.append({'Metric': 'Total Features Checked', 'Value': len(features_df)})\n",
    "            exec_summary_data.append({'Metric': 'Features Enabled', 'Value': int(features_df['Enabled'].sum())})\n",
    "            exec_summary_data.append({'Metric': 'Features Disabled', 'Value': len(features_df) - int(features_df['Enabled'].sum())})\n",
    "            exec_summary_data.append({'Metric': 'Enablement Rate (%)', 'Value': f\"{(features_df['Enabled'].sum() / len(features_df) * 100):.1f}%\"})\n",
    "            \n",
    "            # Health score\n",
    "            if 'workspace_health_score' in locals() and workspace_health_score is not None:\n",
    "                exec_summary_data.append({'Metric': 'Workspace Health Score', 'Value': f\"{workspace_health_score:.1f}/100 ({workspace_health_level})\"})\n",
    "            \n",
    "            # Execution metrics\n",
    "            exec_summary_data.append({'Metric': 'Execution Time (seconds)', 'Value': f\"{execution_time:.2f}\"})\n",
    "            exec_summary_data.append({'Metric': 'API Calls', 'Value': execution_stats['api_calls']})\n",
    "            exec_summary_data.append({'Metric': 'API Failures', 'Value': execution_stats['api_failures']})\n",
    "            exec_summary_data.append({'Metric': 'Success Rate (%)', 'Value': f\"{((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls'] * 100):.1f}\" if execution_stats['api_calls'] > 0 else 'N/A'})\n",
    "            \n",
    "            # Environment\n",
    "            exec_summary_data.append({'Metric': 'Compute Type', 'Value': 'Serverless' if IS_SERVERLESS else 'Traditional Cluster'})\n",
    "            exec_summary_data.append({'Metric': 'Execution Mode', 'Value': 'Full Mode' if USE_FULL_MODE else 'Quick Mode'})\n",
    "            exec_summary_data.append({'Metric': 'Audit Timestamp', 'Value': datetime.now().strftime('%Y-%m-%d %H:%M:%S')})\n",
    "            \n",
    "            pd.DataFrame(exec_summary_data).to_excel(writer, sheet_name='Executive Summary', index=False)\n",
    "            log(\"  ✓ Sheet 1: Executive Summary\")\n",
    "            \n",
    "            # Sheet 2: Features by Category\n",
    "            category_summary = features_df.groupby('Category').agg({\n",
    "                'Feature': 'count',\n",
    "                'Enabled': 'sum'\n",
    "            }).rename(columns={'Feature': 'Total', 'Enabled': 'Enabled'})\n",
    "            category_summary['Disabled'] = category_summary['Total'] - category_summary['Enabled']\n",
    "            category_summary['Enablement Rate (%)'] = (category_summary['Enabled'] / category_summary['Total'] * 100).round(1)\n",
    "            category_summary.to_excel(writer, sheet_name='Features by Category')\n",
    "            log(\"  ✓ Sheet 2: Features by Category\")\n",
    "            \n",
    "            # Sheet 3: All Features (detailed)\n",
    "            features_df.to_excel(writer, sheet_name='All Features', index=False)\n",
    "            log(\"  ✓ Sheet 3: All Features\")\n",
    "            \n",
    "            # Sheet 4: Enabled Features Only\n",
    "            enabled_features = features_df[features_df['Enabled'] == True]\n",
    "            enabled_features.to_excel(writer, sheet_name='Enabled Features', index=False)\n",
    "            log(f\"  ✓ Sheet 4: Enabled Features ({len(enabled_features)} features)\")\n",
    "            \n",
    "            # Sheet 5: Disabled Features Only\n",
    "            disabled_features_export = features_df[features_df['Enabled'] == False]\n",
    "            disabled_features_export.to_excel(writer, sheet_name='Disabled Features', index=False)\n",
    "            log(f\"  ✓ Sheet 5: Disabled Features ({len(disabled_features_export)} features)\")\n",
    "            \n",
    "            # Sheet 6: Recommendations (if available)\n",
    "            if 'rec_df' in locals() and rec_df is not None and len(rec_df) > 0:\n",
    "                rec_df.to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "                log(f\"  ✓ Sheet 6: Recommendations ({len(rec_df)} items)\")\n",
    "            else:\n",
    "                pd.DataFrame([{'Note': 'No recommendations available'}]).to_excel(writer, sheet_name='Recommendations', index=False)\n",
    "                log(\"  ✓ Sheet 6: Recommendations (none)\")\n",
    "            \n",
    "            # Sheet 7: Change Summary (if available)\n",
    "            if 'change_summary_df' in locals() and change_summary_df is not None:\n",
    "                change_summary_df.to_excel(writer, sheet_name='Change Summary', index=False)\n",
    "                log(\"  ✓ Sheet 7: Change Summary\")\n",
    "            else:\n",
    "                pd.DataFrame([{'Note': 'Enable ENABLE_DELTA_EXPORT to track changes'}]).to_excel(writer, sheet_name='Change Summary', index=False)\n",
    "                log(\"  ✓ Sheet 7: Change Summary (not available)\")\n",
    "            \n",
    "            # Sheet 8: Execution Metadata\n",
    "            metadata = pd.DataFrame([{\n",
    "                'Execution Date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'Execution Mode': 'Job' if IS_JOB_MODE else 'Interactive',\n",
    "                'Compute Type': 'Serverless' if IS_SERVERLESS else 'Traditional',\n",
    "                'Full Mode': USE_FULL_MODE,\n",
    "                'Total Execution Time (s)': f\"{execution_time:.2f}\",\n",
    "                'API Calls': execution_stats['api_calls'],\n",
    "                'API Failures': execution_stats['api_failures'],\n",
    "                'API Retries': execution_stats['api_retries'],\n",
    "                'Success Rate (%)': f\"{((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls'] * 100):.1f}\" if execution_stats['api_calls'] > 0 else 'N/A',\n",
    "                'Features Found': len(features_df),\n",
    "                'Features Enabled': int(features_df['Enabled'].sum())\n",
    "            }])\n",
    "            metadata.to_excel(writer, sheet_name='Execution Metadata', index=False)\n",
    "            log(\"  ✓ Sheet 8: Execution Metadata\")\n",
    "        \n",
    "        # Copy to DBFS if needed\n",
    "        try:\n",
    "            dbutils.fs.mkdirs(EXPORT_PATH.replace('/dbfs', 'dbfs:'))\n",
    "            final_path = f\"{EXPORT_PATH}/workspace_features_{timestamp}.xlsx\"\n",
    "            dbutils.fs.cp(f\"file:{temp_excel_file}\", final_path.replace('/dbfs', 'dbfs:'))\n",
    "            excel_file = final_path\n",
    "            log(f\"  ✓ Copied to DBFS: {final_path}\")\n",
    "        except:\n",
    "            excel_file = temp_excel_file\n",
    "            log(f\"  ℹ️  Saved to local temp: {temp_excel_file}\")\n",
    "        \n",
    "        file_size_kb = os.path.getsize(temp_excel_file) / 1024\n",
    "        log(f\"\\n✓ Enhanced Excel export complete!\")\n",
    "        log(f\"  File: {excel_file}\")\n",
    "        log(f\"  Sheets: 8\")\n",
    "        log(f\"  Size: {file_size_kb:.1f} KB\")\n",
    "        log(f\"\\n\uD83D\uDCCA Open the file to view the executive dashboard and detailed feature analysis\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f\"Enhanced Excel export failed: {str(e)}\")\n",
    "        import traceback\n",
    "        if not IS_JOB_MODE:\n",
    "            log(f\"\\nError details: {traceback.format_exc()[:500]}\")\n",
    "elif features_df is None:\n",
    "    log(\"⚠️  Skipping enhanced export: No data to export\")\n",
    "else:\n",
    "    log(\"ℹ️  Enhanced Excel export disabled (ENABLE_EXCEL_EXPORT=False)\")\n",
    "    log(\"   Set ENABLE_EXCEL_EXPORT=True in Cell 2 to enable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d11158e7-8a1c-4bea-966f-7fa0b254e0c0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "JSON export option"
    }
   },
   "outputs": [],
   "source": [
    "# JSON export for API integration and programmatic access\n",
    "ENABLE_JSON_EXPORT = False  # Set to True to enable JSON export\n",
    "\n",
    "if features_df is not None and ENABLE_JSON_EXPORT:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"JSON EXPORT\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        import json\n",
    "        \n",
    "        # Create export directory\n",
    "        dbutils.fs.mkdirs(EXPORT_PATH.replace('/dbfs', 'dbfs:'))\n",
    "        \n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        json_file = f\"{EXPORT_PATH}/workspace_features_{timestamp}.json\"\n",
    "        \n",
    "        # Prepare JSON structure\n",
    "        export_data = {\n",
    "            'metadata': {\n",
    "                'audit_timestamp': datetime.now().isoformat(),\n",
    "                'execution_mode': 'Job' if IS_JOB_MODE else 'Interactive',\n",
    "                'compute_type': 'Serverless' if IS_SERVERLESS else 'Traditional',\n",
    "                'full_mode': USE_FULL_MODE,\n",
    "                'execution_time_seconds': execution_time,\n",
    "                'api_calls': execution_stats['api_calls'],\n",
    "                'api_failures': execution_stats['api_failures'],\n",
    "                'success_rate': f\"{((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls'] * 100):.1f}%\" if execution_stats['api_calls'] > 0 else 'N/A'\n",
    "            },\n",
    "            'summary': {\n",
    "                'total_features': len(features_df),\n",
    "                'enabled_features': int(features_df['Enabled'].sum()),\n",
    "                'disabled_features': len(features_df) - int(features_df['Enabled'].sum()),\n",
    "                'enablement_rate': f\"{(features_df['Enabled'].sum() / len(features_df) * 100):.1f}%\"\n",
    "            },\n",
    "            'health_score': {\n",
    "                'score': workspace_health_score if 'workspace_health_score' in locals() else None,\n",
    "                'level': workspace_health_level if 'workspace_health_level' in locals() else None\n",
    "            },\n",
    "            'features_by_category': {},\n",
    "            'all_features': features_df.to_dict('records')\n",
    "        }\n",
    "        \n",
    "        # Add category breakdown\n",
    "        for category in features_df['Category'].unique():\n",
    "            cat_features = features_df[features_df['Category'] == category]\n",
    "            export_data['features_by_category'][category] = {\n",
    "                'total': len(cat_features),\n",
    "                'enabled': int(cat_features['Enabled'].sum()),\n",
    "                'features': cat_features.to_dict('records')\n",
    "            }\n",
    "        \n",
    "        # Write JSON file\n",
    "        with open(json_file, 'w') as f:\n",
    "            json.dump(export_data, f, indent=2, default=str)\n",
    "        \n",
    "        log(f\"\\n✓ JSON export complete: {json_file}\")\n",
    "        log(f\"  File size: {os.path.getsize(json_file) / 1024:.1f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log_error(f\"JSON export failed: {str(e)}\")\n",
    "elif features_df is None:\n",
    "    log(\"⚠️  Skipping JSON export: No data to export\")\n",
    "else:\n",
    "    log(\"ℹ️  JSON export disabled (ENABLE_JSON_EXPORT=False)\")\n",
    "    log(\"   Set ENABLE_JSON_EXPORT=True in this cell to enable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "593f4cb5-298b-47e5-8f0b-2131865335d2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optional: Add job parameter widgets"
    }
   },
   "outputs": [],
   "source": [
    "# Uncomment this cell to add job parameter widgets for runtime configuration\n",
    "# This allows you to override settings when running as a scheduled job\n",
    "\n",
    "# dbutils.widgets.dropdown(\"use_full_mode\", \"true\", [\"true\", \"false\"], \"Use Full Mode\")\n",
    "# dbutils.widgets.dropdown(\"enable_excel_export\", \"false\", [\"true\", \"false\"], \"Enable Excel Export\")\n",
    "# dbutils.widgets.dropdown(\"enable_delta_export\", \"false\", [\"true\", \"false\"], \"Enable Delta Export\")\n",
    "# dbutils.widgets.text(\"max_workers\", \"10\", \"Max Workers\")\n",
    "# dbutils.widgets.dropdown(\"enable_cluster_check\", \"false\", [\"true\", \"false\"], \"Enable Cluster Check (Slow)\")\n",
    "\n",
    "# To use these parameters, add this code to Cell 2 (Configuration) after the initial settings:\n",
    "# try:\n",
    "#     USE_FULL_MODE = dbutils.widgets.get(\"use_full_mode\").lower() == \"true\"\n",
    "#     ENABLE_EXCEL_EXPORT = dbutils.widgets.get(\"enable_excel_export\").lower() == \"true\"\n",
    "#     ENABLE_DELTA_EXPORT = dbutils.widgets.get(\"enable_delta_export\").lower() == \"true\"\n",
    "#     MAX_WORKERS = int(dbutils.widgets.get(\"max_workers\"))\n",
    "#     ENABLE_CLUSTER_CHECK = dbutils.widgets.get(\"enable_cluster_check\").lower() == \"true\"\n",
    "#     log(\"✓ Using job parameter overrides\")\n",
    "# except:\n",
    "#     log(\"ℹ️  Using default configuration (no job parameters)\")\n",
    "\n",
    "log(\"ℹ️  Job parameter widgets are commented out\")\n",
    "log(\"   Uncomment the code in this cell to enable runtime configuration\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Workspace Features Audit",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
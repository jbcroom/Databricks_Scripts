{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa678ae7-c4bb-45af-94e7-af74e731d47b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Documentation Header"
    }
   },
   "source": [
    "# Databricks Security Review Export\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a **comprehensive security audit** of your Databricks workspace by extracting and analyzing all identity, access, and permission configurations. The output includes detailed reports in multiple formats (Excel, CSV, JSON, Delta tables) containing all security-related information for compliance reviews, access audits, and security assessments.\n",
    "\n",
    "**✨ Enterprise-grade security auditing with complete coverage, change tracking, compliance reporting, and performance optimization.**\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Modes\n",
    "\n",
    "### \uD83D\uDE80 Quick Mode (5-10 minutes)\n",
    "**Recommended for**: Daily monitoring, quick audits, testing, **interactive development**\n",
    "\n",
    "* Core permissions: Jobs, Warehouses, Pipelines\n",
    "* Unity Catalog: Catalogs and schemas\n",
    "* Security essentials: Secrets, IP access lists\n",
    "* **Skips**: Clusters, Workspace objects, Models, Repos, Pools, SQL assets, Volumes, Tokens\n",
    "* **Limits**: MAX_RESOURCES_PER_TYPE = 100\n",
    "\n",
    "### \uD83D\uDD0D Full Mode (20-60 minutes)\n",
    "**Recommended for**: Compliance audits, quarterly reviews, comprehensive analysis, **scheduled jobs**\n",
    "\n",
    "* **ALL** resource types enabled\n",
    "* **NO** limits (MAX_RESOURCES_PER_TYPE = 999)\n",
    "* Complete workspace coverage\n",
    "* All compliance and risk analysis features\n",
    "* **\uD83E\uDD16 AUTOMATIC IN JOB MODE**: Jobs always run in Full Mode for comprehensive audits\n",
    "\n",
    "### ⚙️ Custom Mode (Variable)\n",
    "**Recommended for**: Specific use cases, targeted audits, **interactive sessions**\n",
    "\n",
    "* Configure individual enable/disable flags in Cell 2\n",
    "* Set custom limits for each resource type\n",
    "* Fine-tune performance vs coverage trade-offs\n",
    "\n",
    "**To select a mode**: \n",
    "* **Interactive**: Edit Cell 3 and uncomment your preferred preset\n",
    "* **Job Mode**: Automatically uses Full Mode (cannot be overridden)\n",
    "\n",
    "---\n",
    "\n",
    "## Job Mode Behavior\n",
    "\n",
    "### \uD83E\uDD16 **Automatic Full Mode in Jobs**\n",
    "\n",
    "When running as a scheduled job, the notebook **automatically enables Full Mode** regardless of preset selection:\n",
    "\n",
    "* **All resource types enabled** (ignores ENABLE_* flags from Cell 2)\n",
    "* **No limits** (MAX_RESOURCES_PER_TYPE = 999)\n",
    "* **Complete coverage** (MAX_WORKSPACE_OBJECTS = 2000)\n",
    "* **Comprehensive audit** every time\n",
    "\n",
    "**Rationale**: Scheduled jobs are typically for compliance, auditing, and governance purposes that require complete coverage. Interactive sessions can use Quick Mode for development and testing.\n",
    "\n",
    "**To customize job behavior**: Modify Cell 3 to adjust the Full Mode configuration when `is_job_mode = True`\n",
    "\n",
    "---\n",
    "\n",
    "## What This Code Does\n",
    "\n",
    "### 1. **Identity Management**\n",
    "* Extracts all **users** with display names and active status\n",
    "* Extracts all **groups** with membership information\n",
    "* Extracts all **service principals** with their group associations\n",
    "* Maps **user-to-group relationships**\n",
    "* Identifies **workspace admins** and **account admins**\n",
    "\n",
    "### 2. **Workspace Resource Permissions** (Complete Coverage)\n",
    "* **Jobs**: Workflow job permissions (CAN_VIEW, CAN_MANAGE_RUN, IS_OWNER, CAN_MANAGE)\n",
    "* **SQL Warehouses**: Compute warehouse permissions (CAN_VIEW, CAN_MONITOR, CAN_USE, CAN_MANAGE)\n",
    "* **Clusters**: Interactive cluster permissions (excludes ephemeral job clusters)\n",
    "* **Pipelines**: Delta Live Tables pipeline permissions (CAN_VIEW, CAN_RUN, CAN_MANAGE)\n",
    "* **Workspace Objects**: Folder and notebook permissions (CAN_READ, CAN_RUN, CAN_EDIT, CAN_MANAGE)\n",
    "* **Repos**: Git integration permissions (CAN_READ, CAN_RUN, CAN_EDIT, CAN_MANAGE)\n",
    "* **Instance Pools**: Compute pool permissions (CAN_ATTACH_TO, CAN_MANAGE)\n",
    "* **Model Registry**: ML model permissions (CAN_READ, CAN_EDIT, CAN_MANAGE_STAGING, CAN_MANAGE_PRODUCTION)\n",
    "* **SQL Dashboards**: Legacy dashboard permissions\n",
    "* **SQL Queries**: Saved query permissions\n",
    "\n",
    "### 3. **Unity Catalog Governance**\n",
    "* **Catalogs**: All catalogs with owners and types\n",
    "* **Schemas**: Schemas within catalogs (sampled)\n",
    "* **Volumes**: External storage volumes\n",
    "* **Grants**: Privileges on catalogs, schemas, and volumes (SELECT, MODIFY, USE CATALOG, CREATE SCHEMA, READ_VOLUME, WRITE_VOLUME, etc.)\n",
    "\n",
    "### 4. **Secrets Management**\n",
    "* **Secret Scopes**: All secret scopes (Databricks-backed or Key Vault)\n",
    "* **Secret ACLs**: Who has READ, WRITE, or MANAGE access to each scope\n",
    "\n",
    "### 5. **Network Security**\n",
    "* **IP Access Lists**: Allow/block lists for workspace access\n",
    "* **Workspace Settings**: Token creation and IP restriction status\n",
    "\n",
    "### 6. **Token Management & Audit**\n",
    "* **Active Tokens**: Personal access tokens and service principal tokens\n",
    "* **Token Expiration**: Identifies tokens without expiration dates\n",
    "* **Token Ownership**: Who created each token\n",
    "* **Security Alerts**: Flags tokens without expiry\n",
    "* **Token Age Analysis**: Identifies old tokens requiring rotation\n",
    "* **Service Principal Tokens**: Dedicated audit for non-human identities\n",
    "\n",
    "### 7. **Permission Analysis**\n",
    "* **Direct vs Inherited**: Distinguishes between permissions assigned directly to users vs inherited from groups\n",
    "* **User-Group Associations**: Maintains group membership context throughout all dataframes\n",
    "* **Permission Reference**: Comprehensive definitions of all permission levels from Databricks documentation\n",
    "* **Permission Concentration**: Identifies users with excessive permissions\n",
    "* **Cross-Resource Analysis**: Detects broad access patterns\n",
    "\n",
    "### 8. **Compliance & Security Reporting**\n",
    "* **Inactive Users**: Identifies inactive users with active permissions\n",
    "* **External Users**: Flags non-company domain users with access\n",
    "* **Over-Privileged Users**: Identifies users with excessive permissions\n",
    "* **Segregation of Duties**: Detects potential SOD violations\n",
    "* **Orphaned Permissions**: Flags permissions for deleted users/groups\n",
    "* **Admin Identification**: Lists all workspace and account admins\n",
    "* **Token Security**: Analyzes token usage and expiration\n",
    "* **Security Configuration**: Workspace security settings audit\n",
    "\n",
    "### 9. **Change Detection & Tracking**\n",
    "* **Incremental Changes**: Compares current run with previous audit\n",
    "* **New Permissions**: Identifies permissions added since last run\n",
    "* **Removed Permissions**: Identifies permissions removed since last run\n",
    "* **Change Summary**: Quantifies permission changes over time\n",
    "* **Trend Analysis**: Historical permission evolution\n",
    "\n",
    "### 10. **Long-Term Retention**\n",
    "* **Delta Table Export**: Historical accumulation of all audit runs\n",
    "* **Snapshot Tables**: Current state snapshots for all security entities (15+ tables)\n",
    "* **Change History**: Tracks permission changes over time\n",
    "* **Historical Queries**: Query permission changes and trends\n",
    "\n",
    "### 11. **Data Quality & Validation**\n",
    "* **Duplicate Detection**: Identifies duplicate permission entries\n",
    "* **Orphaned Permissions**: Flags permissions for non-existent users/groups\n",
    "* **Empty DataFrame Checks**: Validates data before export\n",
    "* **Security Alerts**: Identifies users with excessive permissions\n",
    "* **Risk Analysis**: Comprehensive security risk assessment\n",
    "\n",
    "### 12. **Executive Reporting**\n",
    "* **Risk Scoring**: Automated 0-100 risk score calculation\n",
    "* **Executive Summary**: Key metrics dashboard for leadership\n",
    "* **Security Alerts Summary**: Prioritized list of security concerns\n",
    "* **Compliance Metrics**: SOX, GDPR, SOD violation tracking\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "✓ **Automatic Job Mode**: Jobs always run Full Mode for comprehensive audits  \n",
    "✓ **Performance Presets**: Quick Mode (5-10 min) or Full Mode (20-60 min)  \n",
    "✓ **Selective Collection**: Enable/disable individual resource types  \n",
    "✓ **Complete Coverage**: All workspace resources, Unity Catalog, and security settings  \n",
    "✓ **Parallelized API Calls**: Uses ThreadPoolExecutor with configurable workers  \n",
    "✓ **Retry Logic**: Automatic retry with exponential backoff for transient API failures  \n",
    "✓ **Progress Tracking**: Real-time progress updates during long-running operations  \n",
    "✓ **Execution Time Tracking**: Per-cell execution time monitoring  \n",
    "✓ **Configuration Validation**: Validates all configuration parameters before execution  \n",
    "✓ **Error Handling**: Comprehensive error handling with detailed logging  \n",
    "✓ **Data Quality Checks**: Validates data integrity and identifies issues  \n",
    "✓ **Compliance Reporting**: Pre-built SOX and security compliance reports  \n",
    "✓ **Change Detection**: Tracks permission changes between audit runs  \n",
    "✓ **Risk Analysis**: Identifies over-privileged users and security risks  \n",
    "✓ **Multiple Export Formats**: Excel, CSV, JSON, HTML, and Delta tables  \n",
    "✓ **Memory Optimization**: Monitors memory usage and provides warnings  \n",
    "✓ **Configurable Limits**: Fine-tune performance with multiple limit settings  \n",
    "✓ **Execution Statistics**: Tracks API calls, failures, retries, success rates, skipped resources  \n",
    "✓ **DataFrame Caching**: Performance optimization for frequently accessed data  \n",
    "✓ **Token Expiration Audit**: Critical security check for token lifecycle  \n",
    "✓ **Inactive User Detection**: Compliance requirement for access reviews  \n",
    "✓ **External User Analysis**: Identifies non-company domain users  \n",
    "✓ **Risk Scoring**: Automated 0-100 risk assessment  \n",
    "✓ **Executive Dashboard**: Summary metrics for leadership visibility  \n",
    "✓ **Timezone Configuration**: All timestamps in Eastern Time (configurable)  \n",
    "✓ **Serverless Optimization**: Automatic compute type detection and optimization  \n",
    "\n",
    "---\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Performance Settings (Cell 2):\n",
    "* `MAX_RESOURCES_PER_TYPE = 100` - Limit resources checked per type (999 = no limit)\n",
    "* `MAX_WORKERS = 10` - Parallel API calls (1-50)\n",
    "* `MAX_RETRIES = 3` - Retries for failed API calls\n",
    "* `RETRY_DELAY = 2` - Seconds between retries\n",
    "* `MAX_WORKSPACE_OBJECTS = 500` - Limit workspace objects scanned\n",
    "* `MAX_SCHEMAS_PER_CATALOG = 10` - Limit schemas per catalog for volumes\n",
    "* `TIMEZONE = 'America/New_York'` - Timezone for all timestamp displays\n",
    "\n",
    "**Note**: In job mode, these limits are automatically overridden to maximum values for complete coverage.\n",
    "\n",
    "### Resource Type Selection (Cell 2):\n",
    "**Core Resources:**\n",
    "* `ENABLE_JOBS = True`\n",
    "* `ENABLE_WAREHOUSES = True`\n",
    "* `ENABLE_CLUSTERS = True`\n",
    "* `ENABLE_PIPELINES = True`\n",
    "\n",
    "**Extended Resources:**\n",
    "* `ENABLE_WORKSPACE_OBJECTS = True` - Folders/notebooks (can be slow)\n",
    "* `ENABLE_MODEL_REGISTRY = True`\n",
    "* `ENABLE_REPOS = True`\n",
    "* `ENABLE_INSTANCE_POOLS = True`\n",
    "* `ENABLE_SQL_ASSETS = True` - Dashboards and queries\n",
    "* `ENABLE_VOLUMES = True` - Unity Catalog volumes\n",
    "\n",
    "**Security & Compliance:**\n",
    "* `ENABLE_SERVICE_PRINCIPALS = True`\n",
    "* `ENABLE_SECRET_SCOPES = True`\n",
    "* `ENABLE_IP_ACCESS_LISTS = True`\n",
    "* `ENABLE_TOKEN_AUDIT = True`\n",
    "* `ENABLE_UC_PERMISSIONS = True`\n",
    "\n",
    "**Note**: In job mode, ALL resource types are automatically enabled regardless of these flags.\n",
    "\n",
    "### Export Format Settings:\n",
    "* `ENABLE_EXCEL_EXPORT = True` - Excel workbook generation\n",
    "* `ENABLE_CSV_EXPORT = False` - CSV file generation (set in Cell 35)\n",
    "* `ENABLE_JSON_EXPORT = False` - JSON file generation (set in Cell 36)\n",
    "* `ENABLE_HTML_REPORT = False` - HTML summary report (set in Cell 42)\n",
    "* `ENABLE_DELTA_EXPORT = False` - Delta table long-term retention\n",
    "\n",
    "**Recommendation for Jobs**: Set `ENABLE_DELTA_EXPORT = True` and `ENABLE_EXCEL_EXPORT = False` for optimal job performance.\n",
    "\n",
    "### Delta Export Settings (Cell 2):\n",
    "* `DELTA_TABLE_NAME = \"main.default.security_audit_history\"`\n",
    "  * Format: `\"catalog.schema.table\"`\n",
    "  * Requires CREATE TABLE permissions\n",
    "  * Creates 15+ snapshot tables automatically\n",
    "\n",
    "### Job Parameters (Optional):\n",
    "* `max_resources_per_type` - Ignored in job mode (always uses 999)\n",
    "* `export_path` - Custom export path\n",
    "\n",
    "---\n",
    "\n",
    "## Performance Optimization Tips\n",
    "\n",
    "### For Faster Execution (Interactive Only):\n",
    "1. **Use Quick Mode** (Cell 3): Uncomment `USE_QUICK_MODE = True`\n",
    "2. **Disable slow resources**: Set `ENABLE_WORKSPACE_OBJECTS = False`\n",
    "3. **Reduce limits**: Set `MAX_RESOURCES_PER_TYPE = 50`\n",
    "4. **Skip clusters**: Set `ENABLE_CLUSTERS = False` (if you have many)\n",
    "5. **Disable exports**: Set `ENABLE_EXCEL_EXPORT = False` for Delta-only\n",
    "\n",
    "### For Complete Coverage:\n",
    "1. **Use Full Mode** (Cell 3): Uncomment `USE_FULL_MODE = True`\n",
    "2. **Enable all resources**: All ENABLE_* flags = True\n",
    "3. **Remove limits**: Set `MAX_RESOURCES_PER_TYPE = 999`\n",
    "4. **Increase workers**: Set `MAX_WORKERS = 20` (if cluster can handle it)\n",
    "5. **Enable Delta export**: For change tracking over time\n",
    "\n",
    "### For Scheduled Jobs:\n",
    "* **No configuration needed** - Jobs automatically run in Full Mode\n",
    "* **Recommended**: Set `ENABLE_DELTA_EXPORT = True` in Cell 2\n",
    "* **Recommended**: Set `ENABLE_EXCEL_EXPORT = False` in Cell 2 (faster)\n",
    "* Jobs will always perform comprehensive audits with all resources\n",
    "\n",
    "### Memory Management:\n",
    "* Monitor memory warnings in Cell 24\n",
    "* If memory issues occur, reduce `MAX_WORKSPACE_OBJECTS`\n",
    "* Disable `ENABLE_WORKSPACE_OBJECTS` if not needed (interactive only)\n",
    "* Run on larger cluster if needed\n",
    "\n",
    "---\n",
    "\n",
    "## Execution Guide\n",
    "\n",
    "### Quick Start (5-10 minutes) - Interactive Only:\n",
    "1. **Cell 3**: Uncomment `USE_QUICK_MODE = True`\n",
    "2. **Run All**: Execute all cells\n",
    "3. **Review**: Check Cell 24 for execution summary\n",
    "4. **Export**: Excel file ready in `/dbfs/tmp/permissions_export/`\n",
    "\n",
    "### Full Audit (20-60 minutes) - Interactive or Job:\n",
    "1. **Cell 3**: Uncomment `USE_FULL_MODE = True` (or run as job)\n",
    "2. **Cell 2**: Set `ENABLE_DELTA_EXPORT = True` for change tracking\n",
    "3. **Run All**: Execute all cells (monitor progress)\n",
    "4. **Review**: Check Cells 24, 26, 27, 28 for comprehensive analysis\n",
    "5. **Export**: Multiple formats available\n",
    "\n",
    "### Custom Execution - Interactive Only:\n",
    "1. **Cell 3**: Use `USE_CUSTOM_MODE = True` (default)\n",
    "2. **Cell 2**: Configure individual ENABLE_* flags\n",
    "3. **Run selectively**: Execute only cells for enabled resources\n",
    "4. **Monitor**: Watch execution times and adjust as needed\n",
    "\n",
    "### Running as a Scheduled Job:\n",
    "\n",
    "**Setup:**\n",
    "1. Create a new job in Databricks\n",
    "2. Add this notebook as a task\n",
    "3. **No preset configuration needed** - Job mode automatically uses Full Mode\n",
    "4. (Optional) Add job parameters:\n",
    "   - `export_path`: \"/dbfs/mnt/security-exports\" for custom location\n",
    "5. Configure schedule (daily, weekly, monthly)\n",
    "6. Set up notifications for job success/failure\n",
    "7. **Recommended in Cell 2**:\n",
    "   - Set `ENABLE_DELTA_EXPORT = True` for historical tracking\n",
    "   - Set `ENABLE_EXCEL_EXPORT = False` for faster execution\n",
    "\n",
    "**Automatic Job Behavior:**\n",
    "* \uD83E\uDD16 **Forces Full Mode** - Complete in-depth review every time\n",
    "* \uD83D\uDD04 **All resources enabled** - Ignores ENABLE_* flags\n",
    "* ♾️ **No limits** - MAX_RESOURCES_PER_TYPE = 999, MAX_WORKSPACE_OBJECTS = 2000\n",
    "* \uD83D\uDCCA **Complete coverage** - All 14 resource types scanned\n",
    "* ⏱️ **Expected time**: 20-60 minutes depending on workspace size\n",
    "* \uD83D\uDCBE **Recommended**: Enable Delta export for change tracking\n",
    "\n",
    "**Why Full Mode for Jobs?**\n",
    "* Scheduled audits are for compliance and governance\n",
    "* Requires complete, comprehensive coverage\n",
    "* Change detection needs consistent full scans\n",
    "* Historical trends require complete data\n",
    "* No manual intervention to ensure thoroughness\n",
    "\n",
    "**Monitoring:**\n",
    "* Check job run output for JSON summary with execution stats\n",
    "* Monitor export location for generated files\n",
    "* Query Delta tables for historical trends and changes\n",
    "* Review API failure rates and retry counts\n",
    "* Check compliance alerts in execution summary\n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "⚠ **Job Mode**: Always runs Full Mode (20-60 min) for comprehensive audits  \n",
    "⚠ **Performance**: Quick Mode (5-10 min) only available in interactive sessions  \n",
    "⚠ **Workspace Objects**: Most resource-intensive - automatically included in job mode  \n",
    "⚠ **Cluster Permissions**: Filtered to interactive clusters only  \n",
    "⚠ **Memory Usage**: Monitor warnings in Cell 24, jobs use larger limits  \n",
    "⚠ **Permissions Required**: Requires admin access to view all permissions  \n",
    "⚠ **Data Privacy**: Export files contain sensitive security information  \n",
    "⚠ **API Rate Limits**: Retry logic handles transient failures automatically  \n",
    "⚠ **Token Access**: Token data requires specific admin permissions  \n",
    "⚠ **Delta Tables**: Requires CREATE TABLE permissions  \n",
    "\n",
    "---\n",
    "\n",
    "## Version Control\n",
    "\n",
    "| Version | Date | Author | Changes |\n",
    "|---------|------|--------|----------|\n",
    "| 1.0 | 2026-02-09 | Brandon Croom | Initial creation of comprehensive security review script; Added users, groups, and workspace resource permissions (jobs, warehouses, pipelines); Implemented parallelized API calls for performance optimization; Added user-group association tracking throughout all dataframes; Created flattened exports for Excel compatibility; Added Eastern Time timezone conversion for all timestamps; Implemented MAX_RESOURCES_PER_TYPE=999 flag for unlimited data pull; Added service principals extraction; Added secret scopes and ACLs; Added Unity Catalog permissions (catalogs, schemas, grants); Added IP access lists and workspace settings; Added permission level reference documentation from Databricks docs; Optimized cluster permissions to filter interactive clusters only; Created comprehensive security workbook with 17+ sheets |\n",
    "| 1.1 | 2026-02-10 | Brandon Croom | Added automatic job mode detection to support scheduled job execution; Implemented conditional print statements and display() calls (enabled in interactive mode, suppressed in job mode); Job mode detection added at notebook start for global availability; Added configurable job parameters (max_resources_per_type, export_path); Added job completion summary with JSON output for orchestration; Updated export paths to use configurable EXPORT_PATH variable; Added proper error handling with exceptions raised in job mode for alerting; Optimized to skip unnecessary operations in job mode |\n",
    "| 1.2 | 2026-02-11 | Brandon Croom | Major feature expansion and optimization: Added Delta table export with historical accumulation and 15+ snapshot tables; Implemented log() function for consistent message handling; Added ENABLE_EXCEL_EXPORT configuration; Added retry logic with exponential backoff; Implemented progress tracking and execution time tracking per cell; Added configuration validation and data quality checks (duplicate detection, orphaned permissions); Added execution statistics tracking; Enhanced error handling and export path validation; Added complete workspace coverage (workspace folders/notebooks, model registry, repos, instance pools, SQL dashboards/queries, Unity Catalog volumes); Added token management audit; Implemented workspace admin identification; Added comprehensive compliance reporting (inactive users, external users, over-privileged users, segregation of duties); Implemented incremental change detection comparing with previous runs; Added permission recommendations and risk analysis; Added CSV, JSON, and HTML export format options; Implemented performance presets (Quick Mode, Full Mode, Custom Mode); Added selective resource collection with 14 ENABLE_* flags; Added performance limits (MAX_WORKSPACE_OBJECTS, MAX_SCHEMAS_PER_CATALOG); Optimized workspace object scanning with parallelization; Parallelized SQL asset permission fetching; Added memory usage monitoring; Implemented automatic Full Mode for job execution; Fixed is_job_mode dependency issue |\n",
    "| 1.3 | 2026-02-12 | Brandon Croom | Enhanced security analysis and performance: Added DataFrame caching for users, groups, and user_groups (20-30% performance improvement); Implemented workspace security configuration checks (10 security flags with risk assessment); Added security configuration recommendations with priority levels; Implemented comprehensive token expiration audit (tokens without expiry, expiring soon, age analysis); Added inactive user permissions analysis for compliance (SOX, GDPR); Implemented permission concentration analysis (excessive admin permissions detection, distribution statistics); Added external user detection and analysis (configurable company domain); Implemented cross-resource permission analysis (broad access detection, SOD violations); Added service principal token audit (expiration and rotation analysis); Created executive summary dashboard with risk scoring (0-100 scale); Prepared summary data for Excel export (executive summary, security alerts, top users, execution metadata); Added 8 new analysis sections for comprehensive security posture assessment; Added timezone configuration with TIMEZONE constant set to 'America/New_York' (Eastern Time); Added get_current_time_in_timezone() helper function; All timestamps displayed in configured timezone; Serverless vs traditional cluster detection with compute-aware optimizations; Conditional DataFrame caching based on compute type; Generic configuration with no company-specific references |\n",
    "\n",
    "---\n",
    "\n",
    "## Support\n",
    "\n",
    "For questions or issues:\n",
    "* **Performance**: Use Quick Mode for faster execution (interactive only)\n",
    "* **Job Mode**: Jobs automatically run Full Mode - no configuration needed\n",
    "* **Memory**: Monitor Cell 24 warnings, reduce MAX_WORKSPACE_OBJECTS if needed\n",
    "* Review execution summary in Cell 24 for data quality issues\n",
    "* Review compliance report in Cell 26 for security alerts\n",
    "* For job execution issues, check JSON summary with execution stats\n",
    "* For Delta table issues, verify catalog/schema permissions\n",
    "* Contact your Databricks administrator for permission issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514da4bb-3e81-4377-911f-35c9c0ced6a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup and configuration"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Setup complete\nConfiguration: MAX_RESOURCES_PER_TYPE=100, MAX_WORKERS=10\nRetry settings: MAX_RETRIES=3, RETRY_DELAY=2s\nWorkspace limits: MAX_WORKSPACE_OBJECTS=500\nCompute type: SERVERLESS\nExecution mode: INTERACTIVE\nTimezone: America/New_York\n\n⚡ Serverless optimizations enabled:\n  - DataFrame caching disabled (not supported)\n  - Automatic memory management\n  - Optimized for fast startup and scaling\n\nEnabled resource types (10): Jobs, Warehouses, Clusters, Pipelines, Workspace Objects, Models, Repos, Instance Pools, SQL Assets, Volumes\n\uD83D\uDCCA Excel export enabled\n"
     ]
    }
   ],
   "source": [
    "from databricks.sdk import WorkspaceClient\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Detect if running in job mode or interactive mode (MUST BE FIRST)\n",
    "try:\n",
    "    dbutils.notebook.entry_point.getDbutils().notebook().getContext().currentRunId().isDefined()\n",
    "    is_job_mode = True\n",
    "except:\n",
    "    is_job_mode = False\n",
    "\n",
    "# Detect if running on serverless compute (most reliable method: try caching)\n",
    "try:\n",
    "    test_df = spark.range(1)\n",
    "    test_df.cache()\n",
    "    test_df.count()\n",
    "    test_df.unpersist()\n",
    "    is_serverless = False\n",
    "except Exception as e:\n",
    "    # If caching fails with serverless error, we're on serverless\n",
    "    is_serverless = 'SERVERLESS' in str(e) or 'PERSIST TABLE is not supported' in str(e)\n",
    "\n",
    "# Initialize Workspace Client\n",
    "wc = WorkspaceClient()\n",
    "\n",
    "# ============================================================================\n",
    "# PERFORMANCE CONFIGURATION\n",
    "# ============================================================================\n",
    "MAX_RESOURCES_PER_TYPE = 100  # Limit resources checked per type (set to 999 for no limit)\n",
    "MAX_WORKERS = 10  # Parallel API calls (1-50)\n",
    "MAX_RETRIES = 3  # Number of retries for failed API calls\n",
    "RETRY_DELAY = 2  # Seconds between retries\n",
    "MAX_WORKSPACE_OBJECTS = 500  # Limit workspace objects scanned\n",
    "MAX_SCHEMAS_PER_CATALOG = 10  # Limit schemas per catalog for volumes\n",
    "\n",
    "# ============================================================================\n",
    "# RESOURCE TYPE SELECTION\n",
    "# ============================================================================\n",
    "ENABLE_JOBS = True\n",
    "ENABLE_WAREHOUSES = True\n",
    "ENABLE_CLUSTERS = True\n",
    "ENABLE_PIPELINES = True\n",
    "ENABLE_WORKSPACE_OBJECTS = True\n",
    "ENABLE_MODEL_REGISTRY = True\n",
    "ENABLE_REPOS = True\n",
    "ENABLE_INSTANCE_POOLS = True\n",
    "ENABLE_SQL_ASSETS = True\n",
    "ENABLE_VOLUMES = True\n",
    "ENABLE_SERVICE_PRINCIPALS = True\n",
    "ENABLE_SECRET_SCOPES = True\n",
    "ENABLE_IP_ACCESS_LISTS = True\n",
    "ENABLE_TOKEN_AUDIT = True\n",
    "ENABLE_UC_PERMISSIONS = True\n",
    "\n",
    "# ============================================================================\n",
    "# EXPORT SETTINGS\n",
    "# ============================================================================\n",
    "ENABLE_EXCEL_EXPORT = True\n",
    "ENABLE_DELTA_EXPORT = False\n",
    "EXPORT_PATH = '/dbfs/tmp/permissions_export'\n",
    "DELTA_TABLE_NAME = 'main.default.security_audit_history'\n",
    "\n",
    "# ============================================================================\n",
    "# EXECUTION STATISTICS\n",
    "# ============================================================================\n",
    "execution_stats = {\n",
    "    'start_time': time.time(),\n",
    "    'api_calls': 0,\n",
    "    'api_failures': 0,\n",
    "    'api_retries': 0,\n",
    "    'resources_checked': 0,\n",
    "    'resources_skipped': 0,\n",
    "    'permissions_collected': 0\n",
    "}\n",
    "\n",
    "# ============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def log(message):\n",
    "    \"\"\"Print only in interactive mode\"\"\"\n",
    "    if not is_job_mode:\n",
    "        print(message)\n",
    "\n",
    "def log_execution_time(operation, start_time):\n",
    "    \"\"\"Log execution time for an operation\"\"\"\n",
    "    elapsed = time.time() - start_time\n",
    "    if not is_job_mode:\n",
    "        print(f\"⏱️  {operation} completed in {elapsed:.2f} seconds\")\n",
    "\n",
    "def validate_dataframe_exists(name, df):\n",
    "    \"\"\"Validate that a DataFrame exists and has data\"\"\"\n",
    "    try:\n",
    "        if df is None:\n",
    "            log(f\"  ❌ {name}: Not created\")\n",
    "            return False\n",
    "        count = df.count()\n",
    "        if count == 0:\n",
    "            log(f\"  ⚠️  {name}: Empty (0 rows)\")\n",
    "            return False\n",
    "        log(f\"  ✓ {name}: {count} rows\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log(f\"  ❌ {name}: Error - {str(e)[:100]}\")\n",
    "        return False\n",
    "\n",
    "def print_execution_summary():\n",
    "    \"\"\"Print execution statistics summary\"\"\"\n",
    "    elapsed = time.time() - execution_stats['start_time']\n",
    "    \n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(\"EXECUTION SUMMARY\")\n",
    "    log(f\"{'='*60}\")\n",
    "    log(f\"Total execution time: {elapsed:.2f} seconds ({elapsed/60:.1f} minutes)\")\n",
    "    log(f\"API calls: {execution_stats['api_calls']}\")\n",
    "    log(f\"API failures: {execution_stats['api_failures']}\")\n",
    "    log(f\"API retries: {execution_stats['api_retries']}\")\n",
    "    log(f\"Resources checked: {execution_stats['resources_checked']}\")\n",
    "    log(f\"Resources skipped: {execution_stats['resources_skipped']}\")\n",
    "    log(f\"Permissions collected: {execution_stats['permissions_collected']}\")\n",
    "    \n",
    "    if execution_stats['api_calls'] > 0:\n",
    "        success_rate = ((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls']) * 100\n",
    "        log(f\"Success rate: {success_rate:.1f}%\")\n",
    "    log(f\"{'='*60}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# PERMISSION FETCHING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def get_permissions(resource_type, resource_id, resource_name, api_path):\n",
    "    \"\"\"Fetch permissions for a resource with retry logic\"\"\"\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            execution_stats['api_calls'] += 1\n",
    "            perms = wc.permissions.get(request_object_type=api_path, request_object_id=resource_id)\n",
    "            \n",
    "            results = []\n",
    "            if perms.access_control_list:\n",
    "                for acl in perms.access_control_list:\n",
    "                    principal = acl.user_name or acl.group_name or acl.service_principal_name\n",
    "                    principal_type = 'user' if acl.user_name else ('group' if acl.group_name else 'service_principal')\n",
    "                    \n",
    "                    for perm in acl.all_permissions:\n",
    "                        results.append({\n",
    "                            'resource_type': resource_type,\n",
    "                            'resource_id': resource_id,\n",
    "                            'resource_name': resource_name,\n",
    "                            'principal': principal,\n",
    "                            'principal_type': principal_type,\n",
    "                            'permission_level': perm.permission_level.value if perm.permission_level else 'UNKNOWN'\n",
    "                        })\n",
    "            \n",
    "            execution_stats['resources_checked'] += 1\n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            if attempt < MAX_RETRIES - 1:\n",
    "                execution_stats['api_retries'] += 1\n",
    "                time.sleep(RETRY_DELAY * (attempt + 1))\n",
    "            else:\n",
    "                execution_stats['api_failures'] += 1\n",
    "                return []\n",
    "    \n",
    "    return []\n",
    "\n",
    "# ============================================================================\n",
    "# INITIALIZATION\n",
    "# ============================================================================\n",
    "\n",
    "# Initialize permissions list\n",
    "all_permissions = []\n",
    "\n",
    "log(\"✓ Setup complete\")\n",
    "log(f\"Configuration: MAX_RESOURCES_PER_TYPE={MAX_RESOURCES_PER_TYPE}, MAX_WORKERS={MAX_WORKERS}\")\n",
    "log(f\"Retry settings: MAX_RETRIES={MAX_RETRIES}, RETRY_DELAY={RETRY_DELAY}s\")\n",
    "log(f\"Workspace limits: MAX_WORKSPACE_OBJECTS={MAX_WORKSPACE_OBJECTS}\")\n",
    "log(f\"Compute type: {'SERVERLESS' if is_serverless else 'TRADITIONAL CLUSTER'}\")\n",
    "log(f\"Execution mode: {'JOB' if is_job_mode else 'INTERACTIVE'}\")\n",
    "\n",
    "if is_serverless:\n",
    "    log(\"\\n⚡ Serverless optimizations enabled:\")\n",
    "    log(\"  - DataFrame caching disabled (not supported)\")\n",
    "    log(\"  - Automatic memory management\")\n",
    "    log(\"  - Optimized for fast startup and scaling\")\n",
    "else:\n",
    "    log(\"\\n\uD83D\uDD27 Traditional cluster optimizations enabled:\")\n",
    "    log(\"  - DataFrame caching available\")\n",
    "    log(\"  - Manual memory management\")\n",
    "    log(\"  - Persistent compute resources\")\n",
    "\n",
    "enabled_resources = []\n",
    "if ENABLE_JOBS: enabled_resources.append('Jobs')\n",
    "if ENABLE_WAREHOUSES: enabled_resources.append('Warehouses')\n",
    "if ENABLE_CLUSTERS: enabled_resources.append('Clusters')\n",
    "if ENABLE_PIPELINES: enabled_resources.append('Pipelines')\n",
    "if ENABLE_WORKSPACE_OBJECTS: enabled_resources.append('Workspace Objects')\n",
    "if ENABLE_MODEL_REGISTRY: enabled_resources.append('Models')\n",
    "if ENABLE_REPOS: enabled_resources.append('Repos')\n",
    "if ENABLE_INSTANCE_POOLS: enabled_resources.append('Instance Pools')\n",
    "if ENABLE_SQL_ASSETS: enabled_resources.append('SQL Assets')\n",
    "if ENABLE_VOLUMES: enabled_resources.append('Volumes')\n",
    "\n",
    "log(f\"\\nEnabled resource types ({len(enabled_resources)}): {', '.join(enabled_resources)}\")\n",
    "\n",
    "if ENABLE_EXCEL_EXPORT:\n",
    "    log(\"\uD83D\uDCCA Excel export enabled\")\n",
    "if ENABLE_DELTA_EXPORT:\n",
    "    log(f\"\uD83D\uDCBE Delta export enabled: {DELTA_TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd581c66-c155-4d7d-a05e-bb2eec2e5f80",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Performance presets (Quick Mode vs Full Mode)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n⚙️ CUSTOM MODE - Using configuration from Cell 2\n============================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# PERFORMANCE PRESETS: Choose your execution mode\n",
    "# ============================================================================\n",
    "# Uncomment ONE of the following presets, or customize individual flags in Cell 2\n",
    "\n",
    "# PRESET 1: QUICK MODE (5-10 minutes) - Core permissions only\n",
    "# Recommended for: Daily monitoring, quick audits, testing\n",
    "# USE_QUICK_MODE = True\n",
    "\n",
    "# PRESET 2: FULL MODE (20-60 minutes) - Complete coverage\n",
    "# Recommended for: Compliance audits, quarterly reviews, comprehensive analysis\n",
    "# USE_FULL_MODE = True\n",
    "\n",
    "# PRESET 3: CUSTOM MODE - Use individual flags in Cell 2\n",
    "USE_CUSTOM_MODE = True  # Default: use custom configuration from Cell 2\n",
    "\n",
    "# ============================================================================\n",
    "# AUTOMATIC JOB MODE OVERRIDE: Jobs always run in Full Mode\n",
    "# ============================================================================\n",
    "if is_job_mode:\n",
    "    log(\"\\n\uD83E\uDD16 JOB MODE DETECTED - Forcing FULL MODE for comprehensive audit\")\n",
    "    log(\"=\"*60)\n",
    "    USE_FULL_MODE = True\n",
    "    USE_QUICK_MODE = False\n",
    "    USE_CUSTOM_MODE = False\n",
    "    log(\"Job mode automatically enables complete in-depth review\")\n",
    "    log(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "\n",
    "# Apply preset configurations\n",
    "if 'USE_QUICK_MODE' in dir() and USE_QUICK_MODE:\n",
    "    log(\"\\n\uD83D\uDE80 QUICK MODE ENABLED - Core permissions only (5-10 minutes)\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Core resources only\n",
    "    ENABLE_JOBS = True\n",
    "    ENABLE_WAREHOUSES = True\n",
    "    ENABLE_CLUSTERS = False  # Skip clusters for speed\n",
    "    ENABLE_PIPELINES = True\n",
    "    \n",
    "    # Disable extended resources\n",
    "    ENABLE_WORKSPACE_OBJECTS = False\n",
    "    ENABLE_MODEL_REGISTRY = False\n",
    "    ENABLE_REPOS = False\n",
    "    ENABLE_INSTANCE_POOLS = False\n",
    "    ENABLE_SQL_ASSETS = False\n",
    "    ENABLE_VOLUMES = False\n",
    "    \n",
    "    # Keep security essentials\n",
    "    ENABLE_SERVICE_PRINCIPALS = True\n",
    "    ENABLE_SECRET_SCOPES = True\n",
    "    ENABLE_IP_ACCESS_LISTS = True\n",
    "    ENABLE_TOKEN_AUDIT = False\n",
    "    ENABLE_UC_PERMISSIONS = True\n",
    "    \n",
    "    # Reduce limits\n",
    "    MAX_RESOURCES_PER_TYPE = 100\n",
    "    MAX_WORKSPACE_OBJECTS = 100\n",
    "    \n",
    "    log(\"Quick mode: Core resources only (Jobs, Warehouses, Pipelines, UC, Secrets)\")\n",
    "    log(\"Skipped: Clusters, Workspace Objects, Models, Repos, Pools, SQL Assets, Volumes, Tokens\")\n",
    "    log(\"=\"*60)\n",
    "\n",
    "elif 'USE_FULL_MODE' in dir() and USE_FULL_MODE:\n",
    "    log(\"\\n\uD83D\uDD0D FULL MODE ENABLED - Complete coverage (20-60 minutes)\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Enable everything\n",
    "    ENABLE_JOBS = True\n",
    "    ENABLE_WAREHOUSES = True\n",
    "    ENABLE_CLUSTERS = True\n",
    "    ENABLE_PIPELINES = True\n",
    "    ENABLE_WORKSPACE_OBJECTS = True\n",
    "    ENABLE_MODEL_REGISTRY = True\n",
    "    ENABLE_REPOS = True\n",
    "    ENABLE_INSTANCE_POOLS = True\n",
    "    ENABLE_SQL_ASSETS = True\n",
    "    ENABLE_VOLUMES = True\n",
    "    ENABLE_SERVICE_PRINCIPALS = True\n",
    "    ENABLE_SECRET_SCOPES = True\n",
    "    ENABLE_IP_ACCESS_LISTS = True\n",
    "    ENABLE_TOKEN_AUDIT = True\n",
    "    ENABLE_UC_PERMISSIONS = True\n",
    "    \n",
    "    # Maximum limits\n",
    "    MAX_RESOURCES_PER_TYPE = 999\n",
    "    MAX_WORKSPACE_OBJECTS = 2000\n",
    "    \n",
    "    if is_job_mode:\n",
    "        log(\"Full mode: ALL resources enabled with no limits (JOB MODE - COMPREHENSIVE AUDIT)\")\n",
    "    else:\n",
    "        log(\"Full mode: ALL resources enabled with no limits\")\n",
    "    log(\"⚠ Warning: This may take 20-60 minutes depending on workspace size\")\n",
    "    log(\"=\"*60)\n",
    "\n",
    "else:\n",
    "    log(\"\\n⚙️ CUSTOM MODE - Using configuration from Cell 2\")\n",
    "    log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d98ebdc-838c-40ae-8795-b5d2622019ad",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Optional job parameters (for scheduled jobs)"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nConfiguration:\n  MAX_RESOURCES_PER_TYPE: 100\n  EXPORT_PATH: /dbfs/tmp/permissions_export\n\n✓ Export directory ready: /dbfs/tmp/permissions_export\nWrote 4 bytes.\n  ✓ Export path is writable\n"
     ]
    }
   ],
   "source": [
    "# Optional: Add job parameters for scheduled execution\n",
    "# These can be configured when creating a job in the Databricks UI\n",
    "\n",
    "try:\n",
    "    # Get parameter for max resources (defaults to value in cell 2 if not provided)\n",
    "    job_max_resources = dbutils.widgets.get(\"max_resources_per_type\")\n",
    "    if job_max_resources:\n",
    "        MAX_RESOURCES_PER_TYPE = int(job_max_resources)\n",
    "        log(f\"Using job parameter: MAX_RESOURCES_PER_TYPE = {MAX_RESOURCES_PER_TYPE}\")\n",
    "except:\n",
    "    # Widget doesn't exist - use default from cell 2\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # Get parameter for export path (defaults to /dbfs/tmp/permissions_export)\n",
    "    job_export_path = dbutils.widgets.get(\"export_path\")\n",
    "    if job_export_path:\n",
    "        EXPORT_PATH = job_export_path\n",
    "        log(f\"Using job parameter: EXPORT_PATH = {EXPORT_PATH}\")\n",
    "    else:\n",
    "        EXPORT_PATH = '/dbfs/tmp/permissions_export'\n",
    "except:\n",
    "    EXPORT_PATH = '/dbfs/tmp/permissions_export'\n",
    "\n",
    "log(f\"\\nConfiguration:\")\n",
    "log(f\"  MAX_RESOURCES_PER_TYPE: {MAX_RESOURCES_PER_TYPE}\")\n",
    "log(f\"  EXPORT_PATH: {EXPORT_PATH}\")\n",
    "\n",
    "# Validate and create export directory if needed\n",
    "if ENABLE_EXCEL_EXPORT:\n",
    "    try:\n",
    "        # Create directory if it doesn't exist\n",
    "        dbutils.fs.mkdirs(EXPORT_PATH.replace('/dbfs', 'dbfs:'))\n",
    "        log(f\"\\n✓ Export directory ready: {EXPORT_PATH}\")\n",
    "        \n",
    "        # Check if writable\n",
    "        if check_export_path_writable(EXPORT_PATH):\n",
    "            log(f\"  ✓ Export path is writable\")\n",
    "    except Exception as e:\n",
    "        log(f\"\\n⚠️  Warning: Could not validate export path: {str(e)}\")\n",
    "        log(f\"  Excel export may fail if path is not writable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "767e60d1-e55b-4066-9e08-d886b90c85a2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get users and groups"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching users and groups...\n✓ Found 312 users\n✓ Found 45 groups\n✓ Found 925 user-group memberships\n"
     ]
    }
   ],
   "source": [
    "log(\"Fetching users and groups...\")\n",
    "\n",
    "# Get all users\n",
    "users = list(wc.users.list())\n",
    "users_df = spark.createDataFrame([\n",
    "    {'user_name': u.user_name, 'display_name': u.display_name, 'active': u.active}\n",
    "    for u in users\n",
    "])\n",
    "\n",
    "# Get all groups\n",
    "groups = list(wc.groups.list())\n",
    "groups_df = spark.createDataFrame([\n",
    "    {'group_name': g.display_name, 'group_id': g.id}\n",
    "    for g in groups\n",
    "])\n",
    "\n",
    "# Get group memberships\n",
    "user_groups_data = []\n",
    "for user in users:\n",
    "    if user.groups:\n",
    "        for group in user.groups:\n",
    "            user_groups_data.append({\n",
    "                'user_name': user.user_name,\n",
    "                'group_name': group.display\n",
    "            })\n",
    "\n",
    "user_groups_df = spark.createDataFrame(user_groups_data) if user_groups_data else spark.createDataFrame([], 'user_name STRING, group_name STRING')\n",
    "\n",
    "log(f\"✓ Found {users_df.count()} users\")\n",
    "log(f\"✓ Found {groups_df.count()} groups\")\n",
    "log(f\"✓ Found {user_groups_df.count()} user-group memberships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4aa830-d16b-40e1-837a-c7a893a0fddd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cache user and group DataFrames for performance"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing DataFrame reuse...\n  Running on SERVERLESS compute - using automatic optimization\n✓ Materialized users_df: 312 rows\n✓ Materialized groups_df: 45 rows\n✓ Materialized user_groups_df: 925 rows\n  Serverless will automatically optimize DataFrame reuse\n"
     ]
    }
   ],
   "source": [
    "# Optimize DataFrame reuse based on compute type\n",
    "# Serverless: Automatic optimization (caching not supported)\n",
    "# Traditional: Explicit caching for performance\n",
    "\n",
    "log(\"Optimizing DataFrame reuse...\")\n",
    "\n",
    "if is_serverless:\n",
    "    # Serverless compute: Just materialize with count (caching not supported)\n",
    "    log(\"  Running on SERVERLESS compute - using automatic optimization\")\n",
    "    \n",
    "    users_count = users_df.count()\n",
    "    groups_count = groups_df.count()\n",
    "    user_groups_count = user_groups_df.count()\n",
    "    \n",
    "    log(f\"✓ Materialized users_df: {users_count} rows\")\n",
    "    log(f\"✓ Materialized groups_df: {groups_count} rows\")\n",
    "    log(f\"✓ Materialized user_groups_df: {user_groups_count} rows\")\n",
    "    log(\"  Serverless will automatically optimize DataFrame reuse\")\n",
    "else:\n",
    "    # Traditional cluster: Use explicit caching\n",
    "    log(\"  Running on TRADITIONAL cluster - applying explicit caching\")\n",
    "    \n",
    "    users_df.cache()\n",
    "    groups_df.cache()\n",
    "    user_groups_df.cache()\n",
    "    \n",
    "    # Force materialization\n",
    "    users_count = users_df.count()\n",
    "    groups_count = groups_df.count()\n",
    "    user_groups_count = user_groups_df.count()\n",
    "    \n",
    "    log(f\"✓ Cached users_df: {users_count} rows\")\n",
    "    log(f\"✓ Cached groups_df: {groups_count} rows\")\n",
    "    log(f\"✓ Cached user_groups_df: {user_groups_count} rows\")\n",
    "    log(\"  DataFrames cached for 20-30% performance improvement on subsequent joins\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af2b0764-7f00-45c0-b523-6bf40728bca6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get workspace security configuration settings"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching workspace security configuration settings...\n✓ Found 10 security configuration settings\n  High-impact settings: 3\n  Enabled settings: 4\n  Potential risks identified: 2\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>config_key</th><th>config_value</th><th>description</th><th>enabled</th><th>potential_risk</th><th>security_impact</th></tr></thead><tbody><tr><td>enableIpAccessLists</td><td>null</td><td>IP access list restrictions enabled</td><td>false</td><td>true</td><td>High</td></tr><tr><td>enforceUserIsolation</td><td>false</td><td>User isolation enforcement enabled</td><td>false</td><td>true</td><td>High</td></tr><tr><td>enableDbfsFileBrowser</td><td>true</td><td>DBFS file browser enabled</td><td>true</td><td>false</td><td>Medium</td></tr><tr><td>enableDeprecatedClusterNamedInitScripts</td><td>null</td><td>Deprecated cluster init scripts allowed</td><td>false</td><td>false</td><td>Medium</td></tr><tr><td>enableDeprecatedGlobalInitScripts</td><td>null</td><td>Deprecated global init scripts allowed</td><td>false</td><td>false</td><td>Medium</td></tr><tr><td>enableTokensConfig</td><td>true</td><td>Personal access token creation enabled</td><td>true</td><td>false</td><td>Medium</td></tr><tr><td>enableWebTerminal</td><td>true</td><td>Web terminal access enabled</td><td>true</td><td>false</td><td>Medium</td></tr><tr><td>maxTokenLifetimeDays</td><td>null</td><td>Maximum token lifetime (days)</td><td>false</td><td>false</td><td>Medium</td></tr><tr><td>storeInteractiveNotebookResultsInCustomerAccount</td><td>null</td><td>Notebook results stored in customer account</td><td>false</td><td>false</td><td>Medium</td></tr><tr><td>enableVerboseAuditLogs</td><td>true</td><td>Detailed audit logging enabled</td><td>true</td><td>false</td><td>High</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "enableIpAccessLists",
         "null",
         "IP access list restrictions enabled",
         false,
         true,
         "High"
        ],
        [
         "enforceUserIsolation",
         "false",
         "User isolation enforcement enabled",
         false,
         true,
         "High"
        ],
        [
         "enableDbfsFileBrowser",
         "true",
         "DBFS file browser enabled",
         true,
         false,
         "Medium"
        ],
        [
         "enableDeprecatedClusterNamedInitScripts",
         "null",
         "Deprecated cluster init scripts allowed",
         false,
         false,
         "Medium"
        ],
        [
         "enableDeprecatedGlobalInitScripts",
         "null",
         "Deprecated global init scripts allowed",
         false,
         false,
         "Medium"
        ],
        [
         "enableTokensConfig",
         "true",
         "Personal access token creation enabled",
         true,
         false,
         "Medium"
        ],
        [
         "enableWebTerminal",
         "true",
         "Web terminal access enabled",
         true,
         false,
         "Medium"
        ],
        [
         "maxTokenLifetimeDays",
         "null",
         "Maximum token lifetime (days)",
         false,
         false,
         "Medium"
        ],
        [
         "storeInteractiveNotebookResultsInCustomerAccount",
         "null",
         "Notebook results stored in customer account",
         false,
         false,
         "Medium"
        ],
        [
         "enableVerboseAuditLogs",
         "true",
         "Detailed audit logging enabled",
         true,
         false,
         "High"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "config_key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "config_value",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "enabled",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "potential_risk",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "security_impact",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Get workspace security configuration settings completed in 1.81 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "log(\"Fetching workspace security configuration settings...\")\n",
    "\n",
    "security_config_keys = {\n",
    "    'enableTokensConfig': 'Personal access token creation enabled',\n",
    "    'maxTokenLifetimeDays': 'Maximum token lifetime (days)',\n",
    "    'enableIpAccessLists': 'IP access list restrictions enabled',\n",
    "    'enableVerboseAuditLogs': 'Detailed audit logging enabled',\n",
    "    'enforceUserIsolation': 'User isolation enforcement enabled',\n",
    "    'enableDeprecatedClusterNamedInitScripts': 'Deprecated cluster init scripts allowed',\n",
    "    'enableDeprecatedGlobalInitScripts': 'Deprecated global init scripts allowed',\n",
    "    'enableWebTerminal': 'Web terminal access enabled',\n",
    "    'enableDbfsFileBrowser': 'DBFS file browser enabled',\n",
    "    'storeInteractiveNotebookResultsInCustomerAccount': 'Notebook results stored in customer account'\n",
    "}\n",
    "\n",
    "security_config_data = []\n",
    "\n",
    "for key, description in security_config_keys.items():\n",
    "    try:\n",
    "        result = wc.workspace_conf.get_status(keys=key)\n",
    "        if key in result:\n",
    "            value = result[key]\n",
    "            \n",
    "            # Determine security impact\n",
    "            high_impact_keys = ['enableIpAccessLists', 'enforceUserIsolation', 'enableVerboseAuditLogs']\n",
    "            security_impact = 'High' if key in high_impact_keys else 'Medium'\n",
    "            \n",
    "            # Determine if this is a potential security risk\n",
    "            risky_enabled = key in ['enableDeprecatedClusterNamedInitScripts', \n",
    "                                    'enableDeprecatedGlobalInitScripts'] and str(value).lower() == 'true'\n",
    "            risky_disabled = key in ['enableIpAccessLists', 'enforceUserIsolation', \n",
    "                                     'enableVerboseAuditLogs'] and str(value).lower() != 'true'\n",
    "            \n",
    "            security_config_data.append({\n",
    "                'config_key': key,\n",
    "                'config_value': str(value) if value is not None else 'null',\n",
    "                'enabled': str(value).lower() in ['true', '1', 'enabled'],\n",
    "                'description': description,\n",
    "                'security_impact': security_impact,\n",
    "                'potential_risk': risky_enabled or risky_disabled\n",
    "            })\n",
    "    except Exception as e:\n",
    "        if not is_job_mode:\n",
    "            log(f\"  ⚠️  Failed to check {key}: {str(e)[:100]}\")\n",
    "\n",
    "if security_config_data:\n",
    "    security_config_df = spark.createDataFrame(security_config_data)\n",
    "    \n",
    "    log(f\"✓ Found {len(security_config_data)} security configuration settings\")\n",
    "    log(f\"  High-impact settings: {security_config_df.filter(F.col('security_impact') == 'High').count()}\")\n",
    "    log(f\"  Enabled settings: {security_config_df.filter(F.col('enabled') == True).count()}\")\n",
    "    log(f\"  Potential risks identified: {security_config_df.filter(F.col('potential_risk') == True).count()}\")\n",
    "    \n",
    "    # Display ordered by security impact and risk\n",
    "    display(security_config_df.orderBy(\n",
    "        F.col('potential_risk').desc(), \n",
    "        F.col('security_impact').desc(), \n",
    "        F.col('config_key')\n",
    "    ))\n",
    "    \n",
    "    # Add to all_permissions for export\n",
    "    all_permissions.extend(security_config_data)\n",
    "else:\n",
    "    log(\"⚠️  No security configuration settings could be retrieved\")\n",
    "    security_config_df = spark.createDataFrame([], schema=\"config_key STRING, config_value STRING, enabled BOOLEAN, description STRING, security_impact STRING, potential_risk BOOLEAN\")\n",
    "\n",
    "log_execution_time(\"Get workspace security configuration settings\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "005ac125-9b40-45a7-9771-12868ad2da58",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Security configuration analysis and recommendations"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing security configuration and generating recommendations...\n  ✓ security_config_df: 10 rows\n\n⚠️  Found 4 security recommendations:\n  High priority: 3\n  Medium priority: 1\n  Low priority: 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>impact</th><th>issue</th><th>priority</th><th>recommendation</th><th>resource_name</th><th>resource_type</th></tr></thead><tbody><tr><td>Network Security</td><td>Unrestricted network access to workspace</td><td>IP Access Lists not enabled</td><td>High</td><td>Configure IP access lists to restrict workspace access by network</td><td>enableIpAccessLists</td><td>Workspace Config</td></tr><tr><td>Security Configuration</td><td>High security risk</td><td>enableIpAccessLists: null</td><td>High</td><td>Review and remediate: IP access list restrictions enabled</td><td>enableIpAccessLists</td><td>Workspace Config</td></tr><tr><td>Security Configuration</td><td>High security risk</td><td>enforceUserIsolation: false</td><td>High</td><td>Review and remediate: User isolation enforcement enabled</td><td>enforceUserIsolation</td><td>Workspace Config</td></tr><tr><td>Compute Security</td><td>Users may access each other's data on shared clusters</td><td>User isolation not enforced</td><td>Medium</td><td>Enable enforceUserIsolation for enhanced security on shared clusters</td><td>enforceUserIsolation</td><td>Workspace Config</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Network Security",
         "Unrestricted network access to workspace",
         "IP Access Lists not enabled",
         "High",
         "Configure IP access lists to restrict workspace access by network",
         "enableIpAccessLists",
         "Workspace Config"
        ],
        [
         "Security Configuration",
         "High security risk",
         "enableIpAccessLists: null",
         "High",
         "Review and remediate: IP access list restrictions enabled",
         "enableIpAccessLists",
         "Workspace Config"
        ],
        [
         "Security Configuration",
         "High security risk",
         "enforceUserIsolation: false",
         "High",
         "Review and remediate: User isolation enforcement enabled",
         "enforceUserIsolation",
         "Workspace Config"
        ],
        [
         "Compute Security",
         "Users may access each other's data on shared clusters",
         "User isolation not enforced",
         "Medium",
         "Enable enforceUserIsolation for enhanced security on shared clusters",
         "enforceUserIsolation",
         "Workspace Config"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "impact",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "issue",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "priority",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "recommendation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "resource_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "resource_type",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️  Security configuration analysis and recommendations completed in 1.62 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "log(\"Analyzing security configuration and generating recommendations...\")\n",
    "\n",
    "recommendations = []\n",
    "\n",
    "# Check for high-risk configurations\n",
    "if validate_dataframe_exists('security_config_df', security_config_df):\n",
    "    high_risk_configs = security_config_df.filter(\n",
    "        (F.col('potential_risk') == True) & \n",
    "        (F.col('security_impact') == 'High')\n",
    "    ).collect()\n",
    "    \n",
    "    for config in high_risk_configs:\n",
    "        recommendations.append({\n",
    "            'priority': 'High',\n",
    "            'category': 'Security Configuration',\n",
    "            'issue': f\"{config.config_key}: {config.config_value}\",\n",
    "            'recommendation': f\"Review and remediate: {config.description}\",\n",
    "            'impact': 'High security risk',\n",
    "            'resource_type': 'Workspace Config',\n",
    "            'resource_name': config.config_key\n",
    "        })\n",
    "    \n",
    "    # Check for missing IP access lists\n",
    "    ip_config = security_config_df.filter(F.col('config_key') == 'enableIpAccessLists').collect()\n",
    "    if ip_config and not ip_config[0].enabled:\n",
    "        recommendations.append({\n",
    "            'priority': 'High',\n",
    "            'category': 'Network Security',\n",
    "            'issue': 'IP Access Lists not enabled',\n",
    "            'recommendation': 'Configure IP access lists to restrict workspace access by network',\n",
    "            'impact': 'Unrestricted network access to workspace',\n",
    "            'resource_type': 'Workspace Config',\n",
    "            'resource_name': 'enableIpAccessLists'\n",
    "        })\n",
    "    \n",
    "    # Check for user isolation\n",
    "    isolation_config = security_config_df.filter(F.col('config_key') == 'enforceUserIsolation').collect()\n",
    "    if isolation_config and not isolation_config[0].enabled:\n",
    "        recommendations.append({\n",
    "            'priority': 'Medium',\n",
    "            'category': 'Compute Security',\n",
    "            'issue': 'User isolation not enforced',\n",
    "            'recommendation': 'Enable enforceUserIsolation for enhanced security on shared clusters',\n",
    "            'impact': 'Users may access each other\\'s data on shared clusters',\n",
    "            'resource_type': 'Workspace Config',\n",
    "            'resource_name': 'enforceUserIsolation'\n",
    "        })\n",
    "    \n",
    "    # Check for verbose audit logs\n",
    "    audit_config = security_config_df.filter(F.col('config_key') == 'enableVerboseAuditLogs').collect()\n",
    "    if audit_config and not audit_config[0].enabled:\n",
    "        recommendations.append({\n",
    "            'priority': 'High',\n",
    "            'category': 'Compliance',\n",
    "            'issue': 'Verbose audit logs not enabled',\n",
    "            'recommendation': 'Enable verbose audit logs for compliance and security monitoring',\n",
    "            'impact': 'Limited audit trail for compliance reviews',\n",
    "            'resource_type': 'Workspace Config',\n",
    "            'resource_name': 'enableVerboseAuditLogs'\n",
    "        })\n",
    "    \n",
    "    # Check for deprecated features enabled\n",
    "    deprecated_configs = security_config_df.filter(\n",
    "        F.col('config_key').contains('Deprecated') & \n",
    "        (F.col('enabled') == True)\n",
    "    ).collect()\n",
    "    \n",
    "    for config in deprecated_configs:\n",
    "        recommendations.append({\n",
    "            'priority': 'Low',\n",
    "            'category': 'Maintenance',\n",
    "            'issue': f\"{config.config_key} is enabled\",\n",
    "            'recommendation': 'Migrate away from deprecated features before they are removed',\n",
    "            'impact': 'Future compatibility issues',\n",
    "            'resource_type': 'Workspace Config',\n",
    "            'resource_name': config.config_key\n",
    "        })\n",
    "\n",
    "# Create recommendations DataFrame\n",
    "if recommendations:\n",
    "    recommendations_df = spark.createDataFrame(recommendations)\n",
    "    recommendations_df = recommendations_df.orderBy(\n",
    "        F.when(F.col('priority') == 'High', 0)\n",
    "         .when(F.col('priority') == 'Medium', 1)\n",
    "         .otherwise(2),\n",
    "        F.col('category')\n",
    "    )\n",
    "    \n",
    "    log(f\"\\n⚠️  Found {len(recommendations)} security recommendations:\")\n",
    "    log(f\"  High priority: {recommendations_df.filter(F.col('priority') == 'High').count()}\")\n",
    "    log(f\"  Medium priority: {recommendations_df.filter(F.col('priority') == 'Medium').count()}\")\n",
    "    log(f\"  Low priority: {recommendations_df.filter(F.col('priority') == 'Low').count()}\")\n",
    "    \n",
    "    display(recommendations_df)\n",
    "else:\n",
    "    log(\"\\n✓ No security configuration issues identified\")\n",
    "    log(\"   Your workspace security configuration follows best practices\")\n",
    "    recommendations_df = spark.createDataFrame([], schema=\"priority STRING, category STRING, issue STRING, recommendation STRING, impact STRING, resource_type STRING, resource_name STRING\")\n",
    "\n",
    "log_execution_time(\"Security configuration analysis and recommendations\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f50a81f7-f713-42d4-ad83-247c65ce07aa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get job permissions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching job permissions (up to 100)...\n  Found 735 total jobs, checking 100\n  Progress: 20/100 (20.0%)\n  Progress: 40/100 (40.0%)\n  Progress: 60/100 (60.0%)\n  Progress: 80/100 (80.0%)\n  Progress: 100/100 (100.0%)\n✓ Collected 453 total permission entries so far\n⏱️  Get job permissions completed in 5.59 seconds\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_JOBS:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    if MAX_RESOURCES_PER_TYPE == 999:\n",
    "        log(f\"Fetching job permissions (all resources, no limit)...\")\n",
    "    else:\n",
    "        log(f\"Fetching job permissions (up to {MAX_RESOURCES_PER_TYPE})...\")\n",
    "    \n",
    "    try:\n",
    "        jobs_list = list(wc.jobs.list())\n",
    "        \n",
    "        if MAX_RESOURCES_PER_TYPE == 999:\n",
    "            jobs_to_check = jobs_list\n",
    "            log(f\"  Found {len(jobs_list)} total jobs, checking ALL (no limit)\")\n",
    "        else:\n",
    "            jobs_to_check = jobs_list[:MAX_RESOURCES_PER_TYPE]\n",
    "            log(f\"  Found {len(jobs_list)} total jobs, checking {len(jobs_to_check)}\")\n",
    "        \n",
    "        if len(jobs_to_check) > 0:\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(get_permissions, 'jobs', str(job.job_id), \n",
    "                                  job.settings.name if job.settings else '', 'jobs')\n",
    "                    for job in jobs_to_check\n",
    "                ]\n",
    "                \n",
    "                completed = 0\n",
    "                for future in as_completed(futures):\n",
    "                    all_permissions.extend(future.result())\n",
    "                    completed += 1\n",
    "                    if not is_job_mode and completed % 20 == 0:\n",
    "                        progress_pct = (completed / len(jobs_to_check)) * 100\n",
    "                        log(f\"  Progress: {completed}/{len(jobs_to_check)} ({progress_pct:.1f}%)\")\n",
    "            \n",
    "            log(f\"✓ Collected {len(all_permissions)} total permission entries so far\")\n",
    "        else:\n",
    "            log(\"⚠️  No jobs found to check\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching job permissions: {str(e)}\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get job permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Job permissions collection disabled (ENABLE_JOBS=False)\")\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c1d2c98-1357-459d-b194-2e0c758caca1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get warehouse permissions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching warehouse permissions (up to 100)...\n  Found 30 total warehouses, checking 30\n  Progress: 10/30 (33.3%)\n  Progress: 20/30 (66.7%)\n  Progress: 30/30 (100.0%)\n✓ Collected 587 total permission entries so far\n⏱️  Get warehouse permissions completed in 0.37 seconds\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_WAREHOUSES:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    if MAX_RESOURCES_PER_TYPE == 999:\n",
    "        log(f\"Fetching warehouse permissions (all resources, no limit)...\")\n",
    "    else:\n",
    "        log(f\"Fetching warehouse permissions (up to {MAX_RESOURCES_PER_TYPE})...\")\n",
    "    \n",
    "    try:\n",
    "        warehouses_list = list(wc.warehouses.list())\n",
    "        \n",
    "        if MAX_RESOURCES_PER_TYPE == 999:\n",
    "            warehouses_to_check = warehouses_list\n",
    "            log(f\"  Found {len(warehouses_list)} total warehouses, checking ALL (no limit)\")\n",
    "        else:\n",
    "            warehouses_to_check = warehouses_list[:MAX_RESOURCES_PER_TYPE]\n",
    "            log(f\"  Found {len(warehouses_list)} total warehouses, checking {len(warehouses_to_check)}\")\n",
    "        \n",
    "        if len(warehouses_to_check) > 0:\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(get_permissions, 'warehouses', warehouse.id, \n",
    "                                  warehouse.name, 'sql/warehouses')\n",
    "                    for warehouse in warehouses_to_check\n",
    "                ]\n",
    "                \n",
    "                completed = 0\n",
    "                for future in as_completed(futures):\n",
    "                    all_permissions.extend(future.result())\n",
    "                    completed += 1\n",
    "                    if not is_job_mode and completed % 10 == 0:\n",
    "                        progress_pct = (completed / len(warehouses_to_check)) * 100\n",
    "                        log(f\"  Progress: {completed}/{len(warehouses_to_check)} ({progress_pct:.1f}%)\")\n",
    "            \n",
    "            log(f\"✓ Collected {len(all_permissions)} total permission entries so far\")\n",
    "        else:\n",
    "            log(\"⚠️  No warehouses found to check\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching warehouse permissions: {str(e)}\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get warehouse permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Warehouse permissions collection disabled (ENABLE_WAREHOUSES=False)\")\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20e8c949-aa86-4a1e-a655-6092bef64f35",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get cluster permissions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching cluster permissions (interactive clusters only, up to 100)...\n  Found 103220 total clusters\n  Filtered to 449 interactive clusters (excluding job clusters)\n  Checking 100 interactive clusters\n  Progress: 10/100 (10.0%)\n  Progress: 20/100 (20.0%)\n  Progress: 30/100 (30.0%)\n  Progress: 40/100 (40.0%)\n  Progress: 50/100 (50.0%)\n  Progress: 60/100 (60.0%)\n  Progress: 70/100 (70.0%)\n  Progress: 80/100 (80.0%)\n  Progress: 90/100 (90.0%)\n  Progress: 100/100 (100.0%)\n✓ Collected 974 total permission entries so far\n⏱️  Get cluster permissions completed in 2225.00 seconds\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_CLUSTERS:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    if MAX_RESOURCES_PER_TYPE == 999:\n",
    "        log(f\"Fetching cluster permissions (interactive clusters only, all resources, no limit)...\")\n",
    "    else:\n",
    "        log(f\"Fetching cluster permissions (interactive clusters only, up to {MAX_RESOURCES_PER_TYPE})...\")\n",
    "    \n",
    "    try:\n",
    "        all_clusters = list(wc.clusters.list())\n",
    "        \n",
    "        interactive_clusters = [\n",
    "            cluster for cluster in all_clusters \n",
    "            if cluster.cluster_source and cluster.cluster_source.value != 'JOB'\n",
    "        ]\n",
    "        \n",
    "        log(f\"  Found {len(all_clusters)} total clusters\")\n",
    "        log(f\"  Filtered to {len(interactive_clusters)} interactive clusters (excluding job clusters)\")\n",
    "        \n",
    "        if MAX_RESOURCES_PER_TYPE == 999:\n",
    "            clusters_to_check = interactive_clusters\n",
    "            log(f\"  Checking ALL interactive clusters (no limit)\")\n",
    "        else:\n",
    "            clusters_to_check = interactive_clusters[:MAX_RESOURCES_PER_TYPE]\n",
    "            log(f\"  Checking {len(clusters_to_check)} interactive clusters\")\n",
    "        \n",
    "        if len(clusters_to_check) > 0:\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(get_permissions, 'clusters', cluster.cluster_id, \n",
    "                                  cluster.cluster_name, 'clusters')\n",
    "                    for cluster in clusters_to_check\n",
    "                ]\n",
    "                \n",
    "                completed = 0\n",
    "                for future in as_completed(futures):\n",
    "                    all_permissions.extend(future.result())\n",
    "                    completed += 1\n",
    "                    if not is_job_mode and completed % 10 == 0:\n",
    "                        progress_pct = (completed / len(clusters_to_check)) * 100\n",
    "                        log(f\"  Progress: {completed}/{len(clusters_to_check)} ({progress_pct:.1f}%)\")\n",
    "            \n",
    "            log(f\"✓ Collected {len(all_permissions)} total permission entries so far\")\n",
    "        else:\n",
    "            log(\"⚠️  No interactive clusters found to check\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching cluster permissions: {str(e)}\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get cluster permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Cluster permissions collection disabled (ENABLE_CLUSTERS=False)\")\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "beaf6335-6cf0-45a7-8f35-c7cbba51c9bf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get pipeline permissions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching pipeline permissions (up to 100)...\n  Found 8 total pipelines, checking 8\n✓ Collected 1002 total permission entries so far\n⏱️  Get pipeline permissions completed in 0.23 seconds\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_PIPELINES:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    if MAX_RESOURCES_PER_TYPE == 999:\n",
    "        log(f\"Fetching pipeline permissions (all resources, no limit)...\")\n",
    "    else:\n",
    "        log(f\"Fetching pipeline permissions (up to {MAX_RESOURCES_PER_TYPE})...\")\n",
    "    \n",
    "    try:\n",
    "        pipelines_list = list(wc.pipelines.list_pipelines())\n",
    "        \n",
    "        if MAX_RESOURCES_PER_TYPE == 999:\n",
    "            pipelines_to_check = pipelines_list\n",
    "            log(f\"  Found {len(pipelines_list)} total pipelines, checking ALL (no limit)\")\n",
    "        else:\n",
    "            pipelines_to_check = pipelines_list[:MAX_RESOURCES_PER_TYPE]\n",
    "            log(f\"  Found {len(pipelines_list)} total pipelines, checking {len(pipelines_to_check)}\")\n",
    "        \n",
    "        if len(pipelines_to_check) > 0:\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(get_permissions, 'pipelines', pipeline.pipeline_id, \n",
    "                                  pipeline.name, 'pipelines')\n",
    "                    for pipeline in pipelines_to_check\n",
    "                ]\n",
    "                \n",
    "                completed = 0\n",
    "                for future in as_completed(futures):\n",
    "                    all_permissions.extend(future.result())\n",
    "                    completed += 1\n",
    "                    if not is_job_mode and completed % 10 == 0:\n",
    "                        progress_pct = (completed / len(pipelines_to_check)) * 100\n",
    "                        log(f\"  Progress: {completed}/{len(pipelines_to_check)} ({progress_pct:.1f}%)\")\n",
    "            \n",
    "            log(f\"✓ Collected {len(all_permissions)} total permission entries so far\")\n",
    "        else:\n",
    "            log(\"⚠️  No pipelines found to check\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching pipeline permissions: {str(e)}\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get pipeline permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Pipeline permissions collection disabled (ENABLE_PIPELINES=False)\")\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556204d6-96b6-46a1-b081-f70e70f59425",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get workspace folder and notebook permissions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching workspace folder and notebook permissions...\n  Limit: 500 objects\n  Scanning /Shared...\n  ⚠️ Reached MAX_WORKSPACE_OBJECTS limit (500)\n  Collected 500 workspace objects to check permissions\n  Progress: 50/500 (10.0%)\n  Progress: 100/500 (20.0%)\n  Progress: 150/500 (30.0%)\n  Progress: 200/500 (40.0%)\n  Progress: 250/500 (50.0%)\n  Progress: 300/500 (60.0%)\n  Progress: 350/500 (70.0%)\n  Progress: 400/500 (80.0%)\n  Progress: 450/500 (90.0%)\n  Progress: 500/500 (100.0%)\n✓ Found 299 workspace object permissions\n✓ Collected 1301 total permission entries so far\n⏱️  Get workspace folder and notebook permissions completed in 8.51 seconds\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_WORKSPACE_OBJECTS:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching workspace folder and notebook permissions...\")\n",
    "    log(f\"  Limit: {MAX_WORKSPACE_OBJECTS} objects\")\n",
    "    \n",
    "    workspace_perms_data = []\n",
    "    objects_scanned = 0\n",
    "    \n",
    "    try:\n",
    "        # Define key folders to check (customize as needed)\n",
    "        key_folders = [\n",
    "            '/Shared',\n",
    "            '/Users',\n",
    "            '/Repos'\n",
    "        ]\n",
    "        \n",
    "        def collect_workspace_objects(path, current_count, depth=0, max_depth=2):\n",
    "            \"\"\"Collect workspace objects up to max_depth\"\"\"\n",
    "            if depth > max_depth or current_count >= MAX_WORKSPACE_OBJECTS:\n",
    "                return [], current_count\n",
    "            \n",
    "            objects_to_check = []\n",
    "            try:\n",
    "                objects = list(wc.workspace.list(path))\n",
    "                \n",
    "                for obj in objects:\n",
    "                    if current_count >= MAX_WORKSPACE_OBJECTS:\n",
    "                        break\n",
    "                    \n",
    "                    objects_to_check.append(obj)\n",
    "                    current_count += 1\n",
    "                    \n",
    "                    # Recurse into directories\n",
    "                    if obj.object_type and obj.object_type.value == 'DIRECTORY' and depth < max_depth:\n",
    "                        sub_objects, current_count = collect_workspace_objects(obj.path, current_count, depth + 1, max_depth)\n",
    "                        objects_to_check.extend(sub_objects)\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            return objects_to_check, current_count\n",
    "        \n",
    "        # Collect objects from key folders\n",
    "        all_objects = []\n",
    "        for folder in key_folders:\n",
    "            try:\n",
    "                log(f\"  Scanning {folder}...\")\n",
    "                folder_objects, objects_scanned = collect_workspace_objects(folder, objects_scanned, depth=0, max_depth=2)\n",
    "                all_objects.extend(folder_objects)\n",
    "                if objects_scanned >= MAX_WORKSPACE_OBJECTS:\n",
    "                    log(f\"  ⚠️ Reached MAX_WORKSPACE_OBJECTS limit ({MAX_WORKSPACE_OBJECTS})\")\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                log(f\"  ⚠️ Could not scan {folder}: {str(e)}\")\n",
    "        \n",
    "        log(f\"  Collected {len(all_objects)} workspace objects to check permissions\")\n",
    "        \n",
    "        # Parallelize permission fetching for collected objects\n",
    "        if len(all_objects) > 0:\n",
    "            def get_workspace_object_permissions(obj):\n",
    "                \"\"\"Get permissions for a single workspace object\"\"\"\n",
    "                try:\n",
    "                    perms = wc.permissions.get('directories', obj.object_id)\n",
    "                    results = []\n",
    "                    \n",
    "                    if perms.access_control_list:\n",
    "                        for acl in perms.access_control_list:\n",
    "                            principal = acl.user_name or acl.group_name or acl.service_principal_name\n",
    "                            principal_type = 'user' if acl.user_name else ('group' if acl.group_name else 'service_principal')\n",
    "                            permissions = [p.permission_level.value for p in acl.all_permissions] if acl.all_permissions else []\n",
    "                            \n",
    "                            for perm in permissions:\n",
    "                                results.append({\n",
    "                                    'resource_type': 'workspace_object',\n",
    "                                    'object_type': obj.object_type.value if obj.object_type else 'UNKNOWN',\n",
    "                                    'resource_id': str(obj.object_id),\n",
    "                                    'resource_path': obj.path,\n",
    "                                    'principal': principal,\n",
    "                                    'principal_type': principal_type,\n",
    "                                    'permission_level': perm\n",
    "                                })\n",
    "                    return results\n",
    "                except Exception:\n",
    "                    return []\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                futures = [executor.submit(get_workspace_object_permissions, obj) for obj in all_objects]\n",
    "                \n",
    "                completed = 0\n",
    "                for future in as_completed(futures):\n",
    "                    results = future.result()\n",
    "                    workspace_perms_data.extend(results)\n",
    "                    completed += 1\n",
    "                    if not is_job_mode and completed % 50 == 0:\n",
    "                        progress_pct = (completed / len(all_objects)) * 100\n",
    "                        log(f\"  Progress: {completed}/{len(all_objects)} ({progress_pct:.1f}%)\")\n",
    "            \n",
    "            # Add to all_permissions\n",
    "            all_permissions.extend([{\n",
    "                'resource_type': p['resource_type'],\n",
    "                'resource_id': p['resource_id'],\n",
    "                'resource_name': p['resource_path'],\n",
    "                'principal': p['principal'],\n",
    "                'principal_type': p['principal_type'],\n",
    "                'permission_level': p['permission_level']\n",
    "            } for p in workspace_perms_data])\n",
    "        \n",
    "        if workspace_perms_data:\n",
    "            workspace_permissions_df = spark.createDataFrame(workspace_perms_data)\n",
    "        else:\n",
    "            workspace_permissions_df = spark.createDataFrame([], 'resource_type STRING, object_type STRING, resource_id STRING, resource_path STRING, principal STRING, principal_type STRING, permission_level STRING')\n",
    "        \n",
    "        log(f\"✓ Found {len(workspace_perms_data)} workspace object permissions\")\n",
    "        log(f\"✓ Collected {len(all_permissions)} total permission entries so far\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching workspace permissions: {str(e)}\")\n",
    "        workspace_permissions_df = spark.createDataFrame([], 'resource_type STRING, object_type STRING, resource_id STRING, resource_path STRING, principal STRING, principal_type STRING, permission_level STRING')\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get workspace folder and notebook permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Workspace object permissions collection disabled (ENABLE_WORKSPACE_OBJECTS=False)\")\n",
    "    workspace_permissions_df = spark.createDataFrame([], 'resource_type STRING, object_type STRING, resource_id STRING, resource_path STRING, principal STRING, principal_type STRING, permission_level STRING')\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc2498f-b611-4f2c-9be2-8a51c9089418",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get model registry permissions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching model registry permissions...\n  Found 37 workspace model registry models (legacy)\n  Note: Workspace model registry does not support permissions API\n  Workspace models are managed through workspace-level access controls\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.12/site-packages/databricks/sdk/errors/base.py:87: UserWarning: The 'retry_after_secs' parameter of DatabricksError is deprecated and will be removed in a future version.\n  warnings.warn(\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Found 1677 Unity Catalog models, checking 100\n  Progress: 50/100 (50.0%)\n  Progress: 100/100 (100.0%)\n  No Unity Catalog model grants found\n✓ Collected 1301 total permission entries so far\n⏱️  Get model registry permissions completed in 30.33 seconds\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_MODEL_REGISTRY:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching model registry permissions...\")\n",
    "    \n",
    "    try:\n",
    "        # Note: Workspace Model Registry (legacy) does not support permissions API\n",
    "        # Unity Catalog models use grants API instead (covered in UC permissions cell)\n",
    "        \n",
    "        # Check for workspace models\n",
    "        workspace_models = list(wc.model_registry.list_models())\n",
    "        log(f\"  Found {len(workspace_models)} workspace model registry models (legacy)\")\n",
    "        log(f\"  Note: Workspace model registry does not support permissions API\")\n",
    "        log(f\"  Workspace models are managed through workspace-level access controls\")\n",
    "        \n",
    "        # Get Unity Catalog models and their grants\n",
    "        try:\n",
    "            uc_models = list(wc.registered_models.list())\n",
    "            \n",
    "            if MAX_RESOURCES_PER_TYPE == 999:\n",
    "                models_to_check = uc_models\n",
    "                log(f\"  Found {len(uc_models)} Unity Catalog models, checking ALL (no limit)\")\n",
    "            else:\n",
    "                models_to_check = uc_models[:MAX_RESOURCES_PER_TYPE]\n",
    "                log(f\"  Found {len(uc_models)} Unity Catalog models, checking {len(models_to_check)}\")\n",
    "            \n",
    "            uc_model_grants = []\n",
    "            \n",
    "            if len(models_to_check) > 0:\n",
    "                completed = 0\n",
    "                for model in models_to_check:\n",
    "                    try:\n",
    "                        # Get grants for UC model\n",
    "                        grants = wc.grants.get_effective(securable_type='function', full_name=model.full_name)\n",
    "                        \n",
    "                        if grants.privilege_assignments:\n",
    "                            for grant in grants.privilege_assignments:\n",
    "                                for privilege in grant.privileges:\n",
    "                                    uc_model_grants.append({\n",
    "                                        'model_full_name': model.full_name,\n",
    "                                        'model_name': model.name,\n",
    "                                        'catalog_name': model.catalog_name,\n",
    "                                        'schema_name': model.schema_name,\n",
    "                                        'principal': grant.principal,\n",
    "                                        'privilege': privilege.value\n",
    "                                    })\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    \n",
    "                    completed += 1\n",
    "                    if not is_job_mode and completed % 50 == 0:\n",
    "                        progress_pct = (completed / len(models_to_check)) * 100\n",
    "                        log(f\"  Progress: {completed}/{len(models_to_check)} ({progress_pct:.1f}%)\")\n",
    "                \n",
    "                if uc_model_grants:\n",
    "                    uc_model_grants_df = spark.createDataFrame(uc_model_grants)\n",
    "                    log(f\"  ✓ Found {uc_model_grants_df.count()} Unity Catalog model grants\")\n",
    "                else:\n",
    "                    uc_model_grants_df = spark.createDataFrame([], 'model_full_name STRING, model_name STRING, catalog_name STRING, schema_name STRING, principal STRING, privilege STRING')\n",
    "                    log(\"  No Unity Catalog model grants found\")\n",
    "            else:\n",
    "                uc_model_grants_df = spark.createDataFrame([], 'model_full_name STRING, model_name STRING, catalog_name STRING, schema_name STRING, principal STRING, privilege STRING')\n",
    "            \n",
    "            log(f\"✓ Collected {len(all_permissions)} total permission entries so far\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            log(f\"  ⚠️ Could not fetch Unity Catalog models: {str(e)}\")\n",
    "            uc_model_grants_df = spark.createDataFrame([], 'model_full_name STRING, model_name STRING, catalog_name STRING, schema_name STRING, principal STRING, privilege STRING')\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching model registry permissions: {str(e)}\")\n",
    "        uc_model_grants_df = spark.createDataFrame([], 'model_full_name STRING, model_name STRING, catalog_name STRING, schema_name STRING, principal STRING, privilege STRING')\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get model registry permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Model registry permissions collection disabled (ENABLE_MODEL_REGISTRY=False)\")\n",
    "    uc_model_grants_df = spark.createDataFrame([], 'model_full_name STRING, model_name STRING, catalog_name STRING, schema_name STRING, principal STRING, privilege STRING')\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c16f9193-d785-4700-8d5b-fe7796d7febf",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get repos (Git integration) permissions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching repos (Git integration) permissions...\n  Found 155 repos, checking 100\n  Progress: 10/100 (10.0%)\n  Progress: 20/100 (20.0%)\n  Progress: 30/100 (30.0%)\n  Progress: 40/100 (40.0%)\n  Progress: 50/100 (50.0%)\n  Progress: 60/100 (60.0%)\n  Progress: 70/100 (70.0%)\n  Progress: 80/100 (80.0%)\n  Progress: 90/100 (90.0%)\n  Progress: 100/100 (100.0%)\n✓ Collected 1572 total permission entries so far\n⏱️  Get repos permissions completed in 1.40 seconds\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_REPOS:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching repos (Git integration) permissions...\")\n",
    "    \n",
    "    try:\n",
    "        repos_list = list(wc.repos.list())\n",
    "        \n",
    "        if MAX_RESOURCES_PER_TYPE == 999:\n",
    "            repos_to_check = repos_list\n",
    "            log(f\"  Found {len(repos_list)} repos, checking ALL (no limit)\")\n",
    "        else:\n",
    "            repos_to_check = repos_list[:MAX_RESOURCES_PER_TYPE]\n",
    "            log(f\"  Found {len(repos_list)} repos, checking {len(repos_to_check)}\")\n",
    "        \n",
    "        if len(repos_to_check) > 0:\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(get_permissions, 'repos', str(repo.id), \n",
    "                                  repo.path, 'repos')\n",
    "                    for repo in repos_to_check\n",
    "                ]\n",
    "                \n",
    "                completed = 0\n",
    "                for future in as_completed(futures):\n",
    "                    all_permissions.extend(future.result())\n",
    "                    completed += 1\n",
    "                    if not is_job_mode and completed % 10 == 0:\n",
    "                        progress_pct = (completed / len(repos_to_check)) * 100\n",
    "                        log(f\"  Progress: {completed}/{len(repos_to_check)} ({progress_pct:.1f}%)\")\n",
    "            \n",
    "            log(f\"✓ Collected {len(all_permissions)} total permission entries so far\")\n",
    "        else:\n",
    "            log(\"⚠️ No repos found to check\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching repos permissions: {str(e)}\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get repos permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Repos permissions collection disabled (ENABLE_REPOS=False)\")\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3f6be6-b48c-4a1c-9078-6f8f85cfe44e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get instance pool permissions"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching instance pool permissions...\n  Found 4 instance pools, checking 4\n✓ Collected 1582 total permission entries so far\n⏱️  Get instance pool permissions completed in 0.19 seconds\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_INSTANCE_POOLS:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching instance pool permissions...\")\n",
    "    \n",
    "    try:\n",
    "        pools_list = list(wc.instance_pools.list())\n",
    "        \n",
    "        if MAX_RESOURCES_PER_TYPE == 999:\n",
    "            pools_to_check = pools_list\n",
    "            log(f\"  Found {len(pools_list)} instance pools, checking ALL (no limit)\")\n",
    "        else:\n",
    "            pools_to_check = pools_list[:MAX_RESOURCES_PER_TYPE]\n",
    "            log(f\"  Found {len(pools_list)} instance pools, checking {len(pools_to_check)}\")\n",
    "        \n",
    "        if len(pools_to_check) > 0:\n",
    "            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                futures = [\n",
    "                    executor.submit(get_permissions, 'instance_pools', pool.instance_pool_id, \n",
    "                                  pool.instance_pool_name, 'instance-pools')\n",
    "                    for pool in pools_to_check\n",
    "                ]\n",
    "                \n",
    "                completed = 0\n",
    "                for future in as_completed(futures):\n",
    "                    all_permissions.extend(future.result())\n",
    "                    completed += 1\n",
    "                    if not is_job_mode and completed % 5 == 0:\n",
    "                        progress_pct = (completed / len(pools_to_check)) * 100\n",
    "                        log(f\"  Progress: {completed}/{len(pools_to_check)} ({progress_pct:.1f}%)\")\n",
    "            \n",
    "            log(f\"✓ Collected {len(all_permissions)} total permission entries so far\")\n",
    "        else:\n",
    "            log(\"⚠️ No instance pools found to check\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching instance pool permissions: {str(e)}\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get instance pool permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Instance pool permissions collection disabled (ENABLE_INSTANCE_POOLS=False)\")\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9df233c6-948c-437f-ae25-037c485f3255",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get token management audit data"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching token management audit data...\n  No token data available (may require admin permissions)\n⏱️  Get token management audit data completed in 0.14 seconds\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_TOKEN_AUDIT:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching token management audit data...\")\n",
    "    \n",
    "    try:\n",
    "        tokens_data = []\n",
    "        \n",
    "        try:\n",
    "            tokens = list(wc.tokens.list())\n",
    "            \n",
    "            for token in tokens:\n",
    "                tokens_data.append({\n",
    "                    'token_id': token.token_id,\n",
    "                    'created_by_username': token.created_by_username,\n",
    "                    'created_by_id': token.created_by_id,\n",
    "                    'creation_time': token.creation_time,\n",
    "                    'expiry_time': token.expiry_time,\n",
    "                    'comment': token.comment\n",
    "                })\n",
    "        except Exception as e:\n",
    "            log(f\"  ⚠️ Could not fetch tokens (may require additional permissions): {str(e)}\")\n",
    "        \n",
    "        if tokens_data:\n",
    "            tokens_df = spark.createDataFrame(tokens_data)\n",
    "            log(f\"✓ Found {tokens_df.count()} active tokens\")\n",
    "            \n",
    "            if tokens_df.count() > 0:\n",
    "                no_expiry = tokens_df.filter(F.col('expiry_time').isNull()).count()\n",
    "                if no_expiry > 0:\n",
    "                    log(f\"  ⚠️ Security Alert: {no_expiry} tokens have no expiration date\")\n",
    "        else:\n",
    "            tokens_df = spark.createDataFrame([], 'token_id STRING, created_by_username STRING, created_by_id STRING, creation_time BIGINT, expiry_time BIGINT, comment STRING')\n",
    "            log(\"  No token data available (may require admin permissions)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching token data: {str(e)}\")\n",
    "        tokens_df = spark.createDataFrame([], 'token_id STRING, created_by_username STRING, created_by_id STRING, creation_time BIGINT, expiry_time BIGINT, comment STRING')\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get token management audit data\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Token audit disabled (ENABLE_TOKEN_AUDIT=False)\")\n",
    "    tokens_df = spark.createDataFrame([], 'token_id STRING, created_by_username STRING, created_by_id STRING, creation_time BIGINT, expiry_time BIGINT, comment STRING')\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04b5ed0-a68d-46e2-ac18-d50b803558ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Token expiration audit and security analysis"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️  Token expiration audit skipped (no token data available)\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_TOKEN_AUDIT and 'tokens_df' in dir() and tokens_df.count() > 0:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"TOKEN EXPIRATION AUDIT\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    # Identify tokens without expiration dates (CRITICAL SECURITY RISK)\n",
    "    tokens_no_expiry = tokens_df.filter(\n",
    "        (F.col('expiry_time').isNull()) | (F.col('expiry_time') == 0)\n",
    "    )\n",
    "    \n",
    "    log(f\"\\n⚠️  CRITICAL: Tokens without expiration dates\")\n",
    "    log(f\"  Found {tokens_no_expiry.count()} tokens with no expiry\")\n",
    "    \n",
    "    if tokens_no_expiry.count() > 0:\n",
    "        log(\"\\n  Tokens without expiry (SECURITY RISK):\")\n",
    "        tokens_no_expiry_summary = tokens_no_expiry.groupBy('created_by_username', 'comment') \\\n",
    "            .agg(F.count('*').alias('token_count')) \\\n",
    "            .orderBy(F.desc('token_count'))\n",
    "        \n",
    "        if not is_job_mode:\n",
    "            display(tokens_no_expiry_summary)\n",
    "        \n",
    "        log(f\"\\n  ⚠️  RECOMMENDATION: All tokens should have expiration dates\")\n",
    "        log(f\"     Set maxTokenLifetimeDays in workspace settings to enforce expiration\")\n",
    "    else:\n",
    "        log(\"  ✓ All tokens have expiration dates\")\n",
    "    \n",
    "    # Identify tokens expiring soon (within 30 days)\n",
    "    from pyspark.sql.functions import current_timestamp, col\n",
    "    \n",
    "    tokens_expiring_soon = tokens_df.filter(\n",
    "        (F.col('expiry_time').isNotNull()) & \n",
    "        (F.col('expiry_time') > 0) &\n",
    "        (F.col('expiry_time') < (F.unix_timestamp(current_timestamp()) + (30 * 24 * 60 * 60)) * 1000)\n",
    "    )\n",
    "    \n",
    "    log(f\"\\nℹ️  Tokens expiring within 30 days: {tokens_expiring_soon.count()}\")\n",
    "    \n",
    "    if tokens_expiring_soon.count() > 0 and not is_job_mode:\n",
    "        log(\"\\n  Tokens expiring soon:\")\n",
    "        display(tokens_expiring_soon.select(\n",
    "            'created_by_username', \n",
    "            'comment',\n",
    "            F.from_unixtime(F.col('expiry_time') / 1000).alias('expiry_date')\n",
    "        ).orderBy('expiry_time'))\n",
    "    \n",
    "    # Token age analysis\n",
    "    tokens_with_age = tokens_df.withColumn(\n",
    "        'age_days',\n",
    "        (F.unix_timestamp(current_timestamp()) - (F.col('creation_time') / 1000)) / (24 * 60 * 60)\n",
    "    )\n",
    "    \n",
    "    old_tokens = tokens_with_age.filter(F.col('age_days') > 365)\n",
    "    log(f\"\\nℹ️  Tokens older than 1 year: {old_tokens.count()}\")\n",
    "    \n",
    "    if old_tokens.count() > 0:\n",
    "        log(\"  ⚠️  RECOMMENDATION: Review and rotate old tokens regularly\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    log(f\"\\n=== TOKEN SUMMARY ===\")\n",
    "    log(f\"Total tokens: {tokens_df.count()}\")\n",
    "    log(f\"Tokens without expiry: {tokens_no_expiry.count()} (❌ CRITICAL)\")\n",
    "    log(f\"Tokens expiring soon (30 days): {tokens_expiring_soon.count()}\")\n",
    "    log(f\"Tokens older than 1 year: {old_tokens.count()}\")\n",
    "    \n",
    "    log_execution_time(\"Token expiration audit\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Token expiration audit skipped (no token data available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f16ad17-0775-4e4d-a24f-4ebba7f143c9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inactive user permissions analysis"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n============================================================\nINACTIVE USER PERMISSIONS ANALYSIS\n============================================================\n\n⏭️  Skipping inactive user analysis - permissions data not yet available\n   This analysis will run after permissions are collected (Cell 23+)\n⏱️  Inactive user permissions analysis completed in 0.04 seconds\n"
     ]
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"INACTIVE USER PERMISSIONS ANALYSIS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Check if required DataFrames exist\n",
    "if 'users_df' not in dir() or 'permissions_df' not in dir():\n",
    "    log(\"\\n⏭️  Skipping inactive user analysis - permissions data not yet available\")\n",
    "    log(\"   This analysis will run after permissions are collected (Cell 23+)\")\n",
    "    inactive_user_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, principal_type STRING, permission_level STRING')\n",
    "    log_execution_time(\"Inactive user permissions analysis\", cell_start_time)\n",
    "else:\n",
    "    # Identify inactive users with active permissions\n",
    "    if validate_dataframe_exists('users_df', users_df) and validate_dataframe_exists('permissions_df', permissions_df):\n",
    "        \n",
    "        inactive_users = users_df.filter(F.col('active') == False)\n",
    "        \n",
    "        log(f\"\\nInactive users in workspace: {inactive_users.count()}\")\n",
    "        \n",
    "        if inactive_users.count() > 0:\n",
    "            # Find permissions for inactive users\n",
    "            inactive_user_permissions = permissions_df.filter(\n",
    "                F.col('principal_type') == 'user'\n",
    "            ).join(\n",
    "                inactive_users.select('user_name'),\n",
    "                permissions_df.principal == inactive_users.user_name,\n",
    "                'inner'\n",
    "            )\n",
    "            \n",
    "            inactive_perm_count = inactive_user_permissions.count()\n",
    "            \n",
    "            log(f\"\\n⚠️  COMPLIANCE ISSUE: Inactive users with permissions\")\n",
    "            log(f\"  Found {inactive_perm_count} permission entries for inactive users\")\n",
    "            \n",
    "            if inactive_perm_count > 0:\n",
    "                # Summary by user\n",
    "                inactive_summary = inactive_user_permissions.groupBy('principal') \\\n",
    "                    .agg(\n",
    "                        F.count('*').alias('permission_count'),\n",
    "                        F.countDistinct('resource_type').alias('resource_types'),\n",
    "                        F.collect_set('resource_type').alias('resource_type_list')\n",
    "                    ) \\\n",
    "                    .orderBy(F.desc('permission_count'))\n",
    "                \n",
    "                log(f\"\\n  Inactive users with permissions: {inactive_summary.count()}\")\n",
    "                \n",
    "                if not is_job_mode:\n",
    "                    display(inactive_summary)\n",
    "                \n",
    "                # Detailed breakdown by resource type\n",
    "                inactive_by_resource = inactive_user_permissions.groupBy('resource_type') \\\n",
    "                    .agg(\n",
    "                        F.countDistinct('principal').alias('inactive_users'),\n",
    "                        F.count('*').alias('permission_entries')\n",
    "                    ) \\\n",
    "                    .orderBy(F.desc('permission_entries'))\n",
    "                \n",
    "                log(f\"\\n  Permissions by resource type:\")\n",
    "                if not is_job_mode:\n",
    "                    display(inactive_by_resource)\n",
    "                \n",
    "                log(f\"\\n  ⚠️  RECOMMENDATION: Remove permissions for inactive users\")\n",
    "                log(f\"     Inactive users should not retain access to workspace resources\")\n",
    "                log(f\"     This is a common compliance requirement (SOX, GDPR, etc.)\")\n",
    "                \n",
    "                # Store for export\n",
    "                inactive_user_permissions_df = inactive_user_permissions\n",
    "            else:\n",
    "                log(\"  ✓ No permissions found for inactive users\")\n",
    "                inactive_user_permissions_df = spark.createDataFrame([], permissions_df.schema)\n",
    "        else:\n",
    "            log(\"\\n✓ No inactive users in workspace\")\n",
    "            inactive_user_permissions_df = spark.createDataFrame([], permissions_df.schema)\n",
    "    else:\n",
    "        log(\"⚠️  Cannot analyze inactive users - missing required data\")\n",
    "        inactive_user_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, principal_type STRING, permission_level STRING')\n",
    "    \n",
    "    log_execution_time(\"Inactive user permissions analysis\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44b66f7f-b167-4ce6-8552-7ebdba96aa18",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get SQL dashboard and query permissions"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if ENABLE_SQL_ASSETS:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching SQL dashboard and query permissions...\")\n",
    "    \n",
    "    sql_dashboard_perms = []\n",
    "    sql_query_perms = []\n",
    "    \n",
    "    try:\n",
    "        # Get SQL dashboards with parallelization\n",
    "        try:\n",
    "            from databricks.sdk.service.sql import DashboardsAPI\n",
    "            \n",
    "            dashboards = list(wc.dashboards.list())\n",
    "            \n",
    "            if MAX_RESOURCES_PER_TYPE == 999:\n",
    "                dashboards_to_check = dashboards\n",
    "                log(f\"  Found {len(dashboards)} SQL dashboards, checking ALL (no limit)\")\n",
    "            else:\n",
    "                dashboards_to_check = dashboards[:MAX_RESOURCES_PER_TYPE]\n",
    "                log(f\"  Found {len(dashboards)} SQL dashboards, checking {len(dashboards_to_check)}\")\n",
    "            \n",
    "            if len(dashboards_to_check) > 0:\n",
    "                def get_dashboard_permissions(dashboard):\n",
    "                    \"\"\"Get permissions for a single dashboard\"\"\"\n",
    "                    results = []\n",
    "                    try:\n",
    "                        perms = wc.permissions.get('dashboards', dashboard.id)\n",
    "                        \n",
    "                        if perms.access_control_list:\n",
    "                            for acl in perms.access_control_list:\n",
    "                                principal = acl.user_name or acl.group_name or acl.service_principal_name\n",
    "                                principal_type = 'user' if acl.user_name else ('group' if acl.group_name else 'service_principal')\n",
    "                                permissions = [p.permission_level.value for p in acl.all_permissions] if acl.all_permissions else []\n",
    "                                \n",
    "                                for perm in permissions:\n",
    "                                    results.append({\n",
    "                                        'resource_type': 'sql_dashboard',\n",
    "                                        'resource_id': dashboard.id,\n",
    "                                        'resource_name': dashboard.name,\n",
    "                                        'principal': principal,\n",
    "                                        'principal_type': principal_type,\n",
    "                                        'permission_level': perm\n",
    "                                    })\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    return results\n",
    "                \n",
    "                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                    futures = [executor.submit(get_dashboard_permissions, dash) for dash in dashboards_to_check]\n",
    "                    \n",
    "                    completed = 0\n",
    "                    for future in as_completed(futures):\n",
    "                        sql_dashboard_perms.extend(future.result())\n",
    "                        completed += 1\n",
    "                        if not is_job_mode and completed % 10 == 0:\n",
    "                            progress_pct = (completed / len(dashboards_to_check)) * 100\n",
    "                            log(f\"  Dashboard progress: {completed}/{len(dashboards_to_check)} ({progress_pct:.1f}%)\")\n",
    "                \n",
    "                log(f\"  ✓ Collected {len(sql_dashboard_perms)} SQL dashboard permissions\")\n",
    "        except Exception as e:\n",
    "            log(f\"  ⚠️ Could not fetch SQL dashboards: {str(e)}\")\n",
    "        \n",
    "        # Get SQL queries with parallelization\n",
    "        try:\n",
    "            queries = list(wc.queries.list())\n",
    "            \n",
    "            if MAX_RESOURCES_PER_TYPE == 999:\n",
    "                queries_to_check = queries\n",
    "                log(f\"  Found {len(queries)} SQL queries, checking ALL (no limit)\")\n",
    "            else:\n",
    "                queries_to_check = queries[:MAX_RESOURCES_PER_TYPE]\n",
    "                log(f\"  Found {len(queries)} SQL queries, checking {len(queries_to_check)}\")\n",
    "            \n",
    "            if len(queries_to_check) > 0:\n",
    "                def get_query_permissions(query):\n",
    "                    \"\"\"Get permissions for a single query\"\"\"\n",
    "                    results = []\n",
    "                    try:\n",
    "                        perms = wc.permissions.get('queries', query.id)\n",
    "                        \n",
    "                        if perms.access_control_list:\n",
    "                            for acl in perms.access_control_list:\n",
    "                                principal = acl.user_name or acl.group_name or acl.service_principal_name\n",
    "                                principal_type = 'user' if acl.user_name else ('group' if acl.group_name else 'service_principal')\n",
    "                                permissions = [p.permission_level.value for p in acl.all_permissions] if acl.all_permissions else []\n",
    "                                \n",
    "                                for perm in permissions:\n",
    "                                    results.append({\n",
    "                                        'resource_type': 'sql_query',\n",
    "                                        'resource_id': query.id,\n",
    "                                        'resource_name': query.name,\n",
    "                                        'principal': principal,\n",
    "                                        'principal_type': principal_type,\n",
    "                                        'permission_level': perm\n",
    "                                    })\n",
    "                    except Exception:\n",
    "                        pass\n",
    "                    return results\n",
    "                \n",
    "                with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "                    futures = [executor.submit(get_query_permissions, query) for query in queries_to_check]\n",
    "                    \n",
    "                    completed = 0\n",
    "                    for future in as_completed(futures):\n",
    "                        sql_query_perms.extend(future.result())\n",
    "                        completed += 1\n",
    "                        if not is_job_mode and completed % 20 == 0:\n",
    "                            progress_pct = (completed / len(queries_to_check)) * 100\n",
    "                            log(f\"  Query progress: {completed}/{len(queries_to_check)} ({progress_pct:.1f}%)\")\n",
    "                \n",
    "                log(f\"  ✓ Collected {len(sql_query_perms)} SQL query permissions\")\n",
    "        except Exception as e:\n",
    "            log(f\"  ⚠️ Could not fetch SQL queries: {str(e)}\")\n",
    "        \n",
    "        # Add to all_permissions\n",
    "        all_permissions.extend(sql_dashboard_perms)\n",
    "        all_permissions.extend(sql_query_perms)\n",
    "        \n",
    "        log(f\"✓ Collected {len(all_permissions)} total permission entries so far\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching SQL dashboard/query permissions: {str(e)}\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get SQL dashboard and query permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  SQL asset permissions collection disabled (ENABLE_SQL_ASSETS=False)\")\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daa5ef9d-4420-43a5-b0c7-5f8a746b6f0b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get Unity Catalog volume permissions"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if ENABLE_VOLUMES:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching Unity Catalog volume permissions...\")\n",
    "    \n",
    "    try:\n",
    "        volume_grants = []\n",
    "        \n",
    "        if 'uc_catalogs_df' in dir() and uc_catalogs_df.count() > 0:\n",
    "            catalogs = [row['catalog_name'] for row in uc_catalogs_df.select('catalog_name').collect()]\n",
    "            \n",
    "            log(f\"  Scanning {len(catalogs)} catalogs for volumes (limit: {MAX_SCHEMAS_PER_CATALOG} schemas per catalog)...\")\n",
    "            \n",
    "            for catalog_name in catalogs:\n",
    "                try:\n",
    "                    schemas = list(wc.schemas.list(catalog_name=catalog_name))\n",
    "                    schemas_to_check = schemas[:MAX_SCHEMAS_PER_CATALOG]\n",
    "                    \n",
    "                    for schema in schemas_to_check:\n",
    "                        try:\n",
    "                            volumes = list(wc.volumes.list(catalog_name=catalog_name, schema_name=schema.name))\n",
    "                            \n",
    "                            for volume in volumes:\n",
    "                                try:\n",
    "                                    grants = wc.grants.get_effective(securable_type='volume', full_name=volume.full_name)\n",
    "                                    \n",
    "                                    if grants.privilege_assignments:\n",
    "                                        for grant in grants.privilege_assignments:\n",
    "                                            for privilege in grant.privileges:\n",
    "                                                volume_grants.append({\n",
    "                                                    'volume_full_name': volume.full_name,\n",
    "                                                    'volume_name': volume.name,\n",
    "                                                    'catalog_name': catalog_name,\n",
    "                                                    'schema_name': schema.name,\n",
    "                                                    'principal': grant.principal,\n",
    "                                                    'privilege': privilege.value\n",
    "                                                })\n",
    "                                except Exception:\n",
    "                                    pass\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                except Exception:\n",
    "                    pass\n",
    "        \n",
    "        if volume_grants:\n",
    "            uc_volume_grants_df = spark.createDataFrame(volume_grants)\n",
    "            log(f\"✓ Found {uc_volume_grants_df.count()} volume grants\")\n",
    "        else:\n",
    "            uc_volume_grants_df = spark.createDataFrame([], 'volume_full_name STRING, volume_name STRING, catalog_name STRING, schema_name STRING, principal STRING, privilege STRING')\n",
    "            log(\"  No volume permissions found\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching volume permissions: {str(e)}\")\n",
    "        uc_volume_grants_df = spark.createDataFrame([], 'volume_full_name STRING, volume_name STRING, catalog_name STRING, schema_name STRING, principal STRING, privilege STRING')\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get Unity Catalog volume permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Volume permissions collection disabled (ENABLE_VOLUMES=False)\")\n",
    "    uc_volume_grants_df = spark.createDataFrame([], 'volume_full_name STRING, volume_name STRING, catalog_name STRING, schema_name STRING, principal STRING, privilege STRING')\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "690bb552-a01b-4fcd-87f2-836d7ec4bb27",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Memory optimization - flush permissions to DataFrame"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Periodically flush all_permissions list to DataFrame to manage memory\n",
    "# This prevents memory issues when collecting large numbers of permissions\n",
    "\n",
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\nOptimizing memory usage...\")\n",
    "\n",
    "# Check current memory usage\n",
    "import sys\n",
    "permissions_list_size_mb = sys.getsizeof(all_permissions) / (1024 * 1024)\n",
    "log(f\"  Current all_permissions list size: {permissions_list_size_mb:.2f} MB ({len(all_permissions)} entries)\")\n",
    "\n",
    "if permissions_list_size_mb > 100:\n",
    "    log(f\"  ⚠️ Large permissions list detected (>{permissions_list_size_mb:.0f} MB)\")\n",
    "    log(\"  Consider reducing MAX_RESOURCES_PER_TYPE or disabling some resource types\")\n",
    "\n",
    "# No action needed - permissions will be converted to DataFrame in next cell\n",
    "log(\"✓ Memory check complete\")\n",
    "\n",
    "log_execution_time(\"Memory optimization check\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4df66d5-936c-4746-9fb0-d6018255710a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create permissions DataFrame"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Creating permissions DataFrame...\")\n",
    "\n",
    "# Create permissions DataFrame\n",
    "if all_permissions:\n",
    "    permissions_df = spark.createDataFrame(all_permissions)\n",
    "else:\n",
    "    permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, principal_type STRING, permission_level STRING')\n",
    "    log(\"⚠️  Warning: No permissions collected - permissions_df is empty\")\n",
    "\n",
    "# Data Quality Check: Check for duplicates\n",
    "if permissions_df.count() > 0:\n",
    "    duplicate_count = permissions_df.count() - permissions_df.dropDuplicates().count()\n",
    "    if duplicate_count > 0:\n",
    "        log(f\"⚠️  Found {duplicate_count} duplicate permission entries (will be kept for analysis)\")\n",
    "\n",
    "# Add user-group associations to permissions\n",
    "log(\"\\nEnriching permissions with user-group associations...\")\n",
    "\n",
    "# For user permissions, add their group memberships\n",
    "user_perms_with_groups = permissions_df.filter(F.col('principal_type') == 'user') \\\n",
    "    .join(user_groups_df, permissions_df.principal == user_groups_df.user_name, 'left') \\\n",
    "    .select(\n",
    "        permissions_df['*'],\n",
    "        F.col('group_name').alias('user_groups')\n",
    "    )\n",
    "\n",
    "# For group permissions, keep as-is but add null for user_groups\n",
    "group_perms = permissions_df.filter(F.col('principal_type') == 'group') \\\n",
    "    .withColumn('user_groups', F.lit(None).cast('string'))\n",
    "\n",
    "# Service principal permissions\n",
    "sp_perms = permissions_df.filter(F.col('principal_type') == 'service_principal') \\\n",
    "    .withColumn('user_groups', F.lit(None).cast('string'))\n",
    "\n",
    "# Union all permissions with group associations\n",
    "permissions_with_groups_df = user_perms_with_groups.union(group_perms).union(sp_perms)\n",
    "\n",
    "# Create a comprehensive user permissions view that includes inherited group permissions\n",
    "log(\"Creating comprehensive user permissions view (direct + inherited from groups)...\")\n",
    "\n",
    "# Direct user permissions\n",
    "direct_user_perms = permissions_df.filter(F.col('principal_type') == 'user') \\\n",
    "    .withColumn('permission_source', F.lit('direct')) \\\n",
    "    .withColumn('source_group', F.lit(None).cast('string'))\n",
    "\n",
    "# Inherited permissions from groups\n",
    "inherited_perms = permissions_df.filter(F.col('principal_type') == 'group') \\\n",
    "    .join(user_groups_df, permissions_df.principal == user_groups_df.group_name, 'inner') \\\n",
    "    .select(\n",
    "        F.col('resource_type'),\n",
    "        F.col('resource_id'),\n",
    "        F.col('resource_name'),\n",
    "        F.col('user_name').alias('principal'),\n",
    "        F.lit('user').alias('principal_type'),\n",
    "        F.col('permission_level'),\n",
    "        F.lit('inherited').alias('permission_source'),\n",
    "        F.col('group_name').alias('source_group')\n",
    "    )\n",
    "\n",
    "# Combine direct and inherited permissions\n",
    "user_all_permissions_df = direct_user_perms.union(inherited_perms)\n",
    "\n",
    "# Data Quality Check: Identify orphaned permissions\n",
    "log(\"\\nRunning data quality checks...\")\n",
    "\n",
    "# Check for user permissions where user doesn't exist\n",
    "orphaned_user_perms = permissions_df.filter(F.col('principal_type') == 'user') \\\n",
    "    .join(users_df, permissions_df.principal == users_df.user_name, 'left_anti')\n",
    "\n",
    "orphaned_user_count = orphaned_user_perms.count()\n",
    "if orphaned_user_count > 0:\n",
    "    log(f\"⚠️  Found {orphaned_user_count} permissions for users that no longer exist\")\n",
    "\n",
    "# Check for group permissions where group doesn't exist\n",
    "orphaned_group_perms = permissions_df.filter(F.col('principal_type') == 'group') \\\n",
    "    .join(groups_df, permissions_df.principal == groups_df.group_name, 'left_anti')\n",
    "\n",
    "orphaned_group_count = orphaned_group_perms.count()\n",
    "if orphaned_group_count > 0:\n",
    "    log(f\"⚠️  Found {orphaned_group_count} permissions for groups that no longer exist\")\n",
    "\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(\"DATA COLLECTION SUMMARY\")\n",
    "log(f\"{'='*60}\")\n",
    "log(f\"  users_df: {users_df.count()} rows\")\n",
    "log(f\"  groups_df: {groups_df.count()} rows\")\n",
    "log(f\"  user_groups_df: {user_groups_df.count()} rows\")\n",
    "log(f\"  permissions_df: {permissions_df.count()} rows (original)\")\n",
    "log(f\"  permissions_with_groups_df: {permissions_with_groups_df.count()} rows (with group associations)\")\n",
    "log(f\"  user_all_permissions_df: {user_all_permissions_df.count()} rows (direct + inherited)\")\n",
    "if orphaned_user_count > 0 or orphaned_group_count > 0:\n",
    "    log(f\"  orphaned_permissions: {orphaned_user_count + orphaned_group_count} rows (cleanup recommended)\")\n",
    "log(f\"{'='*60}\")\n",
    "log(f\"\\n✓ All data collection complete!\")\n",
    "log(f\"\\nAvailable DataFrames:\")\n",
    "log(f\"  - permissions_df: Original permissions\")\n",
    "log(f\"  - permissions_with_groups_df: Permissions with user-group associations\")\n",
    "log(f\"  - user_all_permissions_df: All user permissions (direct + inherited from groups)\")\n",
    "\n",
    "log_execution_time(\"Create permissions DataFrame\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2b33361-8420-48a1-8eab-a646176c4524",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Inactive user permissions analysis (execution)"
    }
   },
   "outputs": [],
   "source": [
    "# This cell executes AFTER permissions_df is created\n",
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"INACTIVE USER PERMISSIONS ANALYSIS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Identify inactive users with active permissions\n",
    "if validate_dataframe_exists('users_df', users_df) and validate_dataframe_exists('permissions_df', permissions_df):\n",
    "    \n",
    "    inactive_users = users_df.filter(F.col('active') == False)\n",
    "    \n",
    "    log(f\"\\nInactive users in workspace: {inactive_users.count()}\")\n",
    "    \n",
    "    if inactive_users.count() > 0:\n",
    "        # Find permissions for inactive users\n",
    "        inactive_user_permissions = permissions_df.filter(\n",
    "            F.col('principal_type') == 'user'\n",
    "        ).join(\n",
    "            inactive_users.select('user_name'),\n",
    "            permissions_df.principal == inactive_users.user_name,\n",
    "            'inner'\n",
    "        )\n",
    "        \n",
    "        inactive_perm_count = inactive_user_permissions.count()\n",
    "        \n",
    "        log(f\"\\n⚠️  COMPLIANCE ISSUE: Inactive users with permissions\")\n",
    "        log(f\"  Found {inactive_perm_count} permission entries for inactive users\")\n",
    "        \n",
    "        if inactive_perm_count > 0:\n",
    "            # Summary by user\n",
    "            inactive_summary = inactive_user_permissions.groupBy('principal') \\\n",
    "                .agg(\n",
    "                    F.count('*').alias('permission_count'),\n",
    "                    F.countDistinct('resource_type').alias('resource_types'),\n",
    "                    F.collect_set('resource_type').alias('resource_type_list')\n",
    "                ) \\\n",
    "                .orderBy(F.desc('permission_count'))\n",
    "            \n",
    "            log(f\"\\n  Inactive users with permissions: {inactive_summary.count()}\")\n",
    "            \n",
    "            if not is_job_mode:\n",
    "                display(inactive_summary)\n",
    "            \n",
    "            # Detailed breakdown by resource type\n",
    "            inactive_by_resource = inactive_user_permissions.groupBy('resource_type') \\\n",
    "                .agg(\n",
    "                    F.countDistinct('principal').alias('inactive_users'),\n",
    "                    F.count('*').alias('permission_entries')\n",
    "                ) \\\n",
    "                .orderBy(F.desc('permission_entries'))\n",
    "            \n",
    "            log(f\"\\n  Permissions by resource type:\")\n",
    "            if not is_job_mode:\n",
    "                display(inactive_by_resource)\n",
    "            \n",
    "            log(f\"\\n  ⚠️  RECOMMENDATION: Remove permissions for inactive users\")\n",
    "            log(f\"     Inactive users should not retain access to workspace resources\")\n",
    "            log(f\"     This is a common compliance requirement (SOX, GDPR, etc.)\")\n",
    "            \n",
    "            # Store for export\n",
    "            inactive_user_permissions_export_df = inactive_user_permissions\n",
    "        else:\n",
    "            log(\"  ✓ No permissions found for inactive users\")\n",
    "            inactive_user_permissions_export_df = spark.createDataFrame([], permissions_df.schema)\n",
    "    else:\n",
    "        log(\"\\n✓ No inactive users in workspace\")\n",
    "        inactive_user_permissions_export_df = spark.createDataFrame([], permissions_df.schema)\n",
    "else:\n",
    "    log(\"⚠️  Cannot analyze inactive users - missing required data\")\n",
    "    inactive_user_permissions_export_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, principal_type STRING, permission_level STRING')\n",
    "\n",
    "log_execution_time(\"Inactive user permissions analysis\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c38bbfbc-d3ee-4ee6-b43c-011722f66aff",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execution summary and data quality report"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print execution summary with statistics and data quality report\n",
    "\n",
    "print_execution_summary()\n",
    "\n",
    "# Data Quality Report\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(\"DATA QUALITY REPORT\")\n",
    "log(f\"{'='*60}\")\n",
    "\n",
    "# Check for empty DataFrames\n",
    "log(\"\\nDataFrame Validation:\")\n",
    "validate_dataframe_exists(\"users_df\", users_df)\n",
    "validate_dataframe_exists(\"groups_df\", groups_df)\n",
    "validate_dataframe_exists(\"user_groups_df\", user_groups_df)\n",
    "validate_dataframe_exists(\"permissions_df\", permissions_df)\n",
    "\n",
    "# Check for suspicious patterns\n",
    "if permissions_df.count() > 0:\n",
    "    log(\"\\nPermission Distribution:\")\n",
    "    \n",
    "    # Count by resource type\n",
    "    resource_type_counts = permissions_df.groupBy('resource_type').count().orderBy(F.desc('count'))\n",
    "    log(\"  By resource type:\")\n",
    "    for row in resource_type_counts.collect():\n",
    "        log(f\"    - {row['resource_type']}: {row['count']} permissions\")\n",
    "    \n",
    "    # Count by principal type\n",
    "    principal_type_counts = permissions_df.groupBy('principal_type').count().orderBy(F.desc('count'))\n",
    "    log(\"\\n  By principal type:\")\n",
    "    for row in principal_type_counts.collect():\n",
    "        log(f\"    - {row['principal_type']}: {row['count']} permissions\")\n",
    "    \n",
    "    # Identify users with excessive permissions (potential security risk)\n",
    "    if user_all_permissions_df.count() > 0:\n",
    "        user_perm_counts = user_all_permissions_df.groupBy('principal').count().orderBy(F.desc('count'))\n",
    "        top_user = user_perm_counts.first()\n",
    "        if top_user and top_user['count'] > 100:\n",
    "            log(f\"\\n⚠️  Security Alert: User '{top_user['principal']}' has {top_user['count']} permissions (review recommended)\")\n",
    "\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(\"✓ Data quality checks complete\")\n",
    "log(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760a06fe-2864-4738-a6de-55ac0cd2fc61",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify workspace and account admins"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"ADMIN IDENTIFICATION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Identify workspace admins (members of 'admins' group)\n",
    "admin_group_name = 'admins'\n",
    "\n",
    "workspace_admins = user_groups_df.filter(F.col('group_name') == admin_group_name) \\\n",
    "    .join(users_df, 'user_name', 'inner') \\\n",
    "    .select(\n",
    "        'user_name',\n",
    "        'display_name',\n",
    "        'active'\n",
    "    )\n",
    "\n",
    "admin_count = workspace_admins.count()\n",
    "log(f\"\\nWorkspace Admins: {admin_count}\")\n",
    "\n",
    "if admin_count > 0:\n",
    "    log(\"\\nAdmin Users:\")\n",
    "    for row in workspace_admins.collect():\n",
    "        status = '✓ Active' if row['active'] else '⚠️ Inactive'\n",
    "        log(f\"  - {row['display_name']} ({row['user_name']}) - {status}\")\n",
    "    \n",
    "    # Check for inactive admins\n",
    "    inactive_admins = workspace_admins.filter(F.col('active') == False).count()\n",
    "    if inactive_admins > 0:\n",
    "        log(f\"\\n⚠️ Security Alert: {inactive_admins} inactive users still have admin permissions\")\n",
    "else:\n",
    "    log(\"  No admin users found (or 'admins' group doesn't exist)\")\n",
    "\n",
    "# Identify users with IS_OWNER permissions (high privilege)\n",
    "if permissions_df.count() > 0:\n",
    "    owners = permissions_df.filter(F.col('permission_level') == 'IS_OWNER') \\\n",
    "        .select('principal', 'resource_type', 'resource_name') \\\n",
    "        .distinct()\n",
    "    \n",
    "    owner_count = owners.select('principal').distinct().count()\n",
    "    log(f\"\\nResource Owners: {owner_count} users with IS_OWNER permissions\")\n",
    "    \n",
    "    # Count by resource type\n",
    "    owner_by_type = owners.groupBy('resource_type').count().orderBy(F.desc('count'))\n",
    "    log(\"\\nOwnership by resource type:\")\n",
    "    for row in owner_by_type.collect():\n",
    "        log(f\"  - {row['resource_type']}: {row['count']} resources\")\n",
    "\n",
    "log(\"=\"*60)\n",
    "log_execution_time(\"Identify workspace and account admins\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "352ea154-35e1-4a14-9f23-9d4f05beecb5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compliance reporting - SOX and security alerts"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"COMPLIANCE & SECURITY REPORTING\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# 1. Inactive users with permissions\n",
    "log(\"\\n1. Inactive Users with Active Permissions:\")\n",
    "\n",
    "inactive_users_with_perms = users_df.filter(F.col('active') == False) \\\n",
    "    .join(permissions_df.filter(F.col('principal_type') == 'user'), \n",
    "          users_df.user_name == permissions_df.principal, 'inner') \\\n",
    "    .select(\n",
    "        users_df.user_name,\n",
    "        users_df.display_name,\n",
    "        F.col('resource_type'),\n",
    "        F.col('resource_name'),\n",
    "        F.col('permission_level')\n",
    "    )\n",
    "\n",
    "inactive_count = inactive_users_with_perms.select('user_name').distinct().count()\n",
    "if inactive_count > 0:\n",
    "    log(f\"   ⚠️ ALERT: {inactive_count} inactive users still have permissions\")\n",
    "    log(f\"   Total permission entries: {inactive_users_with_perms.count()}\")\n",
    "    \n",
    "    # Show top inactive users by permission count\n",
    "    top_inactive = inactive_users_with_perms.groupBy('user_name', 'display_name').count() \\\n",
    "        .orderBy(F.desc('count')).limit(10)\n",
    "    \n",
    "    log(\"   Top inactive users by permission count:\")\n",
    "    for row in top_inactive.collect():\n",
    "        log(f\"     - {row['display_name']} ({row['user_name']}): {row['count']} permissions\")\n",
    "else:\n",
    "    log(\"   ✓ No inactive users with permissions found\")\n",
    "\n",
    "# 2. External users (non-company domain)\n",
    "log(\"\\n2. External User Access:\")\n",
    "\n",
    "# Identify external users (customize domain pattern as needed)\n",
    "company_domains = ['@bat.com', '@example.com']  # Add your company domains\n",
    "\n",
    "external_users = users_df.filter(\n",
    "    ~F.col('user_name').rlike('|'.join([domain.replace('.', '\\\\.') for domain in company_domains]))\n",
    ")\n",
    "\n",
    "external_count = external_users.count()\n",
    "if external_count > 0:\n",
    "    log(f\"   ⚠️ Found {external_count} external users (non-company domain)\")\n",
    "    \n",
    "    # Check if external users have permissions\n",
    "    external_with_perms = external_users.join(\n",
    "        permissions_df.filter(F.col('principal_type') == 'user'),\n",
    "        external_users.user_name == permissions_df.principal,\n",
    "        'inner'\n",
    "    ).select('user_name', 'display_name').distinct()\n",
    "    \n",
    "    external_with_perms_count = external_with_perms.count()\n",
    "    if external_with_perms_count > 0:\n",
    "        log(f\"   ⚠️ ALERT: {external_with_perms_count} external users have permissions\")\n",
    "else:\n",
    "    log(\"   ✓ No external users found\")\n",
    "\n",
    "# 3. Users with excessive permissions\n",
    "log(\"\\n3. Users with Excessive Permissions:\")\n",
    "\n",
    "if user_all_permissions_df.count() > 0:\n",
    "    excessive_threshold = 100  # Customize threshold\n",
    "    \n",
    "    excessive_perms = user_all_permissions_df.groupBy('principal').count() \\\n",
    "        .filter(F.col('count') > excessive_threshold) \\\n",
    "        .orderBy(F.desc('count'))\n",
    "    \n",
    "    excessive_count = excessive_perms.count()\n",
    "    if excessive_count > 0:\n",
    "        log(f\"   ⚠️ ALERT: {excessive_count} users have more than {excessive_threshold} permissions\")\n",
    "        log(\"   Top users:\")\n",
    "        for row in excessive_perms.limit(10).collect():\n",
    "            log(f\"     - {row['principal']}: {row['count']} permissions\")\n",
    "    else:\n",
    "        log(f\"   ✓ No users with excessive permissions (>{excessive_threshold})\")\n",
    "\n",
    "# 4. Service principals with high privileges\n",
    "log(\"\\n4. Service Principal Access:\")\n",
    "\n",
    "if 'service_principals_df' in dir() and service_principals_df.count() > 0:\n",
    "    sp_count = service_principals_df.count()\n",
    "    active_sp_count = service_principals_df.filter(F.col('active') == True).count()\n",
    "    \n",
    "    log(f\"   Total service principals: {sp_count}\")\n",
    "    log(f\"   Active service principals: {active_sp_count}\")\n",
    "    \n",
    "    # Check for SPs with admin group membership\n",
    "    sp_with_admin = service_principals_df.filter(\n",
    "        F.array_contains(F.col('groups'), 'admins')\n",
    "    )\n",
    "    \n",
    "    sp_admin_count = sp_with_admin.count()\n",
    "    if sp_admin_count > 0:\n",
    "        log(f\"   ⚠️ ALERT: {sp_admin_count} service principals have admin group membership\")\n",
    "else:\n",
    "    log(\"   No service principal data available\")\n",
    "\n",
    "# 5. Orphaned permissions summary\n",
    "log(\"\\n5. Orphaned Permissions (Cleanup Recommended):\")\n",
    "\n",
    "if 'orphaned_user_count' in dir() and 'orphaned_group_count' in dir():\n",
    "    total_orphaned = orphaned_user_count + orphaned_group_count\n",
    "    if total_orphaned > 0:\n",
    "        log(f\"   ⚠️ Found {total_orphaned} orphaned permissions\")\n",
    "        log(f\"     - User permissions: {orphaned_user_count}\")\n",
    "        log(f\"     - Group permissions: {orphaned_group_count}\")\n",
    "        log(\"   Recommendation: Remove permissions for deleted users/groups\")\n",
    "    else:\n",
    "        log(\"   ✓ No orphaned permissions found\")\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"✓ Compliance reporting complete\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "log_execution_time(\"Compliance reporting\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a0a9845-35f5-4e36-9cdb-b928d1723455",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Incremental change detection (compare with previous run)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"INCREMENTAL CHANGE DETECTION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Only run if Delta export is enabled and history table exists\n",
    "if ENABLE_DELTA_EXPORT:\n",
    "    try:\n",
    "        # Check if history table exists\n",
    "        table_exists = spark.catalog.tableExists(DELTA_TABLE_NAME)\n",
    "        \n",
    "        if table_exists:\n",
    "            log(f\"\\nComparing with previous audit run from {DELTA_TABLE_NAME}...\")\n",
    "            \n",
    "            # Get the most recent previous run (not including current run)\n",
    "            previous_run = spark.sql(f\"\"\"\n",
    "                SELECT MAX(audit_run_timestamp) as last_run\n",
    "                FROM {DELTA_TABLE_NAME}\n",
    "                WHERE audit_run_timestamp < current_timestamp() - INTERVAL 1 MINUTE\n",
    "            \"\"\").first()\n",
    "            \n",
    "            if previous_run and previous_run['last_run']:\n",
    "                last_run_time = previous_run['last_run']\n",
    "                log(f\"Previous run found: {last_run_time}\")\n",
    "                \n",
    "                # Get previous permissions\n",
    "                previous_perms = spark.sql(f\"\"\"\n",
    "                    SELECT resource_type, resource_id, resource_name, principal, permission_level\n",
    "                    FROM {DELTA_TABLE_NAME}\n",
    "                    WHERE audit_run_timestamp = '{last_run_time}'\n",
    "                \"\"\")\n",
    "                \n",
    "                # Current permissions\n",
    "                current_perms = permissions_df.select(\n",
    "                    'resource_type', 'resource_id', 'resource_name', 'principal', 'permission_level'\n",
    "                )\n",
    "                \n",
    "                # Find new permissions (in current but not in previous)\n",
    "                new_perms = current_perms.subtract(previous_perms)\n",
    "                new_count = new_perms.count()\n",
    "                \n",
    "                # Find removed permissions (in previous but not in current)\n",
    "                removed_perms = previous_perms.subtract(current_perms)\n",
    "                removed_count = removed_perms.count()\n",
    "                \n",
    "                log(f\"\\nChange Summary:\")\n",
    "                log(f\"  ➕ New permissions: {new_count}\")\n",
    "                log(f\"  ➖ Removed permissions: {removed_count}\")\n",
    "                log(f\"  \uD83D\uDD04 Total changes: {new_count + removed_count}\")\n",
    "                \n",
    "                if new_count > 0:\n",
    "                    log(f\"\\n  Top new permissions by principal:\")\n",
    "                    new_by_principal = new_perms.groupBy('principal').count() \\\n",
    "                        .orderBy(F.desc('count')).limit(10)\n",
    "                    for row in new_by_principal.collect():\n",
    "                        log(f\"    - {row['principal']}: {row['count']} new permissions\")\n",
    "                \n",
    "                if removed_count > 0:\n",
    "                    log(f\"\\n  Top removed permissions by principal:\")\n",
    "                    removed_by_principal = removed_perms.groupBy('principal').count() \\\n",
    "                        .orderBy(F.desc('count')).limit(10)\n",
    "                    for row in removed_by_principal.collect():\n",
    "                        log(f\"    - {row['principal']}: {row['count']} removed permissions\")\n",
    "                \n",
    "                # Store change DataFrames for export\n",
    "                new_permissions_df = new_perms\n",
    "                removed_permissions_df = removed_perms\n",
    "                \n",
    "            else:\n",
    "                log(\"\\n  No previous audit run found for comparison\")\n",
    "                log(\"  This appears to be the first run\")\n",
    "                new_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, permission_level STRING')\n",
    "                removed_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, permission_level STRING')\n",
    "        else:\n",
    "            log(f\"\\n  History table {DELTA_TABLE_NAME} does not exist yet\")\n",
    "            log(\"  Run with ENABLE_DELTA_EXPORT=True to create it\")\n",
    "            new_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, permission_level STRING')\n",
    "            removed_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, permission_level STRING')\n",
    "    \n",
    "    except Exception as e:\n",
    "        log(f\"\\n❌ Error during change detection: {str(e)}\")\n",
    "        new_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, permission_level STRING')\n",
    "        removed_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, permission_level STRING')\n",
    "else:\n",
    "    log(\"\\n  Change detection disabled (ENABLE_DELTA_EXPORT=False)\")\n",
    "    log(\"  Enable Delta export to track changes over time\")\n",
    "    new_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, permission_level STRING')\n",
    "    removed_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, permission_level STRING')\n",
    "\n",
    "log(\"=\"*60)\n",
    "log_execution_time(\"Incremental change detection\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b98e370-4fad-4292-aa8f-76d3a385187b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Permission recommendations and risk analysis"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"PERMISSION RECOMMENDATIONS & RISK ANALYSIS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# 1. Over-privileged users (users with CAN_MANAGE on many resources)\n",
    "log(\"\\n1. Over-Privileged Users:\")\n",
    "\n",
    "if permissions_df.count() > 0:\n",
    "    manage_perms = permissions_df.filter(\n",
    "        F.col('permission_level').isin(['CAN_MANAGE', 'IS_OWNER'])\n",
    "    )\n",
    "    \n",
    "    over_privileged = manage_perms.groupBy('principal', 'principal_type') \\\n",
    "        .agg(\n",
    "            F.count('*').alias('manage_permission_count'),\n",
    "            F.countDistinct('resource_type').alias('resource_types'),\n",
    "            F.collect_set('resource_type').alias('resource_type_list')\n",
    "        ) \\\n",
    "        .filter(F.col('manage_permission_count') > 20) \\\n",
    "        .orderBy(F.desc('manage_permission_count'))\n",
    "    \n",
    "    over_priv_count = over_privileged.count()\n",
    "    if over_priv_count > 0:\n",
    "        log(f\"   ⚠️ Found {over_priv_count} principals with >20 CAN_MANAGE/IS_OWNER permissions\")\n",
    "        log(\"   Top over-privileged principals:\")\n",
    "        for row in over_privileged.limit(10).collect():\n",
    "            log(f\"     - {row['principal']} ({row['principal_type']}): {row['manage_permission_count']} manage permissions across {row['resource_types']} resource types\")\n",
    "        log(\"   Recommendation: Review if all manage permissions are necessary\")\n",
    "    else:\n",
    "        log(\"   ✓ No over-privileged users detected\")\n",
    "\n",
    "# 2. Segregation of duties - users with conflicting permissions\n",
    "log(\"\\n2. Segregation of Duties Analysis:\")\n",
    "\n",
    "# Example: Users who can both create and approve (customize based on your requirements)\n",
    "if user_all_permissions_df.count() > 0:\n",
    "    # Users with both production and development access\n",
    "    prod_and_dev = user_all_permissions_df.filter(\n",
    "        (F.col('resource_name').contains('prod')) | (F.col('resource_name').contains('production'))\n",
    "    ).select('principal').distinct() \\\n",
    "    .intersect(\n",
    "        user_all_permissions_df.filter(\n",
    "            (F.col('resource_name').contains('dev')) | (F.col('resource_name').contains('development'))\n",
    "        ).select('principal').distinct()\n",
    "    )\n",
    "    \n",
    "    sod_count = prod_and_dev.count()\n",
    "    if sod_count > 0:\n",
    "        log(f\"   ⚠️ Found {sod_count} users with both production and development access\")\n",
    "        log(\"   Recommendation: Review segregation of duties policies\")\n",
    "    else:\n",
    "        log(\"   ✓ No obvious segregation of duties violations detected\")\n",
    "\n",
    "# 3. Group membership recommendations\n",
    "log(\"\\n3. Group Membership Optimization:\")\n",
    "\n",
    "# Find users with many direct permissions (should use groups instead)\n",
    "if user_all_permissions_df.count() > 0:\n",
    "    direct_perm_heavy = user_all_permissions_df.filter(F.col('permission_source') == 'direct') \\\n",
    "        .groupBy('principal').count() \\\n",
    "        .filter(F.col('count') > 30) \\\n",
    "        .orderBy(F.desc('count'))\n",
    "    \n",
    "    direct_heavy_count = direct_perm_heavy.count()\n",
    "    if direct_heavy_count > 0:\n",
    "        log(f\"   ⚠️ Found {direct_heavy_count} users with >30 direct permissions\")\n",
    "        log(\"   Recommendation: Consider using groups for permission management\")\n",
    "        log(\"   Top users with direct permissions:\")\n",
    "        for row in direct_perm_heavy.limit(5).collect():\n",
    "            log(f\"     - {row['principal']}: {row['count']} direct permissions\")\n",
    "    else:\n",
    "        log(\"   ✓ Good group usage - no users with excessive direct permissions\")\n",
    "\n",
    "# 4. Unused groups (groups with no permissions)\n",
    "log(\"\\n4. Unused Groups:\")\n",
    "\n",
    "groups_with_perms = permissions_df.filter(F.col('principal_type') == 'group') \\\n",
    "    .select('principal').distinct()\n",
    "\n",
    "unused_groups = groups_df.join(\n",
    "    groups_with_perms,\n",
    "    groups_df.group_name == groups_with_perms.principal,\n",
    "    'left_anti'\n",
    ")\n",
    "\n",
    "unused_count = unused_groups.count()\n",
    "if unused_count > 0:\n",
    "    log(f\"   ⚠️ Found {unused_count} groups with no permissions assigned\")\n",
    "    log(\"   Recommendation: Review if these groups are still needed\")\n",
    "else:\n",
    "    log(\"   ✓ All groups have permissions assigned\")\n",
    "\n",
    "# 5. Token security analysis\n",
    "log(\"\\n5. Token Security:\")\n",
    "\n",
    "if 'tokens_df' in dir() and tokens_df.count() > 0:\n",
    "    total_tokens = tokens_df.count()\n",
    "    no_expiry_tokens = tokens_df.filter(F.col('expiry_time').isNull()).count()\n",
    "    \n",
    "    log(f\"   Total active tokens: {total_tokens}\")\n",
    "    if no_expiry_tokens > 0:\n",
    "        log(f\"   ⚠️ ALERT: {no_expiry_tokens} tokens have no expiration date\")\n",
    "        log(\"   Recommendation: Set expiration dates for all tokens\")\n",
    "    else:\n",
    "        log(\"   ✓ All tokens have expiration dates\")\n",
    "else:\n",
    "    log(\"   No token data available\")\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"✓ Risk analysis complete\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "log_execution_time(\"Permission recommendations and risk analysis\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59006beb-13cb-467e-b466-238809b642e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Permission concentration analysis"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"PERMISSION CONCENTRATION ANALYSIS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if validate_dataframe_exists('permissions_df', permissions_df):\n",
    "    \n",
    "    # Analyze users with high permission counts\n",
    "    log(\"\\n1. Users with Most Permissions:\")\n",
    "    \n",
    "    user_permission_counts = permissions_df.filter(\n",
    "        F.col('principal_type') == 'user'\n",
    "    ).groupBy('principal') \\\n",
    "        .agg(\n",
    "            F.count('*').alias('total_permissions'),\n",
    "            F.countDistinct('resource_type').alias('resource_types'),\n",
    "            F.countDistinct('resource_id').alias('unique_resources'),\n",
    "            F.sum(F.when(F.col('permission_level').isin(['IS_OWNER', 'CAN_MANAGE']), 1).otherwise(0)).alias('admin_permissions')\n",
    "        ) \\\n",
    "        .orderBy(F.desc('total_permissions'))\n",
    "    \n",
    "    top_users = user_permission_counts.limit(10)\n",
    "    \n",
    "    log(f\"  Top 10 users by permission count:\")\n",
    "    if not is_job_mode:\n",
    "        display(top_users)\n",
    "    \n",
    "    # Identify users with excessive admin permissions\n",
    "    excessive_admin_threshold = 20  # Users with 20+ admin permissions\n",
    "    \n",
    "    excessive_admins = user_permission_counts.filter(\n",
    "        F.col('admin_permissions') >= excessive_admin_threshold\n",
    "    )\n",
    "    \n",
    "    log(f\"\\n2. Users with Excessive Admin Permissions (>={excessive_admin_threshold}):\")\n",
    "    log(f\"  Found {excessive_admins.count()} users\")\n",
    "    \n",
    "    if excessive_admins.count() > 0:\n",
    "        log(f\"\\n  ⚠️  SECURITY CONCERN: Users with excessive admin permissions\")\n",
    "        if not is_job_mode:\n",
    "            display(excessive_admins.orderBy(F.desc('admin_permissions')))\n",
    "        log(f\"\\n  ⚠️  RECOMMENDATION: Review and reduce admin permissions\")\n",
    "        log(f\"     Users should follow principle of least privilege\")\n",
    "    else:\n",
    "        log(\"  ✓ No users with excessive admin permissions\")\n",
    "    \n",
    "    # Analyze permission distribution\n",
    "    log(f\"\\n3. Permission Distribution Statistics:\")\n",
    "    \n",
    "    perm_stats = user_permission_counts.agg(\n",
    "        F.avg('total_permissions').alias('avg_permissions'),\n",
    "        F.max('total_permissions').alias('max_permissions'),\n",
    "        F.min('total_permissions').alias('min_permissions'),\n",
    "        F.percentile_approx('total_permissions', 0.5).alias('median_permissions'),\n",
    "        F.percentile_approx('total_permissions', 0.9).alias('p90_permissions')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    log(f\"  Average permissions per user: {perm_stats.avg_permissions:.1f}\")\n",
    "    log(f\"  Median permissions per user: {perm_stats.median_permissions}\")\n",
    "    log(f\"  90th percentile: {perm_stats.p90_permissions}\")\n",
    "    log(f\"  Max permissions (single user): {perm_stats.max_permissions}\")\n",
    "    \n",
    "    # Store for export\n",
    "    permission_concentration_df = user_permission_counts\n",
    "    excessive_admin_permissions_df = excessive_admins\n",
    "    \n",
    "    log_execution_time(\"Permission concentration analysis\", cell_start_time)\n",
    "else:\n",
    "    log(\"⚠️  Cannot analyze permission concentration - missing permissions data\")\n",
    "    permission_concentration_df = spark.createDataFrame([], 'principal STRING, total_permissions LONG, resource_types LONG, unique_resources LONG, admin_permissions LONG')\n",
    "    excessive_admin_permissions_df = spark.createDataFrame([], 'principal STRING, total_permissions LONG, resource_types LONG, unique_resources LONG, admin_permissions LONG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c298392-23a5-4a19-93d0-acc94adabc3d",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "External user detection and analysis"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"EXTERNAL USER DETECTION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if validate_dataframe_exists('users_df', users_df):\n",
    "    \n",
    "    # Define company domain (customize this for your organization)\n",
    "    COMPANY_DOMAIN = 'bat.com'  # Change to your company domain\n",
    "    \n",
    "    log(f\"\\nChecking for users outside company domain: @{COMPANY_DOMAIN}\")\n",
    "    \n",
    "    # Identify external users (non-company domain)\n",
    "    external_users = users_df.filter(\n",
    "        ~F.col('user_name').endswith(f'@{COMPANY_DOMAIN}')\n",
    "    )\n",
    "    \n",
    "    external_count = external_users.count()\n",
    "    active_external = external_users.filter(F.col('active') == True).count()\n",
    "    \n",
    "    log(f\"\\nExternal users found: {external_count}\")\n",
    "    log(f\"  Active external users: {active_external}\")\n",
    "    log(f\"  Inactive external users: {external_count - active_external}\")\n",
    "    \n",
    "    if external_count > 0:\n",
    "        log(f\"\\n⚠️  SECURITY REVIEW: External users detected\")\n",
    "        \n",
    "        # Show external user details\n",
    "        if not is_job_mode:\n",
    "            display(external_users.select('user_name', 'display_name', 'active'))\n",
    "        \n",
    "        # Check permissions for external users\n",
    "        if validate_dataframe_exists('permissions_df', permissions_df):\n",
    "            external_user_permissions = permissions_df.filter(\n",
    "                F.col('principal_type') == 'user'\n",
    "            ).join(\n",
    "                external_users.select('user_name'),\n",
    "                permissions_df.principal == external_users.user_name,\n",
    "                'inner'\n",
    "            )\n",
    "            \n",
    "            external_perm_count = external_user_permissions.count()\n",
    "            \n",
    "            log(f\"\\n  External user permissions: {external_perm_count} entries\")\n",
    "            \n",
    "            if external_perm_count > 0:\n",
    "                # Summary by external user\n",
    "                external_perm_summary = external_user_permissions.groupBy('principal') \\\n",
    "                    .agg(\n",
    "                        F.count('*').alias('permission_count'),\n",
    "                        F.countDistinct('resource_type').alias('resource_types'),\n",
    "                        F.collect_set('resource_type').alias('resource_type_list')\n",
    "                    ) \\\n",
    "                    .orderBy(F.desc('permission_count'))\n",
    "                \n",
    "                if not is_job_mode:\n",
    "                    log(\"\\n  External user permission summary:\")\n",
    "                    display(external_perm_summary)\n",
    "                \n",
    "                log(f\"\\n  ⚠️  RECOMMENDATION: Review external user access\")\n",
    "                log(f\"     Ensure external users have appropriate business justification\")\n",
    "                log(f\"     Consider using service principals for external integrations\")\n",
    "                log(f\"     Verify external users comply with data sharing agreements\")\n",
    "            \n",
    "            # Store for export\n",
    "            external_user_permissions_df = external_user_permissions\n",
    "        else:\n",
    "            external_user_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, principal_type STRING, permission_level STRING')\n",
    "    else:\n",
    "        log(\"\\n✓ No external users detected\")\n",
    "        log(f\"   All users belong to @{COMPANY_DOMAIN} domain\")\n",
    "        external_user_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, principal_type STRING, permission_level STRING')\n",
    "    \n",
    "    # Store for export\n",
    "    external_users_df = external_users\n",
    "    \n",
    "    log_execution_time(\"External user detection\", cell_start_time)\n",
    "else:\n",
    "    log(\"⚠️  Cannot detect external users - missing user data\")\n",
    "    external_users_df = spark.createDataFrame([], 'user_name STRING, display_name STRING, active BOOLEAN')\n",
    "    external_user_permissions_df = spark.createDataFrame([], 'resource_type STRING, resource_id STRING, resource_name STRING, principal STRING, principal_type STRING, permission_level STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca125f6-f3d1-4cc7-b7ff-353447cfe251",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cross-resource permission analysis"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"CROSS-RESOURCE PERMISSION ANALYSIS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if validate_dataframe_exists('permissions_df', permissions_df):\n",
    "    \n",
    "    # Identify users with permissions across multiple resource types\n",
    "    log(\"\\n1. Users with Broad Access Across Resource Types:\")\n",
    "    \n",
    "    cross_resource_users = permissions_df.filter(\n",
    "        F.col('principal_type') == 'user'\n",
    "    ).groupBy('principal') \\\n",
    "        .agg(\n",
    "            F.countDistinct('resource_type').alias('resource_type_count'),\n",
    "            F.collect_set('resource_type').alias('resource_types'),\n",
    "            F.count('*').alias('total_permissions')\n",
    "        ) \\\n",
    "        .filter(F.col('resource_type_count') >= 5) \\\n",
    "        .orderBy(F.desc('resource_type_count'))\n",
    "    \n",
    "    log(f\"  Users with access to 5+ resource types: {cross_resource_users.count()}\")\n",
    "    \n",
    "    if cross_resource_users.count() > 0 and not is_job_mode:\n",
    "        display(cross_resource_users.limit(20))\n",
    "    \n",
    "    # Identify potential segregation of duties violations\n",
    "    log(\"\\n2. Potential Segregation of Duties (SOD) Violations:\")\n",
    "    \n",
    "    # Users with both development (notebooks/repos) and production (jobs/pipelines) access\n",
    "    dev_resources = ['workspace_objects', 'repos']\n",
    "    prod_resources = ['jobs', 'pipelines']\n",
    "    \n",
    "    dev_users = permissions_df.filter(\n",
    "        (F.col('principal_type') == 'user') &\n",
    "        (F.col('resource_type').isin(dev_resources))\n",
    "    ).select('principal').distinct()\n",
    "    \n",
    "    prod_users = permissions_df.filter(\n",
    "        (F.col('principal_type') == 'user') &\n",
    "        (F.col('resource_type').isin(prod_resources))\n",
    "    ).select('principal').distinct()\n",
    "    \n",
    "    sod_violations = dev_users.join(prod_users, 'principal', 'inner')\n",
    "    \n",
    "    log(f\"  Users with both dev and prod access: {sod_violations.count()}\")\n",
    "    \n",
    "    if sod_violations.count() > 0:\n",
    "        log(f\"\\n  ⚠️  COMPLIANCE CONCERN: Potential SOD violations\")\n",
    "        \n",
    "        # Get detailed permissions for SOD violators\n",
    "        sod_details = permissions_df.filter(\n",
    "            (F.col('principal_type') == 'user') &\n",
    "            (F.col('resource_type').isin(dev_resources + prod_resources))\n",
    "        ).join(sod_violations, 'principal', 'inner') \\\n",
    "            .groupBy('principal') \\\n",
    "            .agg(\n",
    "                F.collect_set(F.when(F.col('resource_type').isin(dev_resources), F.col('resource_type'))).alias('dev_access'),\n",
    "                F.collect_set(F.when(F.col('resource_type').isin(prod_resources), F.col('resource_type'))).alias('prod_access'),\n",
    "                F.count('*').alias('total_permissions')\n",
    "            ) \\\n",
    "            .orderBy(F.desc('total_permissions'))\n",
    "        \n",
    "        if not is_job_mode:\n",
    "            display(sod_details.limit(20))\n",
    "        \n",
    "        log(f\"\\n  ⚠️  RECOMMENDATION: Review segregation of duties\")\n",
    "        log(f\"     Consider separating development and production access\")\n",
    "        log(f\"     Use separate accounts or groups for dev vs prod environments\")\n",
    "    else:\n",
    "        log(\"  ✓ No obvious SOD violations detected\")\n",
    "        sod_details = spark.createDataFrame([], 'principal STRING, dev_access ARRAY<STRING>, prod_access ARRAY<STRING>, total_permissions LONG')\n",
    "    \n",
    "    # Store for export\n",
    "    cross_resource_permissions_df = cross_resource_users\n",
    "    sod_violations_df = sod_details\n",
    "    \n",
    "    log_execution_time(\"Cross-resource permission analysis\", cell_start_time)\n",
    "else:\n",
    "    log(\"⚠️  Cannot analyze cross-resource permissions - missing permissions data\")\n",
    "    cross_resource_permissions_df = spark.createDataFrame([], 'principal STRING, resource_type_count LONG, resource_types ARRAY<STRING>, total_permissions LONG')\n",
    "    sod_violations_df = spark.createDataFrame([], 'principal STRING, dev_access ARRAY<STRING>, prod_access ARRAY<STRING>, total_permissions LONG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7c07811-4f2b-4ecf-8a62-789c9f6f24e5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Service principal token audit"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"SERVICE PRINCIPAL TOKEN AUDIT\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if ENABLE_TOKEN_AUDIT and 'tokens_df' in dir() and tokens_df.count() > 0:\n",
    "    \n",
    "    # Identify service principal tokens\n",
    "    sp_tokens = tokens_df.filter(\n",
    "        F.col('created_by_username').isNull() | \n",
    "        F.col('created_by_username').contains('ServicePrincipal')\n",
    "    )\n",
    "    \n",
    "    log(f\"\\nService principal tokens: {sp_tokens.count()}\")\n",
    "    \n",
    "    if sp_tokens.count() > 0:\n",
    "        # Check for tokens without expiration\n",
    "        sp_tokens_no_expiry = sp_tokens.filter(\n",
    "            (F.col('expiry_time').isNull()) | (F.col('expiry_time') == 0)\n",
    "        )\n",
    "        \n",
    "        log(f\"  Service principal tokens without expiry: {sp_tokens_no_expiry.count()}\")\n",
    "        \n",
    "        if sp_tokens_no_expiry.count() > 0:\n",
    "            log(f\"\\n  ⚠️  CRITICAL: Service principal tokens without expiration\")\n",
    "            if not is_job_mode:\n",
    "                display(sp_tokens_no_expiry.select('token_id', 'comment', 'creation_time'))\n",
    "            log(f\"\\n  ⚠️  RECOMMENDATION: Set expiration dates for all service principal tokens\")\n",
    "            log(f\"     Service principals should use short-lived tokens or OAuth\")\n",
    "        else:\n",
    "            log(\"  ✓ All service principal tokens have expiration dates\")\n",
    "        \n",
    "        # Token age for service principals\n",
    "        from pyspark.sql.functions import current_timestamp\n",
    "        \n",
    "        sp_tokens_with_age = sp_tokens.withColumn(\n",
    "            'age_days',\n",
    "            (F.unix_timestamp(current_timestamp()) - (F.col('creation_time') / 1000)) / (24 * 60 * 60)\n",
    "        )\n",
    "        \n",
    "        old_sp_tokens = sp_tokens_with_age.filter(F.col('age_days') > 180)\n",
    "        \n",
    "        log(f\"\\n  Service principal tokens older than 6 months: {old_sp_tokens.count()}\")\n",
    "        \n",
    "        if old_sp_tokens.count() > 0:\n",
    "            log(f\"  ⚠️  RECOMMENDATION: Rotate old service principal tokens\")\n",
    "            log(f\"     Tokens should be rotated regularly (every 90-180 days)\")\n",
    "        \n",
    "        # Store for export\n",
    "        sp_tokens_df = sp_tokens\n",
    "        sp_tokens_no_expiry_df = sp_tokens_no_expiry\n",
    "    else:\n",
    "        log(\"\\nℹ️  No service principal tokens found\")\n",
    "        sp_tokens_df = spark.createDataFrame([], tokens_df.schema)\n",
    "        sp_tokens_no_expiry_df = spark.createDataFrame([], tokens_df.schema)\n",
    "    \n",
    "    log_execution_time(\"Service principal token audit\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Service principal token audit skipped (no token data available)\")\n",
    "    sp_tokens_df = spark.createDataFrame([], 'token_id STRING, created_by_username STRING, created_by_id STRING, creation_time LONG, expiry_time LONG, comment STRING')\n",
    "    sp_tokens_no_expiry_df = spark.createDataFrame([], 'token_id STRING, created_by_username STRING, created_by_id STRING, creation_time LONG, expiry_time LONG, comment STRING')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca1e794-e800-4f5c-a596-4542a54cac6f",
     "showTitle": true,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"principal\":175},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1770660097790}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": "User permissions summary"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log(\"=== User Permissions Summary (Direct + Inherited) ===\")\n",
    "log(f\"Total user permission entries: {user_all_permissions_df.count()}\")\n",
    "\n",
    "# Summary by user showing direct vs inherited\n",
    "user_perm_summary = user_all_permissions_df.groupBy('principal').agg(\n",
    "    F.count('*').alias('total_permissions'),\n",
    "    F.sum(F.when(F.col('permission_source') == 'direct', 1).otherwise(0)).alias('direct_permissions'),\n",
    "    F.sum(F.when(F.col('permission_source') == 'inherited', 1).otherwise(0)).alias('inherited_permissions'),\n",
    "    F.countDistinct('resource_type').alias('resource_types_count'),\n",
    "    F.collect_set('permission_level').alias('permission_levels'),\n",
    "    F.collect_set('source_group').alias('inherited_from_groups')\n",
    ").orderBy(F.desc('total_permissions'))\n",
    "\n",
    "log(f\"\\nUsers with permissions: {user_perm_summary.count()}\")\n",
    "log(\"\\nTop users by total permissions:\")\n",
    "\n",
    "if not is_job_mode:\n",
    "    display(user_perm_summary.limit(50))\n",
    "\n",
    "    # Detailed view - showing permission source\n",
    "    log(\"\\n=== Detailed User Permissions (Top 100) ===\")\n",
    "\n",
    "user_permissions_detailed = user_all_permissions_df.select(\n",
    "    'principal',\n",
    "    'resource_type',\n",
    "    'resource_name',\n",
    "    'permission_level',\n",
    "    'permission_source',\n",
    "    'source_group'\n",
    ").orderBy('principal', 'resource_type', 'permission_source')\n",
    "\n",
    "if not is_job_mode:\n",
    "    display(user_permissions_detailed.limit(100))\n",
    "\n",
    "    # Show users with their group memberships\n",
    "    log(\"\\n=== Users and Their Groups ===\")\n",
    "\n",
    "user_with_groups = users_df.join(user_groups_df, users_df.user_name == user_groups_df.user_name, 'left') \\\n",
    "    .select(\n",
    "        users_df.user_name,\n",
    "        users_df.display_name,\n",
    "        users_df.active,\n",
    "        user_groups_df.group_name\n",
    "    ) \\\n",
    "    .groupBy(users_df.user_name, users_df.display_name, users_df.active).agg(\n",
    "        F.collect_set('group_name').alias('groups')\n",
    "    ).orderBy('user_name')\n",
    "\n",
    "log(f\"Total users: {user_with_groups.count()}\")\n",
    "\n",
    "if not is_job_mode:\n",
    "    display(user_with_groups.limit(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d175a777-5465-4f1a-89ea-af49001612e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Group permissions summary"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log(\"=== Group Permissions Summary ===\")\n",
    "\n",
    "# Filter permissions for groups only\n",
    "group_permissions = permissions_df.filter(permissions_df.principal_type == 'group')\n",
    "\n",
    "log(f\"Total group permission entries: {group_permissions.count()}\")\n",
    "\n",
    "# Summary by group with member count\n",
    "group_perm_summary = group_permissions.groupBy('principal').agg(\n",
    "    F.count('*').alias('total_permissions'),\n",
    "    F.countDistinct('resource_type').alias('resource_types_count'),\n",
    "    F.collect_set('permission_level').alias('permission_levels')\n",
    ")\n",
    "\n",
    "# Add member count to groups\n",
    "group_members_count = user_groups_df.groupBy('group_name').agg(\n",
    "    F.count('*').alias('member_count')\n",
    ")\n",
    "\n",
    "group_summary_with_members = group_perm_summary \\\n",
    "    .join(group_members_count, group_perm_summary.principal == group_members_count.group_name, 'left') \\\n",
    "    .select(\n",
    "        F.col('principal').alias('group_name'),\n",
    "        F.col('total_permissions'),\n",
    "        F.col('resource_types_count'),\n",
    "        F.col('permission_levels'),\n",
    "        F.coalesce(F.col('member_count'), F.lit(0)).alias('member_count')\n",
    "    ).orderBy(F.desc('total_permissions'))\n",
    "\n",
    "log(f\"\\nGroups with permissions: {group_summary_with_members.count()}\")\n",
    "log(\"\\nTop groups by total permissions:\")\n",
    "\n",
    "if not is_job_mode:\n",
    "    display(group_summary_with_members.limit(50))\n",
    "\n",
    "    # Detailed view - group permissions\n",
    "    log(\"\\n=== Detailed Group Permissions (Top 100) ===\")\n",
    "\n",
    "group_permissions_detailed = group_permissions.select(\n",
    "    'principal',\n",
    "    'resource_type',\n",
    "    'resource_name',\n",
    "    'permission_level'\n",
    ").orderBy('principal', 'resource_type')\n",
    "\n",
    "if not is_job_mode:\n",
    "    display(group_permissions_detailed.limit(100))\n",
    "\n",
    "    # Show groups with their members\n",
    "    log(\"\\n=== Groups and Their Members ===\")\n",
    "\n",
    "groups_with_members = groups_df.join(user_groups_df, groups_df.group_name == user_groups_df.group_name, 'left') \\\n",
    "    .select(\n",
    "        groups_df.group_name,\n",
    "        groups_df.group_id,\n",
    "        user_groups_df.user_name\n",
    "    ) \\\n",
    "    .groupBy(groups_df.group_name, groups_df.group_id).agg(\n",
    "        F.collect_set('user_name').alias('members'),\n",
    "        F.count('user_name').alias('member_count')\n",
    "    ).orderBy(F.desc('member_count'))\n",
    "\n",
    "log(f\"Total groups: {groups_with_members.count()}\")\n",
    "\n",
    "if not is_job_mode:\n",
    "    display(groups_with_members.limit(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65d442e7-5c87-40ee-a37c-a1e6bfb61aa5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Permission levels reference"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Permission Levels Reference - Based on Databricks Documentation\n",
    "\n",
    "permission_definitions = {\n",
    "    'Jobs': {\n",
    "        'CAN_VIEW': 'View job details, settings, and results',\n",
    "        'CAN_MANAGE_RUN': 'View results, run now, cancel runs, view Spark UI and logs',\n",
    "        'IS_OWNER': 'Full control including edit, delete, and modify permissions (creator default)',\n",
    "        'CAN_MANAGE': 'Edit job settings, delete job, and modify permissions'\n",
    "    },\n",
    "    'Clusters': {\n",
    "        'CAN_ATTACH_TO': 'Attach notebooks, view Spark UI and metrics',\n",
    "        'CAN_RESTART': 'Attach notebooks, view metrics, start/stop/restart cluster',\n",
    "        'CAN_MANAGE': 'Full control: edit, resize, attach libraries, modify permissions, view driver logs'\n",
    "    },\n",
    "    'SQL Warehouses': {\n",
    "        'CAN_VIEW': 'View warehouse details, query history, and monitoring (cannot run queries)',\n",
    "        'CAN_MONITOR': 'Run queries, view query history and profiles for troubleshooting',\n",
    "        'CAN_USE': 'Start warehouse and run queries',\n",
    "        'IS_OWNER': 'Full control (creator default)',\n",
    "        'CAN_MANAGE': 'Stop, delete, edit warehouse, and modify permissions'\n",
    "    },\n",
    "    'Pipelines': {\n",
    "        'CAN_VIEW': 'View pipeline details, list pipelines, view Spark UI and driver logs',\n",
    "        'CAN_RUN': 'Start/stop pipeline updates, stop pipeline clusters',\n",
    "        'CAN_MANAGE': 'Edit settings, delete pipeline, purge runs, modify permissions',\n",
    "        'IS_OWNER': 'Full control (creator default)'\n",
    "    },\n",
    "    'Notebooks/Files': {\n",
    "        'CAN_VIEW': 'Read file and add comments (view-only access)',\n",
    "        'CAN_RUN': 'Read, comment, attach/detach, and run file interactively',\n",
    "        'CAN_EDIT': 'Read, run, and edit file',\n",
    "        'CAN_MANAGE': 'Full control including modify permissions'\n",
    "    },\n",
    "    'Folders/Directories': {\n",
    "        'CAN_VIEW': 'List and view objects in folder',\n",
    "        'CAN_EDIT': 'View, clone, and export items',\n",
    "        'CAN_RUN': 'View, clone, export, and run objects',\n",
    "        'CAN_MANAGE': 'Full control: create, import, delete, move, rename items, modify permissions'\n",
    "    },\n",
    "    'Instance Pools': {\n",
    "        'CAN_ATTACH_TO': 'Attach clusters to the pool',\n",
    "        'CAN_MANAGE': 'Delete, edit pool, and modify permissions'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"DATABRICKS PERMISSION LEVELS REFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for resource_type, permissions in permission_definitions.items():\n",
    "    print(f\"\\n{resource_type.upper()}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for perm_level, description in permissions.items():\n",
    "        print(f\"  {perm_level:20} {description}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NOTES:\")\n",
    "print(\"=\"*80)\n",
    "print(\"  • IS_OWNER: Automatically assigned to resource creator\")\n",
    "print(\"  • Workspace admins: Automatically inherit CAN_MANAGE on all resources\")\n",
    "print(\"  • Job runs: Execute with job owner's permissions, not the user who clicked 'Run Now'\")\n",
    "print(\"  • Job clusters: Inherit permissions from their parent job\")\n",
    "print(\"  • API names: CAN_VIEW may appear as CAN_READ in API responses\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a flattened reference DataFrame for export\n",
    "reference_data = []\n",
    "for resource_type, permissions in permission_definitions.items():\n",
    "    for perm_level, description in permissions.items():\n",
    "        reference_data.append({\n",
    "            'resource_type': resource_type,\n",
    "            'permission_level': perm_level,\n",
    "            'description': description\n",
    "        })\n",
    "\n",
    "permission_reference_df = spark.createDataFrame(reference_data)\n",
    "print(f\"\\n✓ Created permission_reference_df with {permission_reference_df.count()} permission definitions\")\n",
    "print(\"  (Available for export to Excel)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfcba266-d4a6-4bdc-aecf-ede24d43d50b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display permission reference table"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log(\"\\n=== Permission Levels Reference Table ===\")\n",
    "log(\"This table explains what each permission level allows users to do\\n\")\n",
    "\n",
    "if not is_job_mode:\n",
    "    display(permission_reference_df.orderBy('resource_type', 'permission_level'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3259e324-62e6-4ee9-a87e-21e4cb9e12cd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get service principals"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if ENABLE_SERVICE_PRINCIPALS:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching service principals...\")\n",
    "    \n",
    "    try:\n",
    "        service_principals = list(wc.service_principals.list())\n",
    "        \n",
    "        sp_data = []\n",
    "        for sp in service_principals:\n",
    "            sp_data.append({\n",
    "                'sp_id': sp.id,\n",
    "                'sp_application_id': sp.application_id,\n",
    "                'sp_display_name': sp.display_name,\n",
    "                'active': sp.active,\n",
    "                'groups': [g.display for g in sp.groups] if sp.groups else []\n",
    "            })\n",
    "        \n",
    "        if sp_data:\n",
    "            service_principals_df = spark.createDataFrame(sp_data)\n",
    "        else:\n",
    "            service_principals_df = spark.createDataFrame([], 'sp_id STRING, sp_application_id STRING, sp_display_name STRING, active BOOLEAN, groups ARRAY<STRING>')\n",
    "        \n",
    "        log(f\"✓ Found {service_principals_df.count()} service principals\")\n",
    "        \n",
    "        sp_export = service_principals_df.select(\n",
    "            'sp_id',\n",
    "            'sp_application_id', \n",
    "            'sp_display_name',\n",
    "            'active',\n",
    "            F.explode_outer('groups').alias('group_name')\n",
    "        )\n",
    "        \n",
    "        log(f\"✓ Created sp_export: {sp_export.count()} rows (flattened)\")\n",
    "        \n",
    "        if not is_job_mode:\n",
    "            display(service_principals_df.limit(20))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching service principals: {str(e)}\")\n",
    "        service_principals_df = spark.createDataFrame([], 'sp_id STRING, sp_application_id STRING, sp_display_name STRING, active BOOLEAN, groups ARRAY<STRING>')\n",
    "        sp_export = spark.createDataFrame([], 'sp_id STRING, sp_application_id STRING, sp_display_name STRING, active BOOLEAN, group_name STRING')\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get service principals\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Service principals collection disabled (ENABLE_SERVICE_PRINCIPALS=False)\")\n",
    "    service_principals_df = spark.createDataFrame([], 'sp_id STRING, sp_application_id STRING, sp_display_name STRING, active BOOLEAN, groups ARRAY<STRING>')\n",
    "    sp_export = spark.createDataFrame([], 'sp_id STRING, sp_application_id STRING, sp_display_name STRING, active BOOLEAN, group_name STRING')\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "534a02d3-d3b6-4751-958b-17fc4b1f8260",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get secret scopes and ACLs"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if ENABLE_SECRET_SCOPES:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching secret scopes and their ACLs...\")\n",
    "    \n",
    "    try:\n",
    "        secret_scopes = wc.secrets.list_scopes()\n",
    "        \n",
    "        secret_scope_data = []\n",
    "        secret_acl_data = []\n",
    "        \n",
    "        for scope in secret_scopes:\n",
    "            secret_scope_data.append({\n",
    "                'scope_name': scope.name,\n",
    "                'backend_type': scope.backend_type.value if scope.backend_type else 'UNKNOWN'\n",
    "            })\n",
    "            \n",
    "            try:\n",
    "                acls = wc.secrets.list_acls(scope=scope.name)\n",
    "                for acl in acls:\n",
    "                    secret_acl_data.append({\n",
    "                        'scope_name': scope.name,\n",
    "                        'principal': acl.principal,\n",
    "                        'permission': acl.permission.value if acl.permission else 'UNKNOWN'\n",
    "                    })\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        if secret_scope_data:\n",
    "            secret_scopes_df = spark.createDataFrame(secret_scope_data)\n",
    "        else:\n",
    "            secret_scopes_df = spark.createDataFrame([], 'scope_name STRING, backend_type STRING')\n",
    "        \n",
    "        if secret_acl_data:\n",
    "            secret_acls_df = spark.createDataFrame(secret_acl_data)\n",
    "        else:\n",
    "            secret_acls_df = spark.createDataFrame([], 'scope_name STRING, principal STRING, permission STRING')\n",
    "        \n",
    "        log(f\"✓ Found {secret_scopes_df.count()} secret scopes\")\n",
    "        log(f\"✓ Found {secret_acls_df.count()} secret ACL entries\")\n",
    "        \n",
    "        if not is_job_mode:\n",
    "            log(\"\\nSecret Scopes:\")\n",
    "            display(secret_scopes_df)\n",
    "            \n",
    "            if secret_acls_df.count() > 0:\n",
    "                log(\"\\nSecret ACLs:\")\n",
    "                display(secret_acls_df.limit(50))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching secret scopes: {str(e)}\")\n",
    "        secret_scopes_df = spark.createDataFrame([], 'scope_name STRING, backend_type STRING')\n",
    "        secret_acls_df = spark.createDataFrame([], 'scope_name STRING, principal STRING, permission STRING')\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get secret scopes and ACLs\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Secret scopes collection disabled (ENABLE_SECRET_SCOPES=False)\")\n",
    "    secret_scopes_df = spark.createDataFrame([], 'scope_name STRING, backend_type STRING')\n",
    "    secret_acls_df = spark.createDataFrame([], 'scope_name STRING, principal STRING, permission STRING')\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9afe02a5-ab57-4385-a801-9202a30f1aeb",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get Unity Catalog permissions (catalogs and schemas)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if ENABLE_UC_PERMISSIONS:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching Unity Catalog permissions...\")\n",
    "    \n",
    "    try:\n",
    "        catalogs = list(wc.catalogs.list())\n",
    "        \n",
    "        uc_catalog_data = []\n",
    "        uc_schema_data = []\n",
    "        uc_catalog_grants = []\n",
    "        uc_schema_grants = []\n",
    "        \n",
    "        log(f\"Found {len(catalogs)} catalogs\")\n",
    "        \n",
    "        for catalog in catalogs:\n",
    "            uc_catalog_data.append({\n",
    "                'catalog_name': catalog.name,\n",
    "                'catalog_owner': catalog.owner,\n",
    "                'catalog_type': catalog.catalog_type.value if catalog.catalog_type else 'UNKNOWN',\n",
    "                'created_at': catalog.created_at,\n",
    "                'updated_at': catalog.updated_at\n",
    "            })\n",
    "            \n",
    "            try:\n",
    "                grants = wc.grants.get_effective(securable_type='catalog', full_name=catalog.name)\n",
    "                if grants.privilege_assignments:\n",
    "                    for grant in grants.privilege_assignments:\n",
    "                        for privilege in grant.privileges:\n",
    "                            uc_catalog_grants.append({\n",
    "                                'catalog_name': catalog.name,\n",
    "                                'principal': grant.principal,\n",
    "                                'privilege': privilege.value\n",
    "                            })\n",
    "            except Exception:\n",
    "                pass\n",
    "            \n",
    "            try:\n",
    "                schemas = list(wc.schemas.list(catalog_name=catalog.name))\n",
    "                for schema in schemas[:20]:\n",
    "                    uc_schema_data.append({\n",
    "                        'catalog_name': catalog.name,\n",
    "                        'schema_name': schema.name,\n",
    "                        'schema_owner': schema.owner,\n",
    "                        'full_name': schema.full_name\n",
    "                    })\n",
    "                    \n",
    "                    try:\n",
    "                        schema_grants = wc.grants.get_effective(securable_type='schema', full_name=schema.full_name)\n",
    "                        if schema_grants.privilege_assignments:\n",
    "                            for grant in schema_grants.privilege_assignments:\n",
    "                                for privilege in grant.privileges:\n",
    "                                    uc_schema_grants.append({\n",
    "                                        'schema_full_name': schema.full_name,\n",
    "                                        'principal': grant.principal,\n",
    "                                        'privilege': privilege.value\n",
    "                                    })\n",
    "                    except Exception:\n",
    "                        pass\n",
    "            except Exception:\n",
    "                pass\n",
    "        \n",
    "        uc_catalogs_df = spark.createDataFrame(uc_catalog_data) if uc_catalog_data else spark.createDataFrame([], 'catalog_name STRING, catalog_owner STRING, catalog_type STRING, created_at BIGINT, updated_at BIGINT')\n",
    "        uc_schemas_df = spark.createDataFrame(uc_schema_data) if uc_schema_data else spark.createDataFrame([], 'catalog_name STRING, schema_name STRING, schema_owner STRING, full_name STRING')\n",
    "        uc_catalog_grants_df = spark.createDataFrame(uc_catalog_grants) if uc_catalog_grants else spark.createDataFrame([], 'catalog_name STRING, principal STRING, privilege STRING')\n",
    "        uc_schema_grants_df = spark.createDataFrame(uc_schema_grants) if uc_schema_grants else spark.createDataFrame([], 'schema_full_name STRING, principal STRING, privilege STRING')\n",
    "        \n",
    "        log(f\"\\n✓ Found {uc_catalogs_df.count()} catalogs\")\n",
    "        log(f\"✓ Found {uc_schemas_df.count()} schemas (sampled)\")\n",
    "        log(f\"✓ Found {uc_catalog_grants_df.count()} catalog grants\")\n",
    "        log(f\"✓ Found {uc_schema_grants_df.count()} schema grants\")\n",
    "        \n",
    "        if not is_job_mode:\n",
    "            log(\"\\nCatalogs:\")\n",
    "            display(uc_catalogs_df)\n",
    "            \n",
    "            if uc_catalog_grants_df.count() > 0:\n",
    "                log(\"\\nCatalog Grants (Top 50):\")\n",
    "                display(uc_catalog_grants_df.limit(50))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching Unity Catalog permissions: {str(e)}\")\n",
    "        uc_catalogs_df = spark.createDataFrame([], 'catalog_name STRING, catalog_owner STRING, catalog_type STRING, created_at BIGINT, updated_at BIGINT')\n",
    "        uc_schemas_df = spark.createDataFrame([], 'catalog_name STRING, schema_name STRING, schema_owner STRING, full_name STRING')\n",
    "        uc_catalog_grants_df = spark.createDataFrame([], 'catalog_name STRING, principal STRING, privilege STRING')\n",
    "        uc_schema_grants_df = spark.createDataFrame([], 'schema_full_name STRING, principal STRING, privilege STRING')\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log_execution_time(\"Get Unity Catalog permissions\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  Unity Catalog permissions collection disabled (ENABLE_UC_PERMISSIONS=False)\")\n",
    "    uc_catalogs_df = spark.createDataFrame([], 'catalog_name STRING, catalog_owner STRING, catalog_type STRING, created_at BIGINT, updated_at BIGINT')\n",
    "    uc_schemas_df = spark.createDataFrame([], 'catalog_name STRING, schema_name STRING, schema_owner STRING, full_name STRING')\n",
    "    uc_catalog_grants_df = spark.createDataFrame([], 'catalog_name STRING, principal STRING, privilege STRING')\n",
    "    uc_schema_grants_df = spark.createDataFrame([], 'schema_full_name STRING, principal STRING, privilege STRING')\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eac8942-d816-4b1e-9d9a-b7587c5cf358",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Get workspace settings and IP access lists"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if ENABLE_IP_ACCESS_LISTS:\n",
    "    cell_start_time = time.time()\n",
    "    \n",
    "    log(\"Fetching workspace settings and IP access lists...\")\n",
    "    \n",
    "    try:\n",
    "        ip_access_lists = list(wc.ip_access_lists.list())\n",
    "        \n",
    "        ip_acl_data = []\n",
    "        for ip_list in ip_access_lists:\n",
    "            ip_acl_data.append({\n",
    "                'list_id': ip_list.list_id,\n",
    "                'label': ip_list.label,\n",
    "                'list_type': ip_list.list_type.value if ip_list.list_type else 'UNKNOWN',\n",
    "                'enabled': ip_list.enabled,\n",
    "                'ip_addresses': str(ip_list.ip_addresses) if ip_list.ip_addresses else '[]',\n",
    "                'created_at': ip_list.created_at,\n",
    "                'created_by': ip_list.created_by\n",
    "            })\n",
    "        \n",
    "        if ip_acl_data:\n",
    "            ip_access_lists_df = spark.createDataFrame(ip_acl_data)\n",
    "        else:\n",
    "            ip_access_lists_df = spark.createDataFrame([], 'list_id STRING, label STRING, list_type STRING, enabled BOOLEAN, ip_addresses STRING, created_at BIGINT, created_by STRING')\n",
    "        \n",
    "        log(f\"✓ Found {ip_access_lists_df.count()} IP access lists\")\n",
    "        \n",
    "        if not is_job_mode:\n",
    "            if ip_access_lists_df.count() > 0:\n",
    "                display(ip_access_lists_df)\n",
    "            else:\n",
    "                log(\"  No IP access lists configured\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error fetching IP access lists: {str(e)}\")\n",
    "        ip_access_lists_df = spark.createDataFrame([], 'list_id STRING, label STRING, list_type STRING, enabled BOOLEAN, ip_addresses STRING, created_at BIGINT, created_by STRING')\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "    \n",
    "    log(\"\\nWorkspace Configuration:\")\n",
    "        \n",
    "    try:\n",
    "        workspace_conf = wc.workspace_conf.get_status(keys='enableTokensConfig,enableIpAccessLists')\n",
    "        log(f\"  Token creation enabled: {workspace_conf.get('enableTokensConfig', 'Unknown')}\")\n",
    "        log(f\"  IP access lists enabled: {workspace_conf.get('enableIpAccessLists', 'Unknown')}\")\n",
    "    except Exception as e:\n",
    "        log(f\"  ⚠️ Could not fetch workspace config: {str(e)}\")\n",
    "    \n",
    "    log_execution_time(\"Get workspace settings and IP access lists\", cell_start_time)\n",
    "else:\n",
    "    log(\"⏭️  IP access lists collection disabled (ENABLE_IP_ACCESS_LISTS=False)\")\n",
    "    ip_access_lists_df = spark.createDataFrame([], 'list_id STRING, label STRING, list_type STRING, enabled BOOLEAN, ip_addresses STRING, created_at BIGINT, created_by STRING')\n",
    "    execution_stats['resources_skipped'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e839c47-718c-4d42-8bd6-b87f109ab264",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Update export to include all security data"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log(\"Updating export lists to include all security data...\\n\")\n",
    "\n",
    "# Add all dataframes to export list\n",
    "additional_exports = [\n",
    "    ('service_principals', sp_export),\n",
    "    ('secret_scopes', secret_scopes_df),\n",
    "    ('secret_acls', secret_acls_df),\n",
    "    ('uc_catalogs', uc_catalogs_df),\n",
    "    ('uc_schemas', uc_schemas_df),\n",
    "    ('uc_catalog_grants', uc_catalog_grants_df),\n",
    "    ('uc_schema_grants', uc_schema_grants_df),\n",
    "    ('ip_access_lists', ip_access_lists_df)\n",
    "]\n",
    "\n",
    "# Add new resource types if they exist\n",
    "if 'workspace_permissions_df' in dir():\n",
    "    additional_exports.append(('workspace_permissions', workspace_permissions_df))\n",
    "if 'tokens_df' in dir():\n",
    "    additional_exports.append(('tokens', tokens_df))\n",
    "if 'uc_volume_grants_df' in dir():\n",
    "    additional_exports.append(('uc_volume_grants', uc_volume_grants_df))\n",
    "if 'new_permissions_df' in dir() and new_permissions_df.count() > 0:\n",
    "    additional_exports.append(('new_permissions', new_permissions_df))\n",
    "if 'removed_permissions_df' in dir() and removed_permissions_df.count() > 0:\n",
    "    additional_exports.append(('removed_permissions', removed_permissions_df))\n",
    "if 'workspace_admins' in dir():\n",
    "    additional_exports.append(('workspace_admins', workspace_admins))\n",
    "if 'inactive_users_with_perms' in dir():\n",
    "    additional_exports.append(('inactive_users_with_perms', inactive_users_with_perms))\n",
    "\n",
    "log(\"Additional security data available for export:\")\n",
    "for name, df in additional_exports:\n",
    "    log(f\"  - {name}: {df.count()} rows\")\n",
    "\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(\"COMPLETE SECURITY COVERAGE SUMMARY\")\n",
    "log(f\"{'='*60}\")\n",
    "log(\"\\n✓ Identity & Access:\")\n",
    "log(f\"  - Users: {users_df.count()}\")\n",
    "log(f\"  - Groups: {groups_df.count()}\")\n",
    "log(f\"  - Service Principals: {service_principals_df.count() if 'service_principals_df' in dir() else 0}\")\n",
    "log(f\"  - User-Group Memberships: {user_groups_df.count()}\")\n",
    "if 'workspace_admins' in dir():\n",
    "    log(f\"  - Workspace Admins: {workspace_admins.count()}\")\n",
    "\n",
    "log(\"\\n✓ Workspace Permissions:\")\n",
    "log(f\"  - Jobs: included\")\n",
    "log(f\"  - Warehouses: included\")\n",
    "log(f\"  - Clusters (interactive): included\")\n",
    "log(f\"  - Pipelines: included\")\n",
    "if 'workspace_permissions_df' in dir():\n",
    "    log(f\"  - Workspace Objects (folders/notebooks): {workspace_permissions_df.count()}\")\n",
    "log(f\"  - Repos (Git): included\")\n",
    "log(f\"  - Instance Pools: included\")\n",
    "log(f\"  - Model Registry: included\")\n",
    "log(f\"  - SQL Dashboards/Queries: included\")\n",
    "log(f\"  - Total permission entries: {permissions_df.count()}\")\n",
    "\n",
    "log(\"\\n✓ Unity Catalog:\")\n",
    "log(f\"  - Catalogs: {uc_catalogs_df.count() if 'uc_catalogs_df' in dir() else 0}\")\n",
    "log(f\"  - Schemas: {uc_schemas_df.count() if 'uc_schemas_df' in dir() else 0}\")\n",
    "log(f\"  - Catalog Grants: {uc_catalog_grants_df.count() if 'uc_catalog_grants_df' in dir() else 0}\")\n",
    "log(f\"  - Schema Grants: {uc_schema_grants_df.count() if 'uc_schema_grants_df' in dir() else 0}\")\n",
    "if 'uc_volume_grants_df' in dir():\n",
    "    log(f\"  - Volume Grants: {uc_volume_grants_df.count()}\")\n",
    "\n",
    "log(\"\\n✓ Secrets Management:\")\n",
    "log(f\"  - Secret Scopes: {secret_scopes_df.count() if 'secret_scopes_df' in dir() else 0}\")\n",
    "log(f\"  - Secret ACLs: {secret_acls_df.count() if 'secret_acls_df' in dir() else 0}\")\n",
    "\n",
    "log(\"\\n✓ Network Security:\")\n",
    "log(f\"  - IP Access Lists: {ip_access_lists_df.count() if 'ip_access_lists_df' in dir() else 0}\")\n",
    "\n",
    "log(\"\\n✓ Token Management:\")\n",
    "if 'tokens_df' in dir():\n",
    "    log(f\"  - Active Tokens: {tokens_df.count()}\")\n",
    "else:\n",
    "    log(f\"  - Active Tokens: Not available\")\n",
    "\n",
    "log(\"\\n✓ Change Detection:\")\n",
    "if 'new_permissions_df' in dir():\n",
    "    log(f\"  - New Permissions: {new_permissions_df.count()}\")\n",
    "    log(f\"  - Removed Permissions: {removed_permissions_df.count()}\")\n",
    "else:\n",
    "    log(f\"  - Change detection: Not enabled\")\n",
    "\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(\"\\nNote: Run remaining cells to export all data to Excel and/or Delta tables\")\n",
    "log(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f9067b7-fe54-4b93-857c-3f49b563e05b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export all security data to Excel"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export all security data to individual Excel files\n",
    "# This creates one Excel file per dataframe\n",
    "\n",
    "if ENABLE_EXCEL_EXPORT:\n",
    "    cell_start_time = time.time()\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    \n",
    "    # Create timestamp for filenames in Eastern Time\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "    base_path = EXPORT_PATH if 'EXPORT_PATH' in dir() else '/dbfs/tmp/permissions_export'\n",
    "    \n",
    "    log(f\"\\nExporting ALL security data to Excel...\\n\")\n",
    "    log(f\"Export location: {base_path}\")\n",
    "    log(f\"Timestamp (Eastern Time): {datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')}\\n\")\n",
    "    \n",
    "    # Complete export list with all security data\n",
    "    complete_export_list = [\n",
    "        ('permission_reference', permission_reference_df),\n",
    "        ('users_with_groups', users_export),\n",
    "        ('groups_with_members', groups_export),\n",
    "        ('service_principals', sp_export),\n",
    "        ('user_groups', user_groups_export),\n",
    "        ('permissions', permissions_export),\n",
    "        ('permissions_with_groups', permissions_with_groups_export),\n",
    "        ('user_all_permissions', user_all_permissions_export),\n",
    "        ('user_permissions_summary', user_perm_summary_export),\n",
    "        ('group_permissions_summary', group_summary_export),\n",
    "        ('secret_scopes', secret_scopes_df),\n",
    "        ('secret_acls', secret_acls_df),\n",
    "        ('uc_catalogs', uc_catalogs_df),\n",
    "        ('uc_schemas', uc_schemas_df),\n",
    "        ('uc_catalog_grants', uc_catalog_grants_df),\n",
    "        ('uc_schema_grants', uc_schema_grants_df),\n",
    "        ('ip_access_lists', ip_access_lists_df)\n",
    "    ]\n",
    "    \n",
    "    export_success = 0\n",
    "    export_failed = 0\n",
    "    \n",
    "    for name, df in complete_export_list:\n",
    "        try:\n",
    "            # Validate DataFrame before export\n",
    "            if df is None:\n",
    "                log(f\"⏭️  Skipping {name}: DataFrame is None\")\n",
    "                continue\n",
    "            \n",
    "            row_count = df.count()\n",
    "            if row_count == 0:\n",
    "                log(f\"⏭️  Skipping {name}: DataFrame is empty (0 rows)\")\n",
    "                continue\n",
    "            \n",
    "            # Convert to pandas\n",
    "            pdf = df.toPandas()\n",
    "            \n",
    "            # Convert any timestamp columns to Eastern Time\n",
    "            for col in pdf.columns:\n",
    "                if pd.api.types.is_datetime64_any_dtype(pdf[col]):\n",
    "                    if pdf[col].dt.tz is None:\n",
    "                        pdf[col] = pd.to_datetime(pdf[col]).dt.tz_localize('UTC').dt.tz_convert(eastern)\n",
    "                    else:\n",
    "                        pdf[col] = pdf[col].dt.tz_convert(eastern)\n",
    "            \n",
    "            # Create filename\n",
    "            filename = f\"{base_path}/{name}_{timestamp}.xlsx\"\n",
    "            \n",
    "            # Export to Excel\n",
    "            pdf.to_excel(filename, index=False, engine='openpyxl')\n",
    "            \n",
    "            log(f\"✓ Exported {name}: {len(pdf)} rows\")\n",
    "            export_success += 1\n",
    "        except Exception as e:\n",
    "            log(f\"✗ Error exporting {name}: {str(e)}\")\n",
    "            export_failed += 1\n",
    "            # In job mode, raise the exception to fail the job\n",
    "            if is_job_mode:\n",
    "                raise\n",
    "    \n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(f\"Individual file export complete: {export_success} succeeded, {export_failed} failed\")\n",
    "    log(f\"{'='*60}\")\n",
    "    log_execution_time(\"Export all security data to Excel\", cell_start_time)\n",
    "else:\n",
    "    log(\"\\n⏭️  Excel export disabled\")\n",
    "    log(\"   Set ENABLE_EXCEL_EXPORT = True in Cell 2 to enable Excel file generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4dd594-2f74-4f8c-a2bf-451b92c76e5e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create comprehensive security workbook"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create comprehensive security workbook with all data in one Excel file\n",
    "# This creates a single Excel file with multiple sheets\n",
    "\n",
    "if ENABLE_EXCEL_EXPORT:\n",
    "    cell_start_time = time.time()\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    \n",
    "    # Create timestamp for filename in Eastern Time\n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "    base_path = EXPORT_PATH if 'EXPORT_PATH' in dir() else '/dbfs/tmp/permissions_export'\n",
    "    combined_filename = f\"{base_path}/complete_security_review_{timestamp}.xlsx\"\n",
    "    \n",
    "    log(f\"\\nCreating comprehensive security review workbook...\")\n",
    "    log(f\"Timestamp (Eastern Time): {datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')}\\n\")\n",
    "    \n",
    "    try:\n",
    "        with pd.ExcelWriter(combined_filename, engine='openpyxl') as writer:\n",
    "            \n",
    "            # All sheets in logical order\n",
    "            all_sheets = [\n",
    "                ('Reference', permission_reference_df),\n",
    "                ('Users_Groups', users_export),\n",
    "                ('Groups_Members', groups_export),\n",
    "                ('Service_Principals', sp_export),\n",
    "                ('User_Groups', user_groups_export),\n",
    "                ('Permissions', permissions_export),\n",
    "                ('Perms_With_Groups', permissions_with_groups_export),\n",
    "                ('User_All_Perms', user_all_permissions_export),\n",
    "                ('User_Summary', user_perm_summary_export),\n",
    "                ('Group_Summary', group_summary_export),\n",
    "                ('Secret_Scopes', secret_scopes_df),\n",
    "                ('Secret_ACLs', secret_acls_df),\n",
    "                ('UC_Catalogs', uc_catalogs_df),\n",
    "                ('UC_Schemas', uc_schemas_df),\n",
    "                ('UC_Catalog_Grants', uc_catalog_grants_df),\n",
    "                ('UC_Schema_Grants', uc_schema_grants_df),\n",
    "                ('IP_Access_Lists', ip_access_lists_df)\n",
    "            ]\n",
    "            \n",
    "            # Add new resource types if they exist\n",
    "            if 'workspace_permissions_df' in dir():\n",
    "                all_sheets.append(('Workspace_Perms', workspace_permissions_df))\n",
    "            if 'tokens_df' in dir():\n",
    "                all_sheets.append(('Tokens', tokens_df))\n",
    "            if 'uc_volume_grants_df' in dir():\n",
    "                all_sheets.append(('UC_Volume_Grants', uc_volume_grants_df))\n",
    "            if 'workspace_admins' in dir():\n",
    "                all_sheets.append(('Workspace_Admins', workspace_admins))\n",
    "            if 'inactive_users_with_perms' in dir():\n",
    "                all_sheets.append(('Inactive_Users', inactive_users_with_perms))\n",
    "            if 'new_permissions_df' in dir() and new_permissions_df.count() > 0:\n",
    "                all_sheets.append(('New_Permissions', new_permissions_df))\n",
    "            if 'removed_permissions_df' in dir() and removed_permissions_df.count() > 0:\n",
    "                all_sheets.append(('Removed_Permissions', removed_permissions_df))\n",
    "            \n",
    "            sheets_added = 0\n",
    "            sheets_skipped = 0\n",
    "            \n",
    "            for sheet_name, df in all_sheets:\n",
    "                try:\n",
    "                    # Validate DataFrame before export\n",
    "                    if df is None:\n",
    "                        log(f\"⏭️  Skipping sheet {sheet_name}: DataFrame is None\")\n",
    "                        sheets_skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    row_count = df.count()\n",
    "                    if row_count == 0:\n",
    "                        log(f\"⏭️  Skipping sheet {sheet_name}: DataFrame is empty\")\n",
    "                        sheets_skipped += 1\n",
    "                        continue\n",
    "                    \n",
    "                    pdf = df.toPandas()\n",
    "                    \n",
    "                    # Convert timestamp columns to Eastern Time\n",
    "                    for col in pdf.columns:\n",
    "                        if pd.api.types.is_datetime64_any_dtype(pdf[col]):\n",
    "                            if pdf[col].dt.tz is None:\n",
    "                                pdf[col] = pd.to_datetime(pdf[col]).dt.tz_localize('UTC').dt.tz_convert(eastern)\n",
    "                            else:\n",
    "                                pdf[col] = pdf[col].dt.tz_convert(eastern)\n",
    "                    \n",
    "                    sheet_name_clean = sheet_name[:31]\n",
    "                    pdf.to_excel(writer, sheet_name=sheet_name_clean, index=False)\n",
    "                    \n",
    "                    log(f\"✓ Added sheet '{sheet_name_clean}': {len(pdf)} rows\")\n",
    "                    sheets_added += 1\n",
    "                except Exception as e:\n",
    "                    log(f\"✗ Error adding sheet {sheet_name}: {str(e)}\")\n",
    "                    sheets_skipped += 1\n",
    "        \n",
    "        log(f\"\\n{'='*60}\")\n",
    "        log(\"COMPREHENSIVE SECURITY WORKBOOK CREATED!\")\n",
    "        log(f\"{'='*60}\")\n",
    "        log(f\"\\nFile: {combined_filename}\")\n",
    "        log(f\"Sheets added: {sheets_added}, Sheets skipped: {sheets_skipped}\")\n",
    "        log(f\"\\nContains sheets covering:\")\n",
    "        log(\"• Users, Groups, Service Principals\")\n",
    "        log(\"• Workspace Resource Permissions (Jobs, Warehouses, Clusters, Pipelines)\")\n",
    "        log(\"• Workspace Objects (Folders, Notebooks, Repos)\")\n",
    "        log(\"• Model Registry Permissions\")\n",
    "        log(\"• Unity Catalog Permissions (Catalogs, Schemas, Volumes)\")\n",
    "        log(\"• Secret Scopes and ACLs\")\n",
    "        log(\"• IP Access Lists and Network Security\")\n",
    "        log(\"• Token Management\")\n",
    "        log(\"• Compliance Reports (Admins, Inactive Users, Change Detection)\")\n",
    "        log(\"• Permission Level Reference\")\n",
    "        log(f\"\\nTo download:\")\n",
    "        log(f\"  databricks fs cp {combined_filename.replace('/dbfs', 'dbfs:')} ./\")\n",
    "        \n",
    "        log_execution_time(\"Create comprehensive security workbook\", cell_start_time)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"\\n✗ Error creating workbook: {str(e)}\")\n",
    "        # In job mode, raise the exception to fail the job\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "else:\n",
    "    log(\"\\n⏭️  Comprehensive Excel workbook creation skipped\")\n",
    "    log(\"   Set ENABLE_EXCEL_EXPORT = True in Cell 2 to enable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a95c26d-9c87-44a4-b9ff-a0c9145673ca",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to CSV format (optional)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export security data to CSV format for easy import into other tools\n",
    "# CSV files are smaller and faster to generate than Excel\n",
    "\n",
    "# Configuration: Enable CSV export\n",
    "ENABLE_CSV_EXPORT = False  # Set to True to enable CSV export\n",
    "\n",
    "if ENABLE_CSV_EXPORT:\n",
    "    cell_start_time = time.time()\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    \n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "    csv_path = f\"{EXPORT_PATH}/csv_{timestamp}\"\n",
    "    \n",
    "    log(f\"\\nExporting security data to CSV format...\")\n",
    "    log(f\"Export location: {csv_path}\")\n",
    "    \n",
    "    # Create CSV export directory\n",
    "    dbutils.fs.mkdirs(csv_path.replace('/dbfs', 'dbfs:'))\n",
    "    \n",
    "    # List of DataFrames to export\n",
    "    csv_export_list = [\n",
    "        ('users_with_groups', users_export),\n",
    "        ('groups_with_members', groups_export),\n",
    "        ('permissions', permissions_export),\n",
    "        ('user_all_permissions', user_all_permissions_export),\n",
    "        ('workspace_admins', workspace_admins) if 'workspace_admins' in dir() else None,\n",
    "        ('inactive_users_with_perms', inactive_users_with_perms) if 'inactive_users_with_perms' in dir() else None,\n",
    "        ('new_permissions', new_permissions_df) if 'new_permissions_df' in dir() and new_permissions_df.count() > 0 else None,\n",
    "        ('removed_permissions', removed_permissions_df) if 'removed_permissions_df' in dir() and removed_permissions_df.count() > 0 else None\n",
    "    ]\n",
    "    \n",
    "    # Filter out None entries\n",
    "    csv_export_list = [(name, df) for name, df in csv_export_list if df is not None]\n",
    "    \n",
    "    csv_success = 0\n",
    "    for name, df in csv_export_list:\n",
    "        try:\n",
    "            if validate_dataframe_exists(name, df):\n",
    "                csv_file = f\"{csv_path}/{name}.csv\"\n",
    "                df.coalesce(1).write.mode('overwrite').option('header', 'true').csv(csv_file)\n",
    "                log(f\"✓ Exported {name}: {df.count()} rows\")\n",
    "                csv_success += 1\n",
    "        except Exception as e:\n",
    "            log(f\"✗ Error exporting {name} to CSV: {str(e)}\")\n",
    "    \n",
    "    log(f\"\\n✓ CSV export complete: {csv_success} files created\")\n",
    "    log(f\"Location: {csv_path}\")\n",
    "    log_execution_time(\"Export to CSV format\", cell_start_time)\n",
    "else:\n",
    "    log(\"\\n⏭️  CSV export disabled\")\n",
    "    log(\"   Set ENABLE_CSV_EXPORT = True to enable CSV export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df82e369-bec6-4ee7-a04b-1b5e3d127ec3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare executive summary dashboard data"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"EXECUTIVE SUMMARY DASHBOARD\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "# Collect all key metrics for executive summary\n",
    "summary_metrics = []\n",
    "\n",
    "# Identity metrics\n",
    "if 'users_df' in dir():\n",
    "    total_users = users_df.count()\n",
    "    active_users = users_df.filter(F.col('active') == True).count()\n",
    "    inactive_users = total_users - active_users\n",
    "    summary_metrics.extend([\n",
    "        {'Category': 'Identity', 'Metric': 'Total Users', 'Value': str(total_users), 'Status': '✓'},\n",
    "        {'Category': 'Identity', 'Metric': 'Active Users', 'Value': str(active_users), 'Status': '✓'},\n",
    "        {'Category': 'Identity', 'Metric': 'Inactive Users', 'Value': str(inactive_users), 'Status': '⚠️' if inactive_users > 0 else '✓'}\n",
    "    ])\n",
    "\n",
    "if 'groups_df' in dir():\n",
    "    total_groups = groups_df.count()\n",
    "    summary_metrics.append({'Category': 'Identity', 'Metric': 'Total Groups', 'Value': str(total_groups), 'Status': '✓'})\n",
    "\n",
    "if 'service_principals_df' in dir():\n",
    "    total_sps = service_principals_df.count()\n",
    "    summary_metrics.append({'Category': 'Identity', 'Metric': 'Service Principals', 'Value': str(total_sps), 'Status': '✓'})\n",
    "\n",
    "# Permission metrics\n",
    "if 'permissions_df' in dir():\n",
    "    total_permissions = permissions_df.count()\n",
    "    unique_resources = permissions_df.select('resource_id').distinct().count()\n",
    "    resource_types = permissions_df.select('resource_type').distinct().count()\n",
    "    summary_metrics.extend([\n",
    "        {'Category': 'Permissions', 'Metric': 'Total Permission Entries', 'Value': str(total_permissions), 'Status': '✓'},\n",
    "        {'Category': 'Permissions', 'Metric': 'Unique Resources', 'Value': str(unique_resources), 'Status': '✓'},\n",
    "        {'Category': 'Permissions', 'Metric': 'Resource Types Covered', 'Value': str(resource_types), 'Status': '✓'}\n",
    "    ])\n",
    "\n",
    "# Security alerts\n",
    "security_alert_count = 0\n",
    "\n",
    "if 'tokens_no_expiry' in dir():\n",
    "    tokens_no_expiry_count = tokens_no_expiry.count() if hasattr(tokens_no_expiry, 'count') else 0\n",
    "    summary_metrics.append({\n",
    "        'Category': 'Security Alerts', \n",
    "        'Metric': 'Tokens Without Expiry', \n",
    "        'Value': str(tokens_no_expiry_count), \n",
    "        'Status': '❌' if tokens_no_expiry_count > 0 else '✓'\n",
    "    })\n",
    "    security_alert_count += tokens_no_expiry_count\n",
    "\n",
    "if 'inactive_user_permissions_df' in dir():\n",
    "    inactive_perm_count = inactive_user_permissions_df.count() if hasattr(inactive_user_permissions_df, 'count') else 0\n",
    "    summary_metrics.append({\n",
    "        'Category': 'Security Alerts', \n",
    "        'Metric': 'Inactive User Permissions', \n",
    "        'Value': str(inactive_perm_count), \n",
    "        'Status': '❌' if inactive_perm_count > 0 else '✓'\n",
    "    })\n",
    "    security_alert_count += (1 if inactive_perm_count > 0 else 0)\n",
    "\n",
    "if 'external_users_df' in dir():\n",
    "    external_user_count = external_users_df.count() if hasattr(external_users_df, 'count') else 0\n",
    "    summary_metrics.append({\n",
    "        'Category': 'Security Alerts', \n",
    "        'Metric': 'External Users', \n",
    "        'Value': str(external_user_count), \n",
    "        'Status': '⚠️' if external_user_count > 0 else '✓'\n",
    "    })\n",
    "    security_alert_count += (1 if external_user_count > 0 else 0)\n",
    "\n",
    "if 'excessive_admin_permissions_df' in dir():\n",
    "    excessive_admin_count = excessive_admin_permissions_df.count() if hasattr(excessive_admin_permissions_df, 'count') else 0\n",
    "    summary_metrics.append({\n",
    "        'Category': 'Security Alerts', \n",
    "        'Metric': 'Users with Excessive Admin Permissions', \n",
    "        'Value': str(excessive_admin_count), \n",
    "        'Status': '⚠️' if excessive_admin_count > 0 else '✓'\n",
    "    })\n",
    "    security_alert_count += (1 if excessive_admin_count > 0 else 0)\n",
    "\n",
    "if 'sod_violations_df' in dir():\n",
    "    sod_count = sod_violations_df.count() if hasattr(sod_violations_df, 'count') else 0\n",
    "    summary_metrics.append({\n",
    "        'Category': 'Compliance', \n",
    "        'Metric': 'Potential SOD Violations', \n",
    "        'Value': str(sod_count), \n",
    "        'Status': '⚠️' if sod_count > 0 else '✓'\n",
    "    })\n",
    "\n",
    "# Execution statistics\n",
    "if 'execution_stats' in dir():\n",
    "    summary_metrics.extend([\n",
    "        {'Category': 'Execution', 'Metric': 'Total API Calls', 'Value': str(execution_stats.get('api_calls', 0)), 'Status': '✓'},\n",
    "        {'Category': 'Execution', 'Metric': 'API Failures', 'Value': str(execution_stats.get('api_failures', 0)), 'Status': '⚠️' if execution_stats.get('api_failures', 0) > 0 else '✓'},\n",
    "        {'Category': 'Execution', 'Metric': 'Resources Skipped', 'Value': str(execution_stats.get('resources_skipped', 0)), 'Status': 'ℹ️'}\n",
    "    ])\n",
    "\n",
    "# Calculate risk score (0-100, lower is better)\n",
    "risk_score = 0\n",
    "if 'tokens_no_expiry' in dir():\n",
    "    risk_score += min(tokens_no_expiry.count() * 5, 30)  # Up to 30 points\n",
    "if 'inactive_user_permissions_df' in dir():\n",
    "    risk_score += min(inactive_user_permissions_df.count() * 0.1, 20)  # Up to 20 points\n",
    "if 'external_users_df' in dir():\n",
    "    risk_score += min(external_users_df.count() * 2, 20)  # Up to 20 points\n",
    "if 'excessive_admin_permissions_df' in dir():\n",
    "    risk_score += min(excessive_admin_permissions_df.count() * 3, 30)  # Up to 30 points\n",
    "\n",
    "risk_score = min(risk_score, 100)\n",
    "risk_level = 'LOW' if risk_score < 30 else 'MEDIUM' if risk_score < 60 else 'HIGH'\n",
    "risk_status = '✓' if risk_score < 30 else '⚠️' if risk_score < 60 else '❌'\n",
    "\n",
    "summary_metrics.append({\n",
    "    'Category': 'Risk Assessment', \n",
    "    'Metric': 'Overall Risk Score (0-100)', \n",
    "    'Value': f\"{risk_score:.0f} ({risk_level})\", \n",
    "    'Status': risk_status\n",
    "})\n",
    "\n",
    "# Create summary DataFrame\n",
    "executive_summary_df = spark.createDataFrame(summary_metrics)\n",
    "\n",
    "log(\"\\n\uD83D\uDCCA Executive Summary Dashboard:\")\n",
    "log(f\"  Total metrics: {len(summary_metrics)}\")\n",
    "log(f\"  Security alerts: {security_alert_count}\")\n",
    "log(f\"  Risk score: {risk_score:.0f}/100 ({risk_level})\")\n",
    "\n",
    "if not is_job_mode:\n",
    "    display(executive_summary_df.orderBy('Category', 'Metric'))\n",
    "\n",
    "log_execution_time(\"Prepare executive summary dashboard\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf00d84-d84d-44d1-a87b-6ab448a07d10",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prepare summary dashboard for Excel export"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Prepare summary dashboard data for Excel export\n",
    "# This will be added as the FIRST sheet in the comprehensive workbook\n",
    "\n",
    "log(\"\\nPreparing summary dashboard for Excel export...\")\n",
    "\n",
    "# Convert executive summary to pandas for Excel export\n",
    "if 'executive_summary_df' in dir():\n",
    "    executive_summary_pandas = executive_summary_df.toPandas()\n",
    "    \n",
    "    # Add additional summary tables\n",
    "    \n",
    "    # 1. Permissions by resource type\n",
    "    if 'permissions_df' in dir():\n",
    "        perms_by_type = permissions_df.groupBy('resource_type') \\\n",
    "            .agg(\n",
    "                F.count('*').alias('permission_count'),\n",
    "                F.countDistinct('principal').alias('unique_principals'),\n",
    "                F.countDistinct('resource_id').alias('unique_resources')\n",
    "            ) \\\n",
    "            .orderBy(F.desc('permission_count')) \\\n",
    "            .toPandas()\n",
    "    else:\n",
    "        perms_by_type = pd.DataFrame()\n",
    "    \n",
    "    # 2. Top users by permission count\n",
    "    if 'permission_concentration_df' in dir():\n",
    "        top_users_summary = permission_concentration_df.limit(10).toPandas()\n",
    "    else:\n",
    "        top_users_summary = pd.DataFrame()\n",
    "    \n",
    "    # 3. Security alerts summary\n",
    "    security_alerts_data = []\n",
    "    \n",
    "    if 'tokens_no_expiry' in dir():\n",
    "        tokens_no_expiry_count = tokens_no_expiry.count() if hasattr(tokens_no_expiry, 'count') else 0\n",
    "        if tokens_no_expiry_count > 0:\n",
    "            security_alerts_data.append({\n",
    "                'Alert Type': 'Tokens Without Expiry',\n",
    "                'Severity': 'CRITICAL',\n",
    "                'Count': tokens_no_expiry_count,\n",
    "                'Recommendation': 'Set expiration dates for all tokens'\n",
    "            })\n",
    "    \n",
    "    if 'inactive_user_permissions_df' in dir():\n",
    "        inactive_perm_count = inactive_user_permissions_df.count() if hasattr(inactive_user_permissions_df, 'count') else 0\n",
    "        if inactive_perm_count > 0:\n",
    "            security_alerts_data.append({\n",
    "                'Alert Type': 'Inactive User Permissions',\n",
    "                'Severity': 'HIGH',\n",
    "                'Count': inactive_perm_count,\n",
    "                'Recommendation': 'Remove permissions for inactive users'\n",
    "            })\n",
    "    \n",
    "    if 'external_users_df' in dir():\n",
    "        external_user_count = external_users_df.count() if hasattr(external_users_df, 'count') else 0\n",
    "        if external_user_count > 0:\n",
    "            security_alerts_data.append({\n",
    "                'Alert Type': 'External Users',\n",
    "                'Severity': 'MEDIUM',\n",
    "                'Count': external_user_count,\n",
    "                'Recommendation': 'Review external user access justification'\n",
    "            })\n",
    "    \n",
    "    if 'excessive_admin_permissions_df' in dir():\n",
    "        excessive_admin_count = excessive_admin_permissions_df.count() if hasattr(excessive_admin_permissions_df, 'count') else 0\n",
    "        if excessive_admin_count > 0:\n",
    "            security_alerts_data.append({\n",
    "                'Alert Type': 'Excessive Admin Permissions',\n",
    "                'Severity': 'HIGH',\n",
    "                'Count': excessive_admin_count,\n",
    "                'Recommendation': 'Review and reduce admin permissions'\n",
    "            })\n",
    "    \n",
    "    if 'sod_violations_df' in dir():\n",
    "        sod_count = sod_violations_df.count() if hasattr(sod_violations_df, 'count') else 0\n",
    "        if sod_count > 0:\n",
    "            security_alerts_data.append({\n",
    "                'Alert Type': 'SOD Violations',\n",
    "                'Severity': 'MEDIUM',\n",
    "                'Count': sod_count,\n",
    "                'Recommendation': 'Separate development and production access'\n",
    "            })\n",
    "    \n",
    "    security_alerts_pandas = pd.DataFrame(security_alerts_data) if security_alerts_data else pd.DataFrame()\n",
    "    \n",
    "    # 4. Execution metadata\n",
    "    execution_metadata = pd.DataFrame([{\n",
    "        'Metric': 'Execution Date',\n",
    "        'Value': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }, {\n",
    "        'Metric': 'Execution Mode',\n",
    "        'Value': 'JOB' if is_job_mode else 'INTERACTIVE'\n",
    "    }, {\n",
    "        'Metric': 'Total Execution Time',\n",
    "        'Value': f\"{execution_stats.get('total_execution_time', 0):.2f} seconds\" if 'execution_stats' in dir() else 'N/A'\n",
    "    }, {\n",
    "        'Metric': 'API Calls',\n",
    "        'Value': str(execution_stats.get('api_calls', 0)) if 'execution_stats' in dir() else 'N/A'\n",
    "    }, {\n",
    "        'Metric': 'Success Rate',\n",
    "        'Value': f\"{execution_stats.get('success_rate', 0):.1f}%\" if 'execution_stats' in dir() else 'N/A'\n",
    "    }])\n",
    "    \n",
    "    log(f\"✓ Summary dashboard data prepared\")\n",
    "    log(f\"  Executive summary: {len(executive_summary_pandas)} metrics\")\n",
    "    log(f\"  Permissions by type: {len(perms_by_type)} resource types\")\n",
    "    log(f\"  Security alerts: {len(security_alerts_pandas)} alerts\")\n",
    "    log(f\"  Top users: {len(top_users_summary)} users\")\n",
    "    \n",
    "    # Store for Excel export\n",
    "    excel_summary_data = {\n",
    "        'executive_summary': executive_summary_pandas,\n",
    "        'permissions_by_type': perms_by_type,\n",
    "        'top_users': top_users_summary,\n",
    "        'security_alerts': security_alerts_pandas,\n",
    "        'execution_metadata': execution_metadata\n",
    "    }\n",
    "    \n",
    "    log(\"\\nℹ️  NOTE: To add these to Excel export, update Cell 43 (comprehensive workbook)\")\n",
    "    log(\"   Add these sheets FIRST in the workbook for executive visibility:\")\n",
    "    log(\"   1. Executive Summary\")\n",
    "    log(\"   2. Security Alerts\")\n",
    "    log(\"   3. Permissions by Type\")\n",
    "    log(\"   4. Top Users\")\n",
    "    log(\"   5. Execution Metadata\")\n",
    "else:\n",
    "    log(\"⚠️  Executive summary data not available\")\n",
    "    excel_summary_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3370610-1632-422d-8c91-edcbe8b33443",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to JSON format (optional)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export security data to JSON format for API consumption and programmatic access\n",
    "\n",
    "# Configuration: Enable JSON export\n",
    "ENABLE_JSON_EXPORT = False  # Set to True to enable JSON export\n",
    "\n",
    "if ENABLE_JSON_EXPORT:\n",
    "    cell_start_time = time.time()\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    import json\n",
    "    \n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "    json_path = f\"{EXPORT_PATH}/json_{timestamp}\"\n",
    "    \n",
    "    log(f\"\\nExporting security data to JSON format...\")\n",
    "    log(f\"Export location: {json_path}\")\n",
    "    \n",
    "    # Create JSON export directory\n",
    "    dbutils.fs.mkdirs(json_path.replace('/dbfs', 'dbfs:'))\n",
    "    \n",
    "    # Export key DataFrames to JSON\n",
    "    json_export_list = [\n",
    "        ('permissions', permissions_export),\n",
    "        ('users', users_export),\n",
    "        ('groups', groups_export),\n",
    "        ('compliance_report', {\n",
    "            'inactive_users_with_permissions': inactive_users_with_perms.count() if 'inactive_users_with_perms' in dir() else 0,\n",
    "            'external_users': external_count if 'external_count' in dir() else 0,\n",
    "            'workspace_admins': workspace_admins.count() if 'workspace_admins' in dir() else 0,\n",
    "            'orphaned_permissions': (orphaned_user_count + orphaned_group_count) if 'orphaned_user_count' in dir() else 0,\n",
    "            'tokens_without_expiry': tokens_df.filter(F.col('expiry_time').isNull()).count() if 'tokens_df' in dir() and tokens_df.count() > 0 else 0\n",
    "        })\n",
    "    ]\n",
    "    \n",
    "    json_success = 0\n",
    "    for name, data in json_export_list:\n",
    "        try:\n",
    "            json_file = f\"{json_path}/{name}.json\"\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                # Export dictionary as JSON\n",
    "                dbutils.fs.put(json_file.replace('/dbfs', 'dbfs:'), json.dumps(data, indent=2), overwrite=True)\n",
    "                log(f\"✓ Exported {name}: compliance summary\")\n",
    "                json_success += 1\n",
    "            else:\n",
    "                # Export DataFrame as JSON\n",
    "                if validate_dataframe_exists(name, data):\n",
    "                    data.coalesce(1).write.mode('overwrite').json(json_file)\n",
    "                    log(f\"✓ Exported {name}: {data.count()} rows\")\n",
    "                    json_success += 1\n",
    "        except Exception as e:\n",
    "            log(f\"✗ Error exporting {name} to JSON: {str(e)}\")\n",
    "    \n",
    "    log(f\"\\n✓ JSON export complete: {json_success} files created\")\n",
    "    log(f\"Location: {json_path}\")\n",
    "    log_execution_time(\"Export to JSON format\", cell_start_time)\n",
    "else:\n",
    "    log(\"\\n⏭️  JSON export disabled\")\n",
    "    log(\"   Set ENABLE_JSON_EXPORT = True to enable JSON export\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f8c2c69-ae96-4cc9-b8a5-4e086507ad89",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to Delta tables for long-term retention"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Export security audit data to Delta tables for long-term retention and historical analysis\n",
    "# This enables tracking changes over multiple audit runs\n",
    "\n",
    "if ENABLE_DELTA_EXPORT:\n",
    "    cell_start_time = time.time()\n",
    "    from pyspark.sql.functions import current_timestamp, lit\n",
    "    \n",
    "    log(\"\\n\uD83D\uDCBE Exporting Security Audit Data to Delta Tables\")\n",
    "    log(f\"   Target table: {DELTA_TABLE_NAME}\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Validate that permissions_export exists and has data\n",
    "        if not validate_dataframe_exists(\"permissions_export\", permissions_export):\n",
    "            log(\"❌ Cannot export to Delta: permissions_export is empty or invalid\")\n",
    "            if is_job_mode:\n",
    "                raise ValueError(\"permissions_export DataFrame is empty or invalid\")\n",
    "        else:\n",
    "            # Prepare the main permissions export with metadata\n",
    "            permissions_export_delta = permissions_export \\\n",
    "                .withColumn('audit_run_timestamp', current_timestamp()) \\\n",
    "                .withColumn('max_resources_checked', lit(MAX_RESOURCES_PER_TYPE))\n",
    "            \n",
    "            # Write main permissions history to Delta table\n",
    "            log(f\"Writing {permissions_export_delta.count()} records to {DELTA_TABLE_NAME}...\")\n",
    "            permissions_export_delta.write \\\n",
    "                .format('delta') \\\n",
    "                .mode('append') \\\n",
    "                .option('mergeSchema', 'true') \\\n",
    "                .saveAsTable(DELTA_TABLE_NAME)\n",
    "            \n",
    "            log(f\"\\n✓ Successfully exported permissions history\")\n",
    "            log(f\"   Table: {DELTA_TABLE_NAME}\")\n",
    "            log(f\"   Mode: APPEND (historical accumulation)\")\n",
    "            log(f\"   Schema merge: ENABLED\")\n",
    "        \n",
    "        # Create snapshot tables for current state\n",
    "        snapshot_base = DELTA_TABLE_NAME.rsplit('.', 1)[0]  # Get catalog.schema\n",
    "        snapshot_count = 0\n",
    "        snapshot_tables = []\n",
    "        \n",
    "        # Core snapshots\n",
    "        snapshot_configs = [\n",
    "            ('users_export', 'users_snapshot'),\n",
    "            ('groups_export', 'groups_snapshot'),\n",
    "            ('user_perm_summary_export', 'user_permissions_summary_snapshot'),\n",
    "            ('group_summary_export', 'group_permissions_summary_snapshot'),\n",
    "            ('sp_export', 'service_principals_snapshot'),\n",
    "            ('uc_catalog_grants_export', 'uc_catalog_grants_snapshot'),\n",
    "            ('secret_acls_export', 'secret_acls_snapshot'),\n",
    "            ('workspace_permissions_df', 'workspace_permissions_snapshot'),\n",
    "            ('tokens_df', 'tokens_snapshot'),\n",
    "            ('uc_volume_grants_df', 'uc_volume_grants_snapshot'),\n",
    "            ('workspace_admins', 'workspace_admins_snapshot'),\n",
    "            ('inactive_users_with_perms', 'inactive_users_snapshot')\n",
    "        ]\n",
    "        \n",
    "        for df_name, table_suffix in snapshot_configs:\n",
    "            if df_name in dir() and validate_dataframe_exists(df_name, eval(df_name)):\n",
    "                try:\n",
    "                    snapshot_table = f\"{snapshot_base}.{table_suffix}\"\n",
    "                    eval(df_name) \\\n",
    "                        .withColumn('snapshot_timestamp', current_timestamp()) \\\n",
    "                        .write \\\n",
    "                        .format('delta') \\\n",
    "                        .mode('overwrite') \\\n",
    "                        .option('overwriteSchema', 'true') \\\n",
    "                        .saveAsTable(snapshot_table)\n",
    "                    snapshot_tables.append(snapshot_table)\n",
    "                    snapshot_count += 1\n",
    "                except Exception as e:\n",
    "                    log(f\"  ⚠️ Could not create snapshot for {df_name}: {str(e)}\")\n",
    "        \n",
    "        # Export change detection results if available\n",
    "        if 'new_permissions_df' in dir() and new_permissions_df.count() > 0:\n",
    "            try:\n",
    "                change_table = f\"{snapshot_base}.permission_changes_snapshot\"\n",
    "                new_permissions_df \\\n",
    "                    .withColumn('change_type', lit('NEW')) \\\n",
    "                    .withColumn('snapshot_timestamp', current_timestamp()) \\\n",
    "                    .union(\n",
    "                        removed_permissions_df \\\n",
    "                            .withColumn('change_type', lit('REMOVED')) \\\n",
    "                            .withColumn('snapshot_timestamp', current_timestamp())\n",
    "                    ) \\\n",
    "                    .write \\\n",
    "                    .format('delta') \\\n",
    "                    .mode('overwrite') \\\n",
    "                    .option('overwriteSchema', 'true') \\\n",
    "                    .saveAsTable(change_table)\n",
    "                snapshot_tables.append(change_table)\n",
    "                snapshot_count += 1\n",
    "            except Exception as e:\n",
    "                log(f\"  ⚠️ Could not create change detection snapshot: {str(e)}\")\n",
    "        \n",
    "        log(f\"\\n✓ Successfully created {snapshot_count} snapshot tables\")\n",
    "        if snapshot_tables:\n",
    "            log(\"\\nSnapshot tables created:\")\n",
    "            for table in snapshot_tables:\n",
    "                log(f\"   - {table}\")\n",
    "        \n",
    "        log(\"\\n\uD83D\uDCCA Query your security audit history:\")\n",
    "        log(f\"   -- View all permissions from today's audit run\")\n",
    "        log(f\"   SELECT * FROM {DELTA_TABLE_NAME}\")\n",
    "        log(f\"   WHERE audit_run_timestamp >= current_date()\")\n",
    "        log(f\"   ORDER BY audit_run_timestamp DESC;\")\n",
    "        log(f\"\")\n",
    "        log(f\"   -- Compare permissions over time\")\n",
    "        log(f\"   SELECT principal, resource_type, resource_name, permission_level,\")\n",
    "        log(f\"          audit_run_timestamp\")\n",
    "        log(f\"   FROM {DELTA_TABLE_NAME}\")\n",
    "        log(f\"   WHERE principal = 'user@example.com'\")\n",
    "        log(f\"   ORDER BY audit_run_timestamp DESC;\")\n",
    "        log(f\"\")\n",
    "        log(f\"   -- View permission changes\")\n",
    "        log(f\"   SELECT * FROM {snapshot_base}.permission_changes_snapshot\")\n",
    "        log(f\"   WHERE change_type = 'NEW';\")\n",
    "        \n",
    "        log_execution_time(\"Export to Delta tables\", cell_start_time)\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"\\n❌ Failed to export to Delta tables: {str(e)}\")\n",
    "        log(f\"   Please verify:\")\n",
    "        log(f\"   1. You have CREATE TABLE permissions\")\n",
    "        log(f\"   2. The catalog and schema exist: {DELTA_TABLE_NAME.rsplit('.', 1)[0]}\")\n",
    "        log(f\"   3. The table name is valid\")\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "else:\n",
    "    log(\"\\n⏭️  Delta table export disabled\")\n",
    "    log(\"   Set ENABLE_DELTA_EXPORT = True in Cell 2 to enable long-term retention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0cd7915-229b-4ff0-bbef-658511d302e9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Flatten and prepare dataframes for Excel export"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"Preparing dataframes for Excel export (flattening nested columns)...\\n\")\n",
    "\n",
    "# 1. Users with groups - explode groups array\n",
    "users_export = users_df.join(user_groups_df, users_df.user_name == user_groups_df.user_name, 'left') \\\n",
    "    .select(\n",
    "        users_df.user_name,\n",
    "        users_df.display_name,\n",
    "        users_df.active,\n",
    "        user_groups_df.group_name\n",
    "    )\n",
    "\n",
    "log(f\"✓ users_export: {users_export.count()} rows (users with their groups, one row per user-group)\")\n",
    "\n",
    "# 2. Groups with members - explode members\n",
    "groups_export = groups_df.join(user_groups_df, groups_df.group_name == user_groups_df.group_name, 'left') \\\n",
    "    .select(\n",
    "        groups_df.group_name,\n",
    "        groups_df.group_id,\n",
    "        user_groups_df.user_name.alias('member_user_name')\n",
    "    )\n",
    "\n",
    "log(f\"✓ groups_export: {groups_export.count()} rows (groups with their members, one row per group-member)\")\n",
    "\n",
    "# 3. User-group memberships (already flat)\n",
    "user_groups_export = user_groups_df\n",
    "log(f\"✓ user_groups_export: {user_groups_export.count()} rows (already flat)\")\n",
    "\n",
    "# 4. Permissions (already flat)\n",
    "permissions_export = permissions_df\n",
    "log(f\"✓ permissions_export: {permissions_df.count()} rows (already flat)\")\n",
    "\n",
    "# 5. Permissions with groups (already flat)\n",
    "permissions_with_groups_export = permissions_with_groups_df\n",
    "log(f\"✓ permissions_with_groups_export: {permissions_with_groups_df.count()} rows (already flat)\")\n",
    "\n",
    "# 6. User all permissions (direct + inherited) - already flat\n",
    "user_all_permissions_export = user_all_permissions_df\n",
    "log(f\"✓ user_all_permissions_export: {user_all_permissions_df.count()} rows (already flat)\")\n",
    "\n",
    "# 7. User permissions summary - explode permission_levels and inherited_from_groups\n",
    "user_perm_summary_flat = user_all_permissions_df.groupBy('principal').agg(\n",
    "    F.count('*').alias('total_permissions'),\n",
    "    F.sum(F.when(F.col('permission_source') == 'direct', 1).otherwise(0)).alias('direct_permissions'),\n",
    "    F.sum(F.when(F.col('permission_source') == 'inherited', 1).otherwise(0)).alias('inherited_permissions'),\n",
    "    F.countDistinct('resource_type').alias('resource_types_count')\n",
    ")\n",
    "\n",
    "# Add permission levels as separate rows\n",
    "user_perm_levels = user_all_permissions_df.select('principal', 'permission_level').distinct()\n",
    "user_perm_summary_export = user_perm_summary_flat.join(\n",
    "    user_perm_levels, 'principal', 'left'\n",
    ")\n",
    "\n",
    "# Add inherited groups as separate rows\n",
    "user_inherited_groups = user_all_permissions_df \\\n",
    "    .filter(F.col('permission_source') == 'inherited') \\\n",
    "    .select('principal', 'source_group').distinct()\n",
    "\n",
    "user_perm_summary_export = user_perm_summary_export.join(\n",
    "    user_inherited_groups, 'principal', 'left'\n",
    ")\n",
    "\n",
    "log(f\"✓ user_perm_summary_export: {user_perm_summary_export.count()} rows (flattened)\")\n",
    "\n",
    "# 8. Group permissions summary - flatten\n",
    "group_perm_summary_flat = group_permissions.groupBy('principal').agg(\n",
    "    F.count('*').alias('total_permissions'),\n",
    "    F.countDistinct('resource_type').alias('resource_types_count')\n",
    ")\n",
    "\n",
    "# Add permission levels\n",
    "group_perm_levels = group_permissions.select('principal', 'permission_level').distinct()\n",
    "group_summary_export = group_perm_summary_flat.join(\n",
    "    group_perm_levels, 'principal', 'left'\n",
    ")\n",
    "\n",
    "# Add member count\n",
    "group_members_count = user_groups_df.groupBy('group_name').agg(\n",
    "    F.count('*').alias('member_count')\n",
    ")\n",
    "\n",
    "group_summary_export = group_summary_export \\\n",
    "    .join(group_members_count, group_summary_export.principal == group_members_count.group_name, 'left') \\\n",
    "    .select(\n",
    "        F.col('principal').alias('group_name'),\n",
    "        F.col('total_permissions'),\n",
    "        F.col('resource_types_count'),\n",
    "        F.col('permission_level'),\n",
    "        F.coalesce(F.col('member_count'), F.lit(0)).alias('member_count')\n",
    "    )\n",
    "\n",
    "log(f\"✓ group_summary_export: {group_summary_export.count()} rows (flattened)\")\n",
    "\n",
    "# 9. Flatten additional exports\n",
    "if 'uc_catalog_grants_df' in dir():\n",
    "    uc_catalog_grants_export = uc_catalog_grants_df\n",
    "    log(f\"✓ uc_catalog_grants_export: {uc_catalog_grants_export.count()} rows\")\n",
    "\n",
    "if 'uc_schema_grants_df' in dir():\n",
    "    uc_schema_grants_export = uc_schema_grants_df\n",
    "    log(f\"✓ uc_schema_grants_export: {uc_schema_grants_export.count()} rows\")\n",
    "\n",
    "if 'secret_acls_df' in dir():\n",
    "    secret_acls_export = secret_acls_df\n",
    "    log(f\"✓ secret_acls_export: {secret_acls_export.count()} rows\")\n",
    "\n",
    "log(f\"\\n{'='*60}\")\n",
    "log(\"All dataframes prepared for export!\")\n",
    "log(f\"{'='*60}\")\n",
    "\n",
    "log_execution_time(\"Flatten and prepare dataframes\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e93107dc-b38c-4a61-9134-8f9eb05ee604",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export to Excel files"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Create timestamp for filenames in Eastern Time\n",
    "eastern = pytz.timezone('America/New_York')\n",
    "timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "base_path = EXPORT_PATH if 'EXPORT_PATH' in dir() else '/dbfs/tmp/permissions_export'\n",
    "\n",
    "if not is_job_mode:\n",
    "    print(f\"Exporting dataframes to Excel files...\\n\")\n",
    "    print(f\"Export location: {base_path}\")\n",
    "    print(f\"Timestamp (Eastern Time): {datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')}\\n\")\n",
    "\n",
    "# Export each dataframe\n",
    "export_list = [\n",
    "    ('permission_reference', permission_reference_df),\n",
    "    ('users_with_groups', users_export),\n",
    "    ('groups_with_members', groups_export),\n",
    "    ('user_groups', user_groups_export),\n",
    "    ('permissions', permissions_export),\n",
    "    ('permissions_with_groups', permissions_with_groups_export),\n",
    "    ('user_all_permissions', user_all_permissions_export),\n",
    "    ('user_permissions_summary', user_perm_summary_export),\n",
    "    ('group_permissions_summary', group_summary_export)\n",
    "]\n",
    "\n",
    "for name, df in export_list:\n",
    "    try:\n",
    "        # Convert to pandas\n",
    "        pdf = df.toPandas()\n",
    "        \n",
    "        # Convert any timestamp columns to Eastern Time\n",
    "        for col in pdf.columns:\n",
    "            if pd.api.types.is_datetime64_any_dtype(pdf[col]):\n",
    "                # Convert to Eastern Time if it's a datetime column\n",
    "                if pdf[col].dt.tz is None:\n",
    "                    # If naive, assume UTC and convert\n",
    "                    pdf[col] = pd.to_datetime(pdf[col]).dt.tz_localize('UTC').dt.tz_convert(eastern)\n",
    "                else:\n",
    "                    # If already timezone-aware, just convert\n",
    "                    pdf[col] = pdf[col].dt.tz_convert(eastern)\n",
    "        \n",
    "        # Create filename\n",
    "        filename = f\"{base_path}/{name}_{timestamp}.xlsx\"\n",
    "        \n",
    "        # Export to Excel\n",
    "        pdf.to_excel(filename, index=False, engine='openpyxl')\n",
    "        \n",
    "        if not is_job_mode:\n",
    "            print(f\"✓ Exported {name}: {len(pdf)} rows → {filename}\")\n",
    "    except Exception as e:\n",
    "        if not is_job_mode:\n",
    "            print(f\"✗ Error exporting {name}: {str(e)}\")\n",
    "        # In job mode, raise the exception to fail the job\n",
    "        if is_job_mode:\n",
    "            raise\n",
    "\n",
    "if not is_job_mode:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Export complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nFiles saved to: {base_path}/\")\n",
    "    print(f\"\\nTo download files, use the Databricks file browser or CLI:\")\n",
    "    print(f\"  databricks fs cp -r {base_path.replace('/dbfs', 'dbfs:')} ./local_folder/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abde56b8-29ad-4971-9e4b-1ef431ed3729",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Create combined Excel workbook (all sheets in one file)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Create timestamp for filename in Eastern Time\n",
    "eastern = pytz.timezone('America/New_York')\n",
    "timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "base_path = '/dbfs/tmp/permissions_export'\n",
    "combined_filename = f\"{base_path}/all_permissions_{timestamp}.xlsx\"\n",
    "\n",
    "if not is_job_mode:\n",
    "    print(f\"Creating combined Excel workbook with all sheets...\")\n",
    "    print(f\"Timestamp (Eastern Time): {datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')}\\n\")\n",
    "\n",
    "try:\n",
    "    # Create Excel writer\n",
    "    with pd.ExcelWriter(combined_filename, engine='openpyxl') as writer:\n",
    "        \n",
    "        # Export each dataframe as a separate sheet\n",
    "        sheets = [\n",
    "            ('Reference', permission_reference_df),\n",
    "            ('Users_Groups', users_export),\n",
    "            ('Groups_Members', groups_export),\n",
    "            ('User_Groups', user_groups_export),\n",
    "            ('Permissions', permissions_export),\n",
    "            ('Perms_With_Groups', permissions_with_groups_export),\n",
    "            ('User_All_Perms', user_all_permissions_export),\n",
    "            ('User_Summary', user_perm_summary_export),\n",
    "            ('Group_Summary', group_summary_export)\n",
    "        ]\n",
    "        \n",
    "        for sheet_name, df in sheets:\n",
    "            # Convert to pandas\n",
    "            pdf = df.toPandas()\n",
    "            \n",
    "            # Convert any timestamp columns to Eastern Time\n",
    "            for col in pdf.columns:\n",
    "                if pd.api.types.is_datetime64_any_dtype(pdf[col]):\n",
    "                    # Convert to Eastern Time if it's a datetime column\n",
    "                    if pdf[col].dt.tz is None:\n",
    "                        # If naive, assume UTC and convert\n",
    "                        pdf[col] = pd.to_datetime(pdf[col]).dt.tz_localize('UTC').dt.tz_convert(eastern)\n",
    "                    else:\n",
    "                        # If already timezone-aware, just convert\n",
    "                        pdf[col] = pdf[col].dt.tz_convert(eastern)\n",
    "            \n",
    "            # Write to sheet (Excel sheet names limited to 31 chars)\n",
    "            sheet_name_clean = sheet_name[:31]\n",
    "            pdf.to_excel(writer, sheet_name=sheet_name_clean, index=False)\n",
    "            \n",
    "            if not is_job_mode:\n",
    "                print(f\"✓ Added sheet '{sheet_name_clean}': {len(pdf)} rows\")\n",
    "    \n",
    "    if not is_job_mode:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Combined workbook created successfully!\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nFile: {combined_filename}\")\n",
    "        print(f\"\\nTo download:\")\n",
    "        print(f\"  databricks fs cp dbfs:/tmp/permissions_export/all_permissions_{timestamp}.xlsx ./\")\n",
    "    \n",
    "except Exception as e:\n",
    "    if not is_job_mode:\n",
    "        print(f\"\\n✗ Error creating combined workbook: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5a0aa2f-f397-4c1a-959d-3876d9dfb731",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Job completion summary and status"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Job completion summary with execution statistics\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Calculate total execution time\n",
    "total_execution_time = time.time() - execution_stats['start_time']\n",
    "\n",
    "# Create completion summary\n",
    "eastern = pytz.timezone('America/New_York')\n",
    "completion_time = datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "\n",
    "summary = {\n",
    "    'status': 'SUCCESS',\n",
    "    'completion_time': completion_time,\n",
    "    'execution_time_seconds': round(total_execution_time, 2),\n",
    "    'execution_time_minutes': round(total_execution_time / 60, 2),\n",
    "    'mode': 'JOB' if is_job_mode else 'INTERACTIVE',\n",
    "    'configuration': {\n",
    "        'max_resources_per_type': MAX_RESOURCES_PER_TYPE,\n",
    "        'max_workers': MAX_WORKERS,\n",
    "        'max_retries': MAX_RETRIES,\n",
    "        'export_path': EXPORT_PATH if 'EXPORT_PATH' in dir() else '/dbfs/tmp/permissions_export',\n",
    "        'excel_export_enabled': ENABLE_EXCEL_EXPORT,\n",
    "        'delta_export_enabled': ENABLE_DELTA_EXPORT\n",
    "    },\n",
    "    'data_collected': {\n",
    "        'users': users_df.count() if 'users_df' in dir() else 0,\n",
    "        'groups': groups_df.count() if 'groups_df' in dir() else 0,\n",
    "        'user_groups': user_groups_df.count() if 'user_groups_df' in dir() else 0,\n",
    "        'permissions': permissions_df.count() if 'permissions_df' in dir() else 0,\n",
    "        'service_principals': service_principals_df.count() if 'service_principals_df' in dir() else 0,\n",
    "        'secret_scopes': secret_scopes_df.count() if 'secret_scopes_df' in dir() else 0,\n",
    "        'uc_catalogs': uc_catalogs_df.count() if 'uc_catalogs_df' in dir() else 0,\n",
    "        'workspace_permissions': workspace_permissions_df.count() if 'workspace_permissions_df' in dir() else 0,\n",
    "        'tokens': tokens_df.count() if 'tokens_df' in dir() else 0,\n",
    "        'uc_volumes': uc_volume_grants_df.count() if 'uc_volume_grants_df' in dir() else 0,\n",
    "        'workspace_admins': workspace_admins.count() if 'workspace_admins' in dir() else 0\n",
    "    },\n",
    "    'execution_stats': {\n",
    "        'api_calls': execution_stats['api_calls'],\n",
    "        'api_failures': execution_stats['api_failures'],\n",
    "        'api_retries': execution_stats['api_retries'],\n",
    "        'resources_processed': execution_stats['resources_processed'],\n",
    "        'success_rate_percent': round(((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls'] * 100), 2) if execution_stats['api_calls'] > 0 else 0\n",
    "    },\n",
    "    'compliance_alerts': {\n",
    "        'inactive_users_with_permissions': inactive_users_with_perms.select('user_name').distinct().count() if 'inactive_users_with_perms' in dir() else 0,\n",
    "        'orphaned_permissions': (orphaned_user_count + orphaned_group_count) if 'orphaned_user_count' in dir() and 'orphaned_group_count' in dir() else 0,\n",
    "        'tokens_without_expiry': tokens_df.filter(F.col('expiry_time').isNull()).count() if 'tokens_df' in dir() and tokens_df.count() > 0 else 0,\n",
    "        'permission_changes': (new_permissions_df.count() + removed_permissions_df.count()) if 'new_permissions_df' in dir() and 'removed_permissions_df' in dir() else 0\n",
    "    }\n",
    "}\n",
    "\n",
    "if is_job_mode:\n",
    "    # In job mode, output JSON for programmatic consumption\n",
    "    print(json.dumps(summary, indent=2))\n",
    "    \n",
    "    # Return success status for job orchestration\n",
    "    dbutils.notebook.exit(json.dumps(summary))\n",
    "else:\n",
    "    # In interactive mode, show friendly summary\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"NOTEBOOK EXECUTION COMPLETED SUCCESSFULLY\")\n",
    "    log(\"=\"*60)\n",
    "    log(f\"\\nCompletion Time: {completion_time}\")\n",
    "    log(f\"Total Execution Time: {summary['execution_time_minutes']:.2f} minutes\")\n",
    "    log(f\"Mode: {summary['mode']}\")\n",
    "    \n",
    "    log(f\"\\nData Collected:\")\n",
    "    for key, value in summary['data_collected'].items():\n",
    "        log(f\"  {key}: {value:,}\")\n",
    "    \n",
    "    log(f\"\\nExecution Statistics:\")\n",
    "    log(f\"  API calls: {summary['execution_stats']['api_calls']}\")\n",
    "    log(f\"  Resources processed: {summary['execution_stats']['resources_processed']}\")\n",
    "    log(f\"  API failures: {summary['execution_stats']['api_failures']}\")\n",
    "    log(f\"  API retries: {summary['execution_stats']['api_retries']}\")\n",
    "    log(f\"  Success rate: {summary['execution_stats']['success_rate_percent']:.1f}%\")\n",
    "    \n",
    "    log(f\"\\nCompliance Alerts:\")\n",
    "    for key, value in summary['compliance_alerts'].items():\n",
    "        alert_icon = '⚠️' if value > 0 else '✓'\n",
    "        log(f\"  {alert_icon} {key}: {value}\")\n",
    "    \n",
    "    log(f\"\\nExport Configuration:\")\n",
    "    log(f\"  Excel export: {'ENABLED' if ENABLE_EXCEL_EXPORT else 'DISABLED'}\")\n",
    "    log(f\"  Delta export: {'ENABLED' if ENABLE_DELTA_EXPORT else 'DISABLED'}\")\n",
    "    log(f\"  Export location: {summary['configuration']['export_path']}\")\n",
    "    \n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"✓ Security audit complete!\")\n",
    "    log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af345f4f-7b5f-4bb1-b82b-7b35d7a7b27c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate HTML summary report (optional)"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generate HTML summary report for executive viewing\n",
    "# This creates a human-readable HTML report with key findings\n",
    "\n",
    "ENABLE_HTML_REPORT = False  # Set to True to enable HTML report generation\n",
    "\n",
    "if ENABLE_HTML_REPORT:\n",
    "    cell_start_time = time.time()\n",
    "    from datetime import datetime\n",
    "    import pytz\n",
    "    \n",
    "    eastern = pytz.timezone('America/New_York')\n",
    "    timestamp = datetime.now(eastern).strftime('%Y%m%d_%H%M%S')\n",
    "    report_time = datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "    html_file = f\"{EXPORT_PATH}/security_audit_report_{timestamp}.html\"\n",
    "    \n",
    "    log(\"\\nGenerating HTML summary report...\")\n",
    "    \n",
    "    # Build HTML report\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Databricks Security Audit Report</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 40px; background-color: #f5f5f5; }}\n",
    "            .container {{ max-width: 1200px; margin: 0 auto; background-color: white; padding: 30px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }}\n",
    "            h1 {{ color: #FF3621; border-bottom: 3px solid #FF3621; padding-bottom: 10px; }}\n",
    "            h2 {{ color: #333; margin-top: 30px; border-bottom: 2px solid #ddd; padding-bottom: 5px; }}\n",
    "            .metric {{ display: inline-block; margin: 15px 20px; padding: 15px; background-color: #f9f9f9; border-left: 4px solid #FF3621; }}\n",
    "            .metric-label {{ font-size: 12px; color: #666; text-transform: uppercase; }}\n",
    "            .metric-value {{ font-size: 28px; font-weight: bold; color: #333; }}\n",
    "            .alert {{ background-color: #fff3cd; border-left: 4px solid #ffc107; padding: 15px; margin: 10px 0; }}\n",
    "            .success {{ background-color: #d4edda; border-left: 4px solid #28a745; padding: 15px; margin: 10px 0; }}\n",
    "            .warning {{ background-color: #f8d7da; border-left: 4px solid #dc3545; padding: 15px; margin: 10px 0; }}\n",
    "            table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}\n",
    "            th {{ background-color: #FF3621; color: white; padding: 12px; text-align: left; }}\n",
    "            td {{ padding: 10px; border-bottom: 1px solid #ddd; }}\n",
    "            tr:hover {{ background-color: #f5f5f5; }}\n",
    "            .footer {{ margin-top: 40px; padding-top: 20px; border-top: 2px solid #ddd; color: #666; font-size: 12px; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            <h1>\uD83D\uDD12 Databricks Security Audit Report</h1>\n",
    "            <p><strong>Generated:</strong> {report_time}</p>\n",
    "            <p><strong>Workspace:</strong> Databricks Production Environment</p>\n",
    "            \n",
    "            <h2>\uD83D\uDCCA Executive Summary</h2>\n",
    "            \n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-label\">Total Users</div>\n",
    "                <div class=\"metric-value\">{users_df.count() if 'users_df' in dir() else 0}</div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-label\">Total Groups</div>\n",
    "                <div class=\"metric-value\">{groups_df.count() if 'groups_df' in dir() else 0}</div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-label\">Total Permissions</div>\n",
    "                <div class=\"metric-value\">{permissions_df.count() if 'permissions_df' in dir() else 0}</div>\n",
    "            </div>\n",
    "            \n",
    "            <div class=\"metric\">\n",
    "                <div class=\"metric-label\">Workspace Admins</div>\n",
    "                <div class=\"metric-value\">{workspace_admins.count() if 'workspace_admins' in dir() else 0}</div>\n",
    "            </div>\n",
    "            \n",
    "            <h2>⚠️ Security Alerts</h2>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Add compliance alerts\n",
    "    alerts_found = False\n",
    "    \n",
    "    if 'inactive_users_with_perms' in dir():\n",
    "        inactive_count = inactive_users_with_perms.select('user_name').distinct().count()\n",
    "        if inactive_count > 0:\n",
    "            html_content += f\"\"\"\n",
    "            <div class=\"warning\">\n",
    "                <strong>⚠️ Inactive Users with Permissions:</strong> {inactive_count} inactive users still have active permissions. \n",
    "                <em>Recommendation: Remove permissions for inactive users.</em>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            alerts_found = True\n",
    "    \n",
    "    if 'orphaned_user_count' in dir() and 'orphaned_group_count' in dir():\n",
    "        total_orphaned = orphaned_user_count + orphaned_group_count\n",
    "        if total_orphaned > 0:\n",
    "            html_content += f\"\"\"\n",
    "            <div class=\"warning\">\n",
    "                <strong>⚠️ Orphaned Permissions:</strong> {total_orphaned} permissions exist for deleted users/groups.\n",
    "                <em>Recommendation: Clean up orphaned permissions.</em>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            alerts_found = True\n",
    "    \n",
    "    if 'tokens_df' in dir() and tokens_df.count() > 0:\n",
    "        no_expiry = tokens_df.filter(F.col('expiry_time').isNull()).count()\n",
    "        if no_expiry > 0:\n",
    "            html_content += f\"\"\"\n",
    "            <div class=\"warning\">\n",
    "                <strong>⚠️ Tokens Without Expiration:</strong> {no_expiry} tokens have no expiration date.\n",
    "                <em>Recommendation: Set expiration dates for all tokens.</em>\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            alerts_found = True\n",
    "    \n",
    "    if 'new_permissions_df' in dir() and 'removed_permissions_df' in dir():\n",
    "        changes = new_permissions_df.count() + removed_permissions_df.count()\n",
    "        if changes > 0:\n",
    "            html_content += f\"\"\"\n",
    "            <div class=\"alert\">\n",
    "                <strong>\uD83D\uDD04 Permission Changes Detected:</strong> {new_permissions_df.count()} new permissions, {removed_permissions_df.count()} removed permissions since last audit.\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            alerts_found = True\n",
    "    \n",
    "    if not alerts_found:\n",
    "        html_content += \"\"\"\n",
    "        <div class=\"success\">\n",
    "            <strong>✓ No Critical Security Alerts</strong> - All security checks passed.\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    \n",
    "    # Add resource coverage summary\n",
    "    html_content += f\"\"\"\n",
    "        <h2>\uD83D\uDCCB Resource Coverage</h2>\n",
    "        <table>\n",
    "            <tr><th>Resource Type</th><th>Count</th><th>Status</th></tr>\n",
    "            <tr><td>Jobs</td><td>Included</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>SQL Warehouses</td><td>Included</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>Clusters (Interactive)</td><td>Included</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>Pipelines</td><td>Included</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>Workspace Objects</td><td>{workspace_permissions_df.count() if 'workspace_permissions_df' in dir() else 0}</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>Model Registry</td><td>Included</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>Repos (Git)</td><td>Included</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>Instance Pools</td><td>Included</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>Unity Catalog</td><td>{uc_catalogs_df.count() if 'uc_catalogs_df' in dir() else 0} catalogs</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>Secret Scopes</td><td>{secret_scopes_df.count() if 'secret_scopes_df' in dir() else 0}</td><td>✓ Covered</td></tr>\n",
    "            <tr><td>Tokens</td><td>{tokens_df.count() if 'tokens_df' in dir() else 0}</td><td>✓ Covered</td></tr>\n",
    "        </table>\n",
    "        \n",
    "        <h2>\uD83D\uDCC8 Execution Statistics</h2>\n",
    "        <table>\n",
    "            <tr><th>Metric</th><th>Value</th></tr>\n",
    "            <tr><td>Total Execution Time</td><td>{round(total_execution_time / 60, 2)} minutes</td></tr>\n",
    "            <tr><td>API Calls Made</td><td>{execution_stats['api_calls']}</td></tr>\n",
    "            <tr><td>Resources Processed</td><td>{execution_stats['resources_processed']}</td></tr>\n",
    "            <tr><td>API Failures</td><td>{execution_stats['api_failures']}</td></tr>\n",
    "            <tr><td>API Retries</td><td>{execution_stats['api_retries']}</td></tr>\n",
    "            <tr><td>Success Rate</td><td>{round(((execution_stats['api_calls'] - execution_stats['api_failures']) / execution_stats['api_calls'] * 100), 2) if execution_stats['api_calls'] > 0 else 0}%</td></tr>\n",
    "        </table>\n",
    "        \n",
    "        <div class=\"footer\">\n",
    "            <p>This report was generated by the Databricks Security Review Export notebook.</p>\n",
    "            <p>For detailed data, refer to the Excel workbook or Delta tables.</p>\n",
    "        </div>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write HTML file\n",
    "    try:\n",
    "        dbutils.fs.put(html_file.replace('/dbfs', 'dbfs:'), html_content, overwrite=True)\n",
    "        log(f\"✓ HTML report generated: {html_file}\")\n",
    "        log(f\"\\nTo view the report:\")\n",
    "        log(f\"  1. Download: databricks fs cp {html_file.replace('/dbfs', 'dbfs:')} ./\")\n",
    "        log(f\"  2. Open in web browser\")\n",
    "        log_execution_time(\"Generate HTML summary report\", cell_start_time)\n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error generating HTML report: {str(e)}\")\n",
    "else:\n",
    "    log(\"\\n⏭️  HTML report generation disabled\")\n",
    "    log(\"   Set ENABLE_HTML_REPORT = True to enable HTML summary report\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "193d0413-084d-49f1-b3b0-68dda42454f2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Display export summary"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "Command skipped"
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXPORT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\nDataframes exported:\")\n",
    "print(\"\\n0. permission_reference.xlsx\")\n",
    "print(\"   - Definitions of all permission levels by resource type\")\n",
    "print(f\"   - {permission_reference_df.count()} rows\")\n",
    "\n",
    "print(\"\\n1. users_with_groups.xlsx\")\n",
    "print(\"   - User name, display name, active status, and group membership\")\n",
    "print(\"   - One row per user-group relationship\")\n",
    "print(f\"   - {users_export.count()} rows\")\n",
    "\n",
    "print(\"\\n2. groups_with_members.xlsx\")\n",
    "print(\"   - Group name, ID, and member users\")\n",
    "print(\"   - One row per group-member relationship\")\n",
    "print(f\"   - {groups_export.count()} rows\")\n",
    "\n",
    "print(\"\\n3. user_groups.xlsx\")\n",
    "print(\"   - Simple user-to-group mapping\")\n",
    "print(f\"   - {user_groups_export.count()} rows\")\n",
    "\n",
    "print(\"\\n4. permissions.xlsx\")\n",
    "print(\"   - All permissions (users, groups, service principals)\")\n",
    "print(\"   - Resource type, ID, name, principal, permission level\")\n",
    "print(f\"   - {permissions_export.count()} rows\")\n",
    "\n",
    "print(\"\\n5. permissions_with_groups.xlsx\")\n",
    "print(\"   - Permissions enriched with user group associations\")\n",
    "print(f\"   - {permissions_with_groups_export.count()} rows\")\n",
    "\n",
    "print(\"\\n6. user_all_permissions.xlsx\")\n",
    "print(\"   - All user permissions (direct + inherited from groups)\")\n",
    "print(\"   - Shows permission source and source group\")\n",
    "print(f\"   - {user_all_permissions_export.count()} rows\")\n",
    "\n",
    "print(\"\\n7. user_permissions_summary.xlsx\")\n",
    "print(\"   - Summary by user with permission counts and levels\")\n",
    "print(f\"   - {user_perm_summary_export.count()} rows\")\n",
    "\n",
    "print(\"\\n8. group_permissions_summary.xlsx\")\n",
    "print(\"   - Summary by group with permission counts and member count\")\n",
    "print(f\"   - {group_summary_export.count()} rows\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nAll files are located in: /dbfs/tmp/permissions_export/\")\n",
    "print(\"\\nCombined workbook: all_permissions_<timestamp>.xlsx\")\n",
    "print(\"  (Contains all 9 sheets in one file, including permission reference)\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"\\nTIP: Open the 'Reference' sheet first to understand permission levels\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Security Review Export",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "646b4b58-7552-4d4f-8eb3-79a953d0289e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Compliance & Audit Trail Monitor\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides **comprehensive compliance monitoring and audit trail analysis** for your Databricks workspace. It identifies sensitive data, tracks access patterns, monitors data retention compliance, validates regulatory requirements (GDPR, CCPA, SOX), detects anomalies, and generates executive compliance reports.\n",
    "\n",
    "**✨ Enterprise-grade compliance monitoring with PII detection, audit trail analysis, retention tracking, regulatory compliance checks, and anomaly detection.**\n",
    "\n",
    "---\n",
    "\n",
    "## Features\n",
    "\n",
    "### Sensitive Data Discovery\n",
    "* **PII Detection**: Automatic identification of sensitive columns (SSN, email, phone, etc.)\n",
    "* **Sensitivity Classification**: HIGH/MEDIUM/LOW risk categorization\n",
    "* **Table-Level Aggregation**: Sensitive column counts per table\n",
    "* **Column Pattern Matching**: Keyword-based PII detection\n",
    "* **Data Type Analysis**: Type-based sensitivity assessment\n",
    "\n",
    "### Audit Trail Analysis\n",
    "* **Workspace Activity Tracking**: Complete audit log analysis\n",
    "* **Event Categorization**: Data access, modifications, permission changes, exports\n",
    "* **User Activity Monitoring**: Access patterns by user\n",
    "* **Permission Change Tracking**: Grant/revoke operations\n",
    "* **Data Export Monitoring**: Download and export activities\n",
    "* **Failed Access Attempts**: Security incident detection\n",
    "\n",
    "### Data Retention Compliance\n",
    "* **Retention Policy Validation**: Tables exceeding maximum retention (7 years)\n",
    "* **Stale Data Identification**: Tables not modified in 180+ days\n",
    "* **Compliance Status**: COMPLIANT, WITHIN_POLICY, EXCEEDS_POLICY\n",
    "* **Sensitive Data Retention**: Cross-reference with PII tables\n",
    "* **Staleness Categories**: 6mo-1yr, 1-2yrs, 2-5yrs, 5+yrs\n",
    "\n",
    "### Regulatory Compliance\n",
    "* **GDPR Compliance**: Right to erasure, data portability, consent tracking, retention limits\n",
    "* **CCPA Compliance**: Right to know, right to delete, opt-out tracking\n",
    "* **SOX Compliance**: Financial data controls, audit trail completeness, segregation of duties\n",
    "* **HIPAA Considerations**: Healthcare data protection (if applicable)\n",
    "* **Violation Detection**: Automated compliance gap identification\n",
    "\n",
    "### Anomaly Detection\n",
    "* **After-Hours Access**: Activity outside 6 AM - 8 PM\n",
    "* **High-Frequency Access**: Users with >1000 events/day\n",
    "* **Failed Access Attempts**: Unauthorized access detection\n",
    "* **Geographic Analysis**: Source IP tracking and multi-user IPs\n",
    "* **Unusual Patterns**: Behavioral anomaly identification\n",
    "\n",
    "### Compliance Reporting\n",
    "* **Compliance Score**: Weighted overall score (0-100%)\n",
    "* **Metric Breakdown**: Retention, sensitive data, audit coverage, access control\n",
    "* **Executive Summary**: Key findings and recommendations\n",
    "* **Visualizations**: Charts for compliance metrics, retention status, audit events\n",
    "* **Export Capabilities**: Delta tables, Excel, JSON formats\n",
    "\n",
    "---\n",
    "\n",
    "## Version Control\n",
    "\n",
    "| Version | Date | Author | Changes |\n",
    "|---------|------|--------|---------|  \n",
    "| 1.0.0 | 2026-02-17 | Assistant | Comprehensive compliance and audit trail monitoring system with enterprise-grade features. Includes: sensitive data discovery with PII detection across all catalogs using keyword matching (SSN, email, phone, credit card, passport, medical, etc.) and sensitivity classification (HIGH/MEDIUM/LOW), table-level aggregation of sensitive columns, audit trail analysis via system.access.audit with event categorization (data access, modifications, permission changes, exports), user activity monitoring, permission change tracking, data export monitoring, failed access attempt detection, data retention compliance validation against 7-year policy with stale data identification (180+ days), compliance status tracking (COMPLIANT/WITHIN_POLICY/EXCEEDS_POLICY), cross-reference with sensitive tables, regulatory compliance checks for GDPR (right to erasure, data portability, consent tracking, retention limits), CCPA (right to know/delete, opt-out tracking), SOX (financial data controls, audit trail completeness), HIPAA considerations, automated violation detection, anomaly detection for after-hours access (outside 6 AM-8 PM), high-frequency access (>1000 events/day), failed access attempts, geographic analysis by source IP, compliance reporting with weighted scoring (retention 30%, sensitive data 25%, audit coverage 25%, access control 20%), metric breakdown, executive summary with key findings and recommendations, interactive visualizations (compliance metrics, retention status, audit events, sensitive data distribution), multiple export formats (Delta tables with historical tracking, Excel workbooks, JSON), job mode support with automatic configuration, serverless compute optimization, execution time tracking, DataFrame caching, error handling, and comprehensive logging. |\n",
    "\n",
    "---\n",
    "\n",
    "## Configuration\n",
    "\n",
    "### Analysis Period:\n",
    "* `days_back = 30` - Days of audit logs to analyze (default: 30)\n",
    "* `start_date` / `end_date` - Automatically calculated from days_back\n",
    "\n",
    "### Compliance Thresholds:\n",
    "* `max_retention_days = 2555` - Maximum retention period (7 years)\n",
    "* `min_retention_days = 90` - Minimum retention period\n",
    "* `sensitive_access_threshold = 100` - Alert threshold for sensitive data access\n",
    "\n",
    "### PII Keywords:\n",
    "* Customizable list of sensitive data patterns\n",
    "* Default: SSN, email, phone, address, credit card, passport, DOB, salary, account numbers, tax ID, medical, health\n",
    "\n",
    "### Performance Settings:\n",
    "* `MAX_WORKERS = 10` - Parallel processing threads\n",
    "* `ENABLE_CACHING = True` - DataFrame caching for performance\n",
    "\n",
    "### Export Settings:\n",
    "* `ENABLE_EXCEL_EXPORT = False` - Excel workbook generation\n",
    "* `ENABLE_DELTA_EXPORT = False` - Delta table for historical tracking\n",
    "* `ENABLE_JSON_EXPORT = False` - JSON report generation\n",
    "* `ENABLE_VISUALIZATIONS = True` - Generate charts\n",
    "* `DELTA_CATALOG = 'main'` - Target catalog for exports\n",
    "* `DELTA_SCHEMA = 'compliance_audit'` - Target schema for exports\n",
    "\n",
    "---\n",
    "\n",
    "## Usage\n",
    "\n",
    "### Interactive Mode\n",
    "1. Configure analysis period and thresholds in Cell 2\n",
    "2. Customize PII keywords for your organization\n",
    "3. Run all cells to perform compliance analysis\n",
    "4. Review sensitive data inventory and audit findings\n",
    "5. Examine compliance score and recommendations\n",
    "6. Enable exports if needed for reporting\n",
    "\n",
    "### Job Mode\n",
    "1. Schedule as a Databricks job (monthly recommended)\n",
    "2. Set `ENABLE_DELTA_EXPORT = True` for historical tracking\n",
    "3. Automatically analyzes compliance posture\n",
    "4. Exports to Delta tables for trend analysis\n",
    "5. Returns execution summary for orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## Data Sources\n",
    "\n",
    "| Data Source | Purpose |\n",
    "|-------------|----------|\n",
    "| `system.access.audit` | Workspace audit logs, access patterns |\n",
    "| `system.information_schema.tables` | Table metadata, retention analysis |\n",
    "| `system.information_schema.columns` | Column-level PII detection |\n",
    "| Unity Catalog Tags | Sensitivity classification (future) |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Features\n",
    "\n",
    "✓ **Sensitive Data Discovery**: Automatic PII detection across all catalogs  \n",
    "✓ **Audit Trail Analysis**: Complete workspace activity tracking  \n",
    "✓ **Retention Compliance**: 7-year policy validation  \n",
    "✓ **Regulatory Compliance**: GDPR, CCPA, SOX checks  \n",
    "✓ **Anomaly Detection**: After-hours, high-frequency, failed access  \n",
    "✓ **Compliance Scoring**: Weighted metrics (0-100%)  \n",
    "✓ **Executive Reporting**: Key findings and recommendations  \n",
    "✓ **Multiple Export Formats**: Delta, Excel, JSON  \n",
    "✓ **Interactive Visualizations**: Compliance dashboards  \n",
    "✓ **Job Mode Support**: Automated scheduled execution  \n",
    "✓ **Serverless Optimized**: Compute-aware optimizations  \n",
    "✓ **Performance Tracking**: Execution time logging  \n",
    "✓ **Historical Tracking**: Delta table with append mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c0df5d7-5d57-40b9-aaa5-7eb0bf94548f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Configuration Parameters"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "\n",
    "# Standard library\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# Third-party\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pytz\n",
    "\n",
    "# Databricks SDK\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.errors import NotFound, PermissionDenied\n",
    "\n",
    "# PySpark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Execution mode detection\n",
    "try:\n",
    "    dbutils.widgets.get('run_mode')\n",
    "    is_job_mode = True\n",
    "except:\n",
    "    is_job_mode = False\n",
    "\n",
    "# Timezone\n",
    "eastern = pytz.timezone('America/New_York')\n",
    "\n",
    "# Analysis period (default: last 30 days)\n",
    "days_back = 30\n",
    "start_date = (datetime.now(eastern) - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "end_date = datetime.now(eastern).strftime('%Y-%m-%d')\n",
    "\n",
    "# Compliance thresholds\n",
    "max_retention_days = 2555  # 7 years for regulatory compliance\n",
    "min_retention_days = 90     # Minimum retention period\n",
    "sensitive_access_threshold = 100  # Alert if sensitive data accessed more than this\n",
    "\n",
    "# PII/Sensitive data keywords (customize based on your organization)\n",
    "pii_keywords = ['ssn', 'social_security', 'email', 'phone', 'address', 'credit_card', \n",
    "                'passport', 'driver_license', 'dob', 'date_of_birth', 'salary', \n",
    "                'account_number', 'tax_id', 'national_id', 'medical', 'health']\n",
    "\n",
    "# Sensitive data discovery optimization\n",
    "SCAN_SPECIFIC_CATALOGS = True  # Set to False to scan all catalogs\n",
    "CATALOGS_TO_SCAN = ['main', 'rai_prod_uc', 'rai_qa_uc', 'rai_dev_uc']  # Customize for your environment\n",
    "\n",
    "# Schema exclusions (skip known non-sensitive schemas)\n",
    "EXCLUDE_SCHEMAS = ['information_schema', 'default', 'tmp', 'temp', 'test', 'sandbox', \n",
    "                   'dev_scratch', 'adhoc', 'samples']  # Customize for your environment\n",
    "ENABLE_SCHEMA_EXCLUSIONS = True  # Set to False to scan all schemas\n",
    "\n",
    "USE_SPARK_CLASSIFICATION = True  # Use Spark instead of pandas for classification (faster)\n",
    "\n",
    "# Incremental scanning configuration\n",
    "ENABLE_INCREMENTAL_SCAN = True  # Set to False for full scan every time\n",
    "FORCE_FULL_SCAN = False  # Set to True to force a full scan this run (overrides incremental)\n",
    "INCREMENTAL_TABLE_CATALOG = 'main'\n",
    "INCREMENTAL_TABLE_SCHEMA = 'compliance_audit'\n",
    "INCREMENTAL_TABLE_NAME = 'sensitive_columns_history'\n",
    "INCREMENTAL_LOOKBACK_DAYS = 7  # Only scan tables modified in last N days (incremental mode)\n",
    "\n",
    "# Service principals to exclude from user-focused analysis\n",
    "# These are automated accounts that create noise in compliance monitoring\n",
    "SERVICE_PRINCIPALS = [\n",
    "    'System-User',\n",
    "    'System user',\n",
    "    'unknown',\n",
    "    ''  # Empty string for null emails\n",
    "]\n",
    "\n",
    "# Filter service principals by pattern (UUIDs, system accounts)\n",
    "FILTER_SERVICE_PRINCIPALS = True  # Set to False to include all accounts\n",
    "FILTER_UUID_ACCOUNTS = True       # Filter accounts that look like UUIDs\n",
    "FILTER_GROUP_ACCOUNTS = True      # Filter accounts starting with 'Developers-'\n",
    "\n",
    "# Performance settings\n",
    "MAX_WORKERS = 10  # Parallel processing threads\n",
    "ENABLE_CACHING = False  # Disable caching on serverless (not beneficial)\n",
    "\n",
    "# Export settings\n",
    "ENABLE_EXCEL_EXPORT = False  # Set to True to export Excel reports\n",
    "ENABLE_DELTA_EXPORT = False  # Set to True to save to Delta tables\n",
    "ENABLE_JSON_EXPORT = False   # Set to True to export JSON reports\n",
    "ENABLE_VISUALIZATIONS = True  # Set to False to skip visualizations\n",
    "\n",
    "# Export paths\n",
    "EXPORT_BASE_PATH = '/Workspace/Users/85055763@bat.com/exports/compliance'\n",
    "DELTA_CATALOG = 'main'\n",
    "DELTA_SCHEMA = 'compliance_audit'\n",
    "\n",
    "# Initialize Workspace Client\n",
    "wc = WorkspaceClient()\n",
    "\n",
    "# Logging function\n",
    "def log(message):\n",
    "    \"\"\"Print timestamped log message\"\"\"\n",
    "    timestamp = datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"[{timestamp}] {message}\")\n",
    "\n",
    "if is_job_mode:\n",
    "    log(\"\uD83E\uDD16 Job mode: Exports ENABLED\")\n",
    "else:\n",
    "    log(\"\uD83D\uDCBB Interactive mode: Exports DISABLED\")\n",
    "\n",
    "log(\"\")\n",
    "log(\"=\"*60)\n",
    "log(\"COMPLIANCE & AUDIT TRAIL MONITOR\")\n",
    "log(\"=\"*60)\n",
    "log(f\"Execution mode: {'JOB' if is_job_mode else 'INTERACTIVE'}\")\n",
    "log(f\"Compute: Serverless (caching disabled for performance)\")\n",
    "log(f\"Timezone: {eastern.zone}\")\n",
    "log(f\"Analysis Period: {start_date} to {end_date}\")\n",
    "log(f\"Retention Policy: {min_retention_days} - {max_retention_days} days\")\n",
    "log(f\"Sensitive Access Threshold: {sensitive_access_threshold} accesses\")\n",
    "if SCAN_SPECIFIC_CATALOGS:\n",
    "    log(f\"Scanning catalogs: {', '.join(CATALOGS_TO_SCAN)}\")\n",
    "else:\n",
    "    log(f\"Scanning: ALL catalogs\")\n",
    "if ENABLE_SCHEMA_EXCLUSIONS:\n",
    "    log(f\"Excluding schemas: {', '.join(EXCLUDE_SCHEMAS)}\")\n",
    "log(f\"Incremental scanning: {'ENABLED' if ENABLE_INCREMENTAL_SCAN else 'DISABLED'}\")\n",
    "if FORCE_FULL_SCAN:\n",
    "    log(f\"⚠️  FORCE_FULL_SCAN = True - Will perform full scan this run\")\n",
    "if ENABLE_INCREMENTAL_SCAN:\n",
    "    log(f\"Incremental table: {INCREMENTAL_TABLE_CATALOG}.{INCREMENTAL_TABLE_SCHEMA}.{INCREMENTAL_TABLE_NAME}\")\n",
    "    log(f\"Incremental lookback: {INCREMENTAL_LOOKBACK_DAYS} days\")\n",
    "log(f\"Service principal filtering: {'ENABLED' if FILTER_SERVICE_PRINCIPALS else 'DISABLED'}\")\n",
    "log(f\"Excel export: {'ENABLED' if ENABLE_EXCEL_EXPORT else 'DISABLED'}\")\n",
    "log(f\"Delta export: {'ENABLED' if ENABLE_DELTA_EXPORT else 'DISABLED'}\")\n",
    "log(f\"JSON export: {'ENABLED' if ENABLE_JSON_EXPORT else 'DISABLED'}\")\n",
    "log(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cacd81af-d893-41f6-9859-10ecb49efd9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Helper Functions"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def log_execution_time(cell_name, start_time):\n",
    "    \"\"\"Log execution time for a cell\"\"\"\n",
    "    elapsed = time.time() - start_time\n",
    "    log(f\"⏱️  {cell_name} completed in {elapsed:.2f} seconds\")\n",
    "\n",
    "def validate_dataframe_exists(df_name, df):\n",
    "    \"\"\"Validate that a DataFrame exists and has data\"\"\"\n",
    "    if df is None:\n",
    "        log(f\"⚠️  {df_name} is None, skipping dependent operations\")\n",
    "        return False\n",
    "    try:\n",
    "        count = df.count()\n",
    "        if count == 0:\n",
    "            log(f\"⚠️  {df_name} is empty (0 rows)\")\n",
    "            return False\n",
    "        log(f\"✓ {df_name} validated: {count:,} rows\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error validating {df_name}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def safe_cache(df, df_name):\n",
    "    \"\"\"Safely cache a DataFrame if caching is enabled and not on serverless\"\"\"\n",
    "    if not ENABLE_CACHING or df is None:\n",
    "        return df\n",
    "    \n",
    "    try:\n",
    "        # Try to cache - if it fails due to serverless, catch and skip silently\n",
    "        df.cache()\n",
    "        log(f\"\uD83D\uDCBE Cached {df_name}\")\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        # Check if it's a serverless-related error\n",
    "        if 'PERSIST' in error_msg or 'serverless' in error_msg.lower():\n",
    "            log(f\"ℹ️  Skipping cache for {df_name} (serverless compute)\")\n",
    "        else:\n",
    "            # For other errors, log the actual error\n",
    "            log(f\"⚠️  Could not cache {df_name}: {error_msg}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def is_service_principal(email):\n",
    "    \"\"\"Determine if an email/user is a service principal\"\"\"\n",
    "    if not email or pd.isna(email):\n",
    "        return True  # Treat null/empty as service principal\n",
    "    \n",
    "    email_str = str(email).strip()\n",
    "    \n",
    "    # Check explicit list\n",
    "    if email_str in SERVICE_PRINCIPALS:\n",
    "        return True\n",
    "    \n",
    "    # Check UUID pattern (8-4-4-4-12 hex digits)\n",
    "    if FILTER_UUID_ACCOUNTS:\n",
    "        import re\n",
    "        uuid_pattern = r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$'\n",
    "        if re.match(uuid_pattern, email_str.lower()):\n",
    "            return True\n",
    "    \n",
    "    # Check group accounts\n",
    "    if FILTER_GROUP_ACCOUNTS:\n",
    "        if email_str.startswith('Developers-') or email_str.startswith('developers-'):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def filter_service_principals_spark(df, email_column='user_email'):\n",
    "    \"\"\"Filter out service principals from a Spark DataFrame\"\"\"\n",
    "    if not FILTER_SERVICE_PRINCIPALS:\n",
    "        return df\n",
    "    \n",
    "    # Filter explicit list\n",
    "    df_filtered = df.filter(~F.col(email_column).isin(SERVICE_PRINCIPALS))\n",
    "    \n",
    "    # Filter UUIDs\n",
    "    if FILTER_UUID_ACCOUNTS:\n",
    "        df_filtered = df_filtered.filter(\n",
    "            ~F.col(email_column).rlike('^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$')\n",
    "        )\n",
    "    \n",
    "    # Filter group accounts\n",
    "    if FILTER_GROUP_ACCOUNTS:\n",
    "        df_filtered = df_filtered.filter(\n",
    "            ~F.lower(F.col(email_column)).startswith('developers-')\n",
    "        )\n",
    "    \n",
    "    return df_filtered\n",
    "\n",
    "def classify_sensitivity(column_name, data_type):\n",
    "    \"\"\"Classify column sensitivity based on name and type\"\"\"\n",
    "    column_lower = column_name.lower()\n",
    "    for keyword in pii_keywords:\n",
    "        if keyword in column_lower:\n",
    "            return 'HIGH'\n",
    "    if any(term in column_lower for term in ['name', 'user', 'customer', 'employee']):\n",
    "        return 'MEDIUM'\n",
    "    return 'LOW'\n",
    "\n",
    "def calculate_retention_status(last_modified_days):\n",
    "    \"\"\"Determine retention compliance status\"\"\"\n",
    "    if last_modified_days is None:\n",
    "        return 'UNKNOWN'\n",
    "    if last_modified_days > max_retention_days:\n",
    "        return 'EXCEEDS_POLICY'\n",
    "    elif last_modified_days < min_retention_days:\n",
    "        return 'WITHIN_POLICY'\n",
    "    else:\n",
    "        return 'COMPLIANT'\n",
    "\n",
    "def categorize_action(action_name):\n",
    "    \"\"\"Categorize audit actions into compliance-relevant groups\"\"\"\n",
    "    action_lower = action_name.lower()\n",
    "    \n",
    "    # Authentication & Authorization (tokenLogin, oidcTokenAuthorization, mintOAuthToken)\n",
    "    if any(term in action_lower for term in ['login', 'auth', 'token', 'oauth', 'authenticate']):\n",
    "        return 'AUTHENTICATION'\n",
    "    \n",
    "    # Credential Generation (generateTemporaryPathCredential, generateTemporaryTableCredential)\n",
    "    if any(term in action_lower for term in ['credential', 'getsessioncredentials']):\n",
    "        return 'CREDENTIAL_GENERATION'\n",
    "    \n",
    "    # Metadata Access (getTable, getSchema, metadataSnapshot, tableExists, getPipeline)\n",
    "    if any(term in action_lower for term in ['gettable', 'getschema', 'metadata', 'tableexists', 'getpipeline', 'getcatalog', 'listschemas', 'listtables', 'gettablebyid']):\n",
    "        return 'METADATA_ACCESS'\n",
    "    \n",
    "    # Job/Cluster Execution (runCommand, runStart, runSucceeded, submitRun)\n",
    "    if any(term in action_lower for term in ['runcommand', 'runstart', 'runsucceeded', 'runfailed', 'runtriggered', 'submitrun', 'submitcommand']):\n",
    "        return 'JOB_EXECUTION'\n",
    "    \n",
    "    # Cluster Operations (create, delete, resize clusters)\n",
    "    if any(term in action_lower for term in ['cluster']) and any(term in action_lower for term in ['create', 'delete', 'resize', 'start', 'terminate']):\n",
    "        return 'CLUSTER_OPERATIONS'\n",
    "    \n",
    "    # Secret Access (getSecret)\n",
    "    if 'secret' in action_lower:\n",
    "        return 'SECRET_ACCESS'\n",
    "    \n",
    "    # Data Access (read, select, query, scan)\n",
    "    if any(term in action_lower for term in ['read', 'select', 'query', 'scan']):\n",
    "        return 'DATA_ACCESS'\n",
    "    \n",
    "    # Data Modification (create, update, delete, drop, alter)\n",
    "    if any(term in action_lower for term in ['create', 'update', 'delete', 'drop', 'alter', 'insert', 'merge', 'modify']):\n",
    "        return 'DATA_MODIFICATION'\n",
    "    \n",
    "    # Permission Changes (grant, revoke, permission, changeJobAcl)\n",
    "    if any(term in action_lower for term in ['grant', 'revoke', 'permission', 'acl']):\n",
    "        return 'PERMISSION_CHANGE'\n",
    "    \n",
    "    # Data Export (export, download)\n",
    "    if any(term in action_lower for term in ['export', 'download']):\n",
    "        return 'DATA_EXPORT'\n",
    "    \n",
    "    # Monitoring & Metrics (PutMetrics, getRunOutput)\n",
    "    if any(term in action_lower for term in ['metric', 'getrunoutput', 'monitoring']):\n",
    "        return 'MONITORING'\n",
    "    \n",
    "    # Everything else\n",
    "    return 'OTHER'\n",
    "\n",
    "log(\"✓ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae6c6c4-6422-48cd-89af-366b2ee87046",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Section: Sensitive Data Discovery"
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 1. Sensitive Data Discovery & Classification\n",
    "\n",
    "Identifying tables and columns containing PII or sensitive information based on:\n",
    "* Column naming patterns\n",
    "* Unity Catalog tags\n",
    "* Data types commonly associated with sensitive data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "161621b5-4672-4408-aa89-6a2ce815b0e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## \uD83D\uDCD6 Incremental Scanning Guide\n",
    "\n",
    "### How It Works\n",
    "\n",
    "**First Run (Automatic):**\n",
    "* No history table exists → Full scan of all tables (~15-20 minutes)\n",
    "* Creates `main.compliance_audit.sensitive_columns_history`\n",
    "* Saves all results for future reference\n",
    "\n",
    "**Subsequent Runs (Automatic):**\n",
    "* History table exists → Incremental scan (1-3 minutes) ⚡\n",
    "* Only scans tables modified in last 7 days\n",
    "* Merges with existing history\n",
    "* Returns complete inventory\n",
    "\n",
    "### Configuration Options\n",
    "\n",
    "#### Normal Operation (Default)\n",
    "```python\n",
    "ENABLE_INCREMENTAL_SCAN = True   # Incremental mode enabled\n",
    "FORCE_FULL_SCAN = False          # Use incremental logic\n",
    "INCREMENTAL_LOOKBACK_DAYS = 7    # Scan last 7 days of changes\n",
    "```\n",
    "\n",
    "#### Force Full Scan (Monthly/Quarterly Refresh)\n",
    "```python\n",
    "ENABLE_INCREMENTAL_SCAN = True   # Keep history\n",
    "FORCE_FULL_SCAN = True           # Override incremental, scan everything\n",
    "INCREMENTAL_LOOKBACK_DAYS = 7    # Ignored when FORCE_FULL_SCAN = True\n",
    "```\n",
    "**Use when:** You want a comprehensive refresh without losing history\n",
    "\n",
    "#### Disable Incremental (Always Full Scan)\n",
    "```python\n",
    "ENABLE_INCREMENTAL_SCAN = False  # No history tracking\n",
    "FORCE_FULL_SCAN = False          # Not applicable\n",
    "```\n",
    "**Use when:** Testing or one-time analysis\n",
    "\n",
    "### When to Force a Full Scan\n",
    "\n",
    "✅ **Monthly/Quarterly Compliance Reports** - Comprehensive refresh\n",
    "\n",
    "✅ **After Major Schema Changes** - Ensure all new tables are captured\n",
    "\n",
    "✅ **Audit Requirements** - Complete inventory verification\n",
    "\n",
    "✅ **Suspect Missing Data** - Validate incremental logic\n",
    "\n",
    "### How to Force a Full Scan\n",
    "\n",
    "**Option 1: Use FORCE_FULL_SCAN (Recommended)**\n",
    "1. Go to Cell 2 (Configuration)\n",
    "2. Set `FORCE_FULL_SCAN = True`\n",
    "3. Run Cell 2 and Cell 5 (Incremental)\n",
    "4. After completion, set `FORCE_FULL_SCAN = False` and rerun Cell 2\n",
    "\n",
    "**Option 2: Drop History Table (Nuclear Option)**\n",
    "```python\n",
    "spark.sql('DROP TABLE IF EXISTS main.compliance_audit.sensitive_columns_history')\n",
    "```\n",
    "* Loses all historical scan data\n",
    "* Next run will be a fresh start\n",
    "\n",
    "### Performance Expectations\n",
    "\n",
    "| Scan Type | Duration | When It Runs |\n",
    "|-----------|----------|-------------|\n",
    "| **First Run** | 15-20 min | Only once (no history table) |\n",
    "| **Incremental** | 1-3 min | Every run after first |\n",
    "| **Force Full** | 15-20 min | When FORCE_FULL_SCAN = True |\n",
    "\n",
    "### Schema Exclusions\n",
    "\n",
    "To further optimize, exclude non-sensitive schemas:\n",
    "```python\n",
    "ENABLE_SCHEMA_EXCLUSIONS = True\n",
    "EXCLUDE_SCHEMAS = ['information_schema', 'default', 'tmp', 'temp', 'test', 'sandbox']\n",
    "```\n",
    "\n",
    "### Viewing History\n",
    "\n",
    "Run Cell 6 (View Incremental Scan History) to see:\n",
    "* Scan dates and statistics\n",
    "* Sensitive data by schema\n",
    "* Tables not scanned recently\n",
    "* Management commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ee139f5-9d49-4d1c-bf22-2a100e40e47c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify Sensitive Columns (Incremental)"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"IDENTIFYING SENSITIVE COLUMNS (INCREMENTAL)\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Build full table name for incremental tracking\n",
    "    incremental_table_full = f\"{INCREMENTAL_TABLE_CATALOG}.{INCREMENTAL_TABLE_SCHEMA}.{INCREMENTAL_TABLE_NAME}\"\n",
    "    \n",
    "    # Check if force full scan is enabled\n",
    "    if FORCE_FULL_SCAN:\n",
    "        log(\"⚠️  FORCE_FULL_SCAN enabled - performing full scan regardless of history\")\n",
    "        is_first_run = False  # Keep history, just rescan everything\n",
    "        lookback_days = 36500  # ~100 years (effectively all tables)\n",
    "    # Check if incremental mode is enabled\n",
    "    elif ENABLE_INCREMENTAL_SCAN:\n",
    "        log(f\"Incremental mode ENABLED\")\n",
    "        log(f\"Checking for existing history table: {incremental_table_full}\")\n",
    "        \n",
    "        # Check if history table exists\n",
    "        try:\n",
    "            history_exists = spark.catalog.tableExists(incremental_table_full)\n",
    "        except:\n",
    "            history_exists = False\n",
    "        \n",
    "        if history_exists:\n",
    "            # Get the last scan date\n",
    "            last_scan_df = spark.sql(f\"\"\"\n",
    "                SELECT MAX(scan_date) as last_scan_date\n",
    "                FROM {incremental_table_full}\n",
    "            \"\"\")\n",
    "            last_scan_date = last_scan_df.collect()[0]['last_scan_date']\n",
    "            \n",
    "            if last_scan_date:\n",
    "                log(f\"✓ History table found. Last scan: {last_scan_date}\")\n",
    "                log(f\"Scanning only tables modified in last {INCREMENTAL_LOOKBACK_DAYS} days...\")\n",
    "                is_first_run = False\n",
    "                lookback_days = INCREMENTAL_LOOKBACK_DAYS\n",
    "            else:\n",
    "                log(\"⚠️  History table exists but is empty. Performing full scan...\")\n",
    "                is_first_run = True\n",
    "                lookback_days = 36500  # ~100 years (effectively all tables)\n",
    "        else:\n",
    "            log(\"ℹ️  No history table found. Performing initial full scan...\")\n",
    "            is_first_run = True\n",
    "            lookback_days = 36500  # ~100 years (effectively all tables)\n",
    "    else:\n",
    "        log(\"Incremental mode DISABLED. Performing full scan...\")\n",
    "        is_first_run = True\n",
    "        lookback_days = 36500\n",
    "    \n",
    "    # Build catalog filter\n",
    "    if SCAN_SPECIFIC_CATALOGS:\n",
    "        catalog_list = \"', '\".join(CATALOGS_TO_SCAN)\n",
    "        catalog_filter = f\"AND t.table_catalog IN ('{catalog_list}')\"\n",
    "        log(f\"Scanning catalogs: {', '.join(CATALOGS_TO_SCAN)}\")\n",
    "    else:\n",
    "        catalog_filter = \"\"\n",
    "    \n",
    "    # Build schema exclusion filter\n",
    "    if ENABLE_SCHEMA_EXCLUSIONS:\n",
    "        schema_list = \"', '\".join(EXCLUDE_SCHEMAS)\n",
    "        schema_filter = f\"AND t.table_schema NOT IN ('{schema_list}')\"\n",
    "        log(f\"Excluding schemas: {', '.join(EXCLUDE_SCHEMAS)}\")\n",
    "    else:\n",
    "        schema_filter = \"\"\n",
    "    \n",
    "    # Build PII pattern for SQL RLIKE\n",
    "    pii_pattern = '|'.join(pii_keywords)\n",
    "    medium_pattern = 'name|user|customer|employee'\n",
    "    \n",
    "    # Query with incremental filter\n",
    "    log(f\"Executing query (lookback: {lookback_days} days)...\")\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        c.table_catalog,\n",
    "        c.table_schema,\n",
    "        c.table_name,\n",
    "        c.column_name,\n",
    "        c.data_type,\n",
    "        c.comment,\n",
    "        CONCAT(c.table_catalog, '.', c.table_schema, '.', c.table_name) as full_table_name,\n",
    "        CASE \n",
    "            WHEN LOWER(c.column_name) RLIKE '{pii_pattern}' THEN 'HIGH'\n",
    "            WHEN LOWER(c.column_name) RLIKE '{medium_pattern}' THEN 'MEDIUM'\n",
    "            ELSE 'LOW'\n",
    "        END as sensitivity_level,\n",
    "        CURRENT_DATE() as scan_date,\n",
    "        t.last_altered as table_last_altered\n",
    "    FROM system.information_schema.columns c\n",
    "    INNER JOIN system.information_schema.tables t\n",
    "        ON c.table_catalog = t.table_catalog\n",
    "        AND c.table_schema = t.table_schema\n",
    "        AND c.table_name = t.table_name\n",
    "    WHERE c.table_catalog NOT IN ('system', '__databricks_internal')\n",
    "        AND t.table_type IN ('MANAGED', 'EXTERNAL')\n",
    "        AND DATEDIFF(CURRENT_DATE(), DATE(t.last_altered)) <= {lookback_days}\n",
    "        {catalog_filter}\n",
    "        {schema_filter}\n",
    "    \"\"\"\n",
    "\n",
    "    sensitive_columns_classified = spark.sql(query)\n",
    "    \n",
    "    # Filter to HIGH and MEDIUM only\n",
    "    log(\"Filtering to HIGH and MEDIUM sensitivity columns...\")\n",
    "    sensitive_spark = sensitive_columns_classified.filter(\n",
    "        F.col('sensitivity_level').isin(['HIGH', 'MEDIUM'])\n",
    "    )\n",
    "    \n",
    "    # Get counts\n",
    "    log(\"Calculating sensitivity statistics...\")\n",
    "    sensitivity_counts = sensitive_spark.groupBy('sensitivity_level').count().collect()\n",
    "    count_dict = {row['sensitivity_level']: row['count'] for row in sensitivity_counts}\n",
    "    high_count = count_dict.get('HIGH', 0)\n",
    "    medium_count = count_dict.get('MEDIUM', 0)\n",
    "    total_sensitive = high_count + medium_count\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCCA Sensitive columns identified in this scan:\")\n",
    "    log(f\"  HIGH sensitivity: {high_count:,}\")\n",
    "    log(f\"  MEDIUM sensitivity: {medium_count:,}\")\n",
    "    log(f\"  Total sensitive: {total_sensitive:,}\")\n",
    "    \n",
    "    # Save to history table if incremental mode is enabled\n",
    "    if ENABLE_INCREMENTAL_SCAN and total_sensitive > 0:\n",
    "        log(f\"\\nSaving results to history table: {incremental_table_full}\")\n",
    "        try:\n",
    "            # Create schema if it doesn't exist\n",
    "            spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {INCREMENTAL_TABLE_CATALOG}.{INCREMENTAL_TABLE_SCHEMA}\")\n",
    "            \n",
    "            if is_first_run:\n",
    "                # First run: create table\n",
    "                log(\"Creating new history table...\")\n",
    "                sensitive_spark.write.mode('overwrite').saveAsTable(incremental_table_full)\n",
    "                log(\"✓ History table created successfully\")\n",
    "            else:\n",
    "                # Subsequent runs or forced full scan: merge new/updated data\n",
    "                if FORCE_FULL_SCAN:\n",
    "                    log(\"Full scan mode: Replacing entire history table...\")\n",
    "                    sensitive_spark.write.mode('overwrite').saveAsTable(incremental_table_full)\n",
    "                    log(\"✓ History table replaced with full scan results\")\n",
    "                else:\n",
    "                    log(\"Merging with existing history...\")\n",
    "                    \n",
    "                    # Delete old records for tables that were rescanned\n",
    "                    tables_scanned = sensitive_spark.select('full_table_name').distinct()\n",
    "                    tables_scanned.createOrReplaceTempView('tables_scanned_temp')\n",
    "                    \n",
    "                    spark.sql(f\"\"\"\n",
    "                        DELETE FROM {incremental_table_full}\n",
    "                        WHERE full_table_name IN (SELECT full_table_name FROM tables_scanned_temp)\n",
    "                    \"\"\")\n",
    "                    \n",
    "                    # Append new records\n",
    "                    sensitive_spark.write.mode('append').saveAsTable(incremental_table_full)\n",
    "                    log(\"✓ History table updated successfully\")\n",
    "                \n",
    "            # Load complete history for analysis\n",
    "            log(\"Loading complete history from Delta table...\")\n",
    "            sensitive_spark = spark.table(incremental_table_full).filter(\n",
    "                F.col('sensitivity_level').isin(['HIGH', 'MEDIUM'])\n",
    "            )\n",
    "            \n",
    "            # Recalculate totals from complete history\n",
    "            sensitivity_counts = sensitive_spark.groupBy('sensitivity_level').count().collect()\n",
    "            count_dict = {row['sensitivity_level']: row['count'] for row in sensitivity_counts}\n",
    "            high_count = count_dict.get('HIGH', 0)\n",
    "            medium_count = count_dict.get('MEDIUM', 0)\n",
    "            total_sensitive = high_count + medium_count\n",
    "            \n",
    "            log(f\"\\n\uD83D\uDCCA Complete inventory (from history):\")\n",
    "            log(f\"  HIGH sensitivity: {high_count:,}\")\n",
    "            log(f\"  MEDIUM sensitivity: {medium_count:,}\")\n",
    "            log(f\"  Total sensitive: {total_sensitive:,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            log(f\"⚠️  Could not save to history table: {str(e)}\")\n",
    "            log(\"Continuing with current scan results...\")\n",
    "    \n",
    "    # Convert to pandas for downstream analysis\n",
    "    log(\"\\nConverting to pandas for analysis...\")\n",
    "    if total_sensitive > 50000:\n",
    "        log(f\"⚠️  Large dataset ({total_sensitive:,} rows), limiting to 50,000\")\n",
    "        sensitive_data = sensitive_spark.limit(50000).toPandas()\n",
    "    else:\n",
    "        sensitive_data = sensitive_spark.toPandas()\n",
    "    \n",
    "    log(f\"Loaded {len(sensitive_data):,} sensitive columns into memory\")\n",
    "    display(sensitive_data.head(20))\n",
    "    \n",
    "    # Reset FORCE_FULL_SCAN flag reminder\n",
    "    if FORCE_FULL_SCAN:\n",
    "        log(\"\\n\" + \"=\"*60)\n",
    "        log(\"⚠️  REMINDER: FORCE_FULL_SCAN is still enabled\")\n",
    "        log(\"Set FORCE_FULL_SCAN = False in the configuration cell\")\n",
    "        log(\"to return to incremental mode for faster scans.\")\n",
    "        log(\"=\"*60)\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error in incremental scan: {str(e)}\")\n",
    "    import traceback\n",
    "    log(traceback.format_exc())\n",
    "    sensitive_data = None\n",
    "    \n",
    "log_execution_time(\"Identify Sensitive Columns (Incremental)\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70640d63-ada9-420f-8fb8-509e7409c9d4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "View Incremental Scan History (Optional)"
    }
   },
   "outputs": [],
   "source": [
    "# Optional: View incremental scan history and statistics\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"INCREMENTAL SCAN HISTORY\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "incremental_table_full = f\"{INCREMENTAL_TABLE_CATALOG}.{INCREMENTAL_TABLE_SCHEMA}.{INCREMENTAL_TABLE_NAME}\"\n",
    "\n",
    "try:\n",
    "    # Check if history table exists\n",
    "    if spark.catalog.tableExists(incremental_table_full):\n",
    "        log(f\"✓ History table exists: {incremental_table_full}\")\n",
    "        \n",
    "        # Get scan history statistics\n",
    "        history_stats = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                scan_date,\n",
    "                COUNT(*) as total_columns,\n",
    "                COUNT(DISTINCT full_table_name) as total_tables,\n",
    "                SUM(CASE WHEN sensitivity_level = 'HIGH' THEN 1 ELSE 0 END) as high_sensitivity,\n",
    "                SUM(CASE WHEN sensitivity_level = 'MEDIUM' THEN 1 ELSE 0 END) as medium_sensitivity\n",
    "            FROM {incremental_table_full}\n",
    "            GROUP BY scan_date\n",
    "            ORDER BY scan_date DESC\n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "        \n",
    "        log(\"\\nRecent scan history:\")\n",
    "        display(history_stats)\n",
    "        \n",
    "        # Get table-level summary\n",
    "        table_summary = spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                table_catalog,\n",
    "                table_schema,\n",
    "                COUNT(DISTINCT table_name) as table_count,\n",
    "                COUNT(*) as sensitive_column_count,\n",
    "                MAX(scan_date) as last_scanned\n",
    "            FROM {incremental_table_full}\n",
    "            GROUP BY table_catalog, table_schema\n",
    "            ORDER BY sensitive_column_count DESC\n",
    "        \"\"\")\n",
    "        \n",
    "        log(\"\\nSensitive data by schema:\")\n",
    "        display(table_summary)\n",
    "        \n",
    "        # Show total size\n",
    "        total_rows = spark.table(incremental_table_full).count()\n",
    "        log(f\"\\nTotal rows in history table: {total_rows:,}\")\n",
    "        \n",
    "        log(\"\\n\" + \"=\"*60)\n",
    "        log(\"MANAGEMENT OPTIONS\")\n",
    "        log(\"=\"*60)\n",
    "        log(\"\\nTo reset the incremental scan history (force full rescan):\")\n",
    "        log(f\"  spark.sql('DROP TABLE IF EXISTS {incremental_table_full}')\")\n",
    "        log(\"\\nTo view specific tables:\")\n",
    "        log(f\"  spark.sql('SELECT * FROM {incremental_table_full} WHERE table_name = \\\"your_table\\\"').display()\")\n",
    "        log(\"\\nTo see tables not scanned recently:\")\n",
    "        log(f\"  spark.sql('SELECT DISTINCT full_table_name, MAX(scan_date) as last_scan FROM {incremental_table_full} GROUP BY full_table_name HAVING DATEDIFF(CURRENT_DATE(), MAX(scan_date)) > 30').display()\")\n",
    "        \n",
    "    else:\n",
    "        log(f\"ℹ️  No history table found: {incremental_table_full}\")\n",
    "        log(\"Run the incremental scan cell to create it.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error viewing history: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c82ee1-bb9b-4a53-9595-a13e3b61f0a7",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify Sensitive Columns (Fast - Sample-Based)"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"IDENTIFYING SENSITIVE COLUMNS (SAMPLE-BASED)\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # PERFORMANCE OPTIMIZATION: Only scan recently modified tables\n",
    "    # This reduces scan from 1.5M columns to ~100K columns\n",
    "    \n",
    "    # Build catalog filter\n",
    "    if SCAN_SPECIFIC_CATALOGS:\n",
    "        catalog_list = \"', '\".join(CATALOGS_TO_SCAN)\n",
    "        catalog_filter = f\"AND t.table_catalog IN ('{catalog_list}')\"\n",
    "        log(f\"Scanning specific catalogs: {', '.join(CATALOGS_TO_SCAN)}\")\n",
    "    else:\n",
    "        catalog_filter = \"\"\n",
    "        log(\"Scanning ALL catalogs\")\n",
    "    \n",
    "    # Build PII pattern for SQL RLIKE\n",
    "    pii_pattern = '|'.join(pii_keywords)\n",
    "    medium_pattern = 'name|user|customer|employee'\n",
    "    \n",
    "    log(\"Using sample-based approach: scanning only recently modified tables (last 180 days)...\")\n",
    "    \n",
    "    # Join columns with tables to filter by last_altered date\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        c.table_catalog,\n",
    "        c.table_schema,\n",
    "        c.table_name,\n",
    "        c.column_name,\n",
    "        c.data_type,\n",
    "        c.comment,\n",
    "        CONCAT(c.table_catalog, '.', c.table_schema, '.', c.table_name) as full_table_name,\n",
    "        CASE \n",
    "            WHEN LOWER(c.column_name) RLIKE '{pii_pattern}' THEN 'HIGH'\n",
    "            WHEN LOWER(c.column_name) RLIKE '{medium_pattern}' THEN 'MEDIUM'\n",
    "            ELSE 'LOW'\n",
    "        END as sensitivity_level\n",
    "    FROM system.information_schema.columns c\n",
    "    INNER JOIN system.information_schema.tables t\n",
    "        ON c.table_catalog = t.table_catalog\n",
    "        AND c.table_schema = t.table_schema\n",
    "        AND c.table_name = t.table_name\n",
    "    WHERE c.table_catalog NOT IN ('system', '__databricks_internal')\n",
    "        AND t.table_type IN ('MANAGED', 'EXTERNAL')\n",
    "        AND DATEDIFF(CURRENT_DATE(), DATE(t.last_altered)) <= 180\n",
    "        {catalog_filter}\n",
    "    \"\"\"\n",
    "\n",
    "    log(\"Executing optimized query...\")\n",
    "    sensitive_columns_classified = spark.sql(query)\n",
    "    \n",
    "    # Filter to HIGH and MEDIUM only\n",
    "    log(\"Filtering to HIGH and MEDIUM sensitivity columns...\")\n",
    "    sensitive_spark = sensitive_columns_classified.filter(\n",
    "        F.col('sensitivity_level').isin(['HIGH', 'MEDIUM'])\n",
    "    )\n",
    "    \n",
    "    # Get counts using single-pass aggregation\n",
    "    log(\"Calculating sensitivity statistics...\")\n",
    "    sensitivity_counts = sensitive_spark.groupBy('sensitivity_level').count().collect()\n",
    "    \n",
    "    # Extract counts\n",
    "    count_dict = {row['sensitivity_level']: row['count'] for row in sensitivity_counts}\n",
    "    high_count = count_dict.get('HIGH', 0)\n",
    "    medium_count = count_dict.get('MEDIUM', 0)\n",
    "    total_sensitive = high_count + medium_count\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCCA Sensitive columns identified (recently modified tables only):\")\n",
    "    log(f\"  HIGH sensitivity: {high_count:,}\")\n",
    "    log(f\"  MEDIUM sensitivity: {medium_count:,}\")\n",
    "    log(f\"  Total sensitive: {total_sensitive:,}\")\n",
    "    log(f\"\\nℹ️  Note: This is a sample based on tables modified in last 180 days\")\n",
    "    log(f\"     For full scan, use the previous cell (slower but comprehensive)\")\n",
    "    \n",
    "    # Convert to pandas for downstream analysis\n",
    "    log(\"Converting to pandas for analysis...\")\n",
    "    if total_sensitive > 50000:\n",
    "        log(f\"⚠️  Large dataset ({total_sensitive:,} rows), limiting to 50,000\")\n",
    "        sensitive_data = sensitive_spark.limit(50000).toPandas()\n",
    "    else:\n",
    "        sensitive_data = sensitive_spark.toPandas()\n",
    "    \n",
    "    log(f\"Loaded {len(sensitive_data):,} sensitive columns into memory\")\n",
    "    display(sensitive_data.head(20))\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error identifying sensitive columns: {str(e)}\")\n",
    "    sensitive_data = None\n",
    "    \n",
    "log_execution_time(\"Identify Sensitive Columns (Sample-Based)\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21441d4b-9072-45cc-8e38-1fe83dd2abe1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Identify Sensitive Columns"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"IDENTIFYING SENSITIVE COLUMNS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Build catalog filter for performance\n",
    "    if SCAN_SPECIFIC_CATALOGS:\n",
    "        catalog_list = \"', '\".join(CATALOGS_TO_SCAN)\n",
    "        catalog_filter = f\"AND table_catalog IN ('{catalog_list}')\"\n",
    "        log(f\"Scanning specific catalogs: {', '.join(CATALOGS_TO_SCAN)}\")\n",
    "    else:\n",
    "        catalog_filter = \"\"\n",
    "        log(\"Scanning ALL catalogs (this may take longer)\")\n",
    "    \n",
    "    # Build PII pattern for SQL RLIKE\n",
    "    pii_pattern = '|'.join(pii_keywords)\n",
    "    medium_pattern = 'name|user|customer|employee'\n",
    "    \n",
    "    # Use SQL-based classification (fastest approach - single query)\n",
    "    log(\"Querying and classifying columns using SQL...\")\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        table_catalog,\n",
    "        table_schema,\n",
    "        table_name,\n",
    "        column_name,\n",
    "        data_type,\n",
    "        comment,\n",
    "        CONCAT(table_catalog, '.', table_schema, '.', table_name) as full_table_name,\n",
    "        CASE \n",
    "            WHEN LOWER(column_name) RLIKE '{pii_pattern}' THEN 'HIGH'\n",
    "            WHEN LOWER(column_name) RLIKE '{medium_pattern}' THEN 'MEDIUM'\n",
    "            ELSE 'LOW'\n",
    "        END as sensitivity_level\n",
    "    FROM system.information_schema.columns\n",
    "    WHERE table_catalog NOT IN ('system', '__databricks_internal')\n",
    "    {catalog_filter}\n",
    "    \"\"\"\n",
    "\n",
    "    sensitive_columns_classified = spark.sql(query)\n",
    "    \n",
    "    # Filter to HIGH and MEDIUM only (push-down filter)\n",
    "    log(\"Filtering to HIGH and MEDIUM sensitivity columns...\")\n",
    "    sensitive_spark = sensitive_columns_classified.filter(\n",
    "        F.col('sensitivity_level').isin(['HIGH', 'MEDIUM'])\n",
    "    )\n",
    "    \n",
    "    # Get counts using single-pass aggregation\n",
    "    log(\"Calculating sensitivity statistics...\")\n",
    "    sensitivity_counts = sensitive_spark.groupBy('sensitivity_level').count().collect()\n",
    "    \n",
    "    # Extract counts from results\n",
    "    count_dict = {row['sensitivity_level']: row['count'] for row in sensitivity_counts}\n",
    "    high_count = count_dict.get('HIGH', 0)\n",
    "    medium_count = count_dict.get('MEDIUM', 0)\n",
    "    total_sensitive = high_count + medium_count\n",
    "    \n",
    "    log(f\"\\n\uD83D\uDCCA Sensitive columns identified:\")\n",
    "    log(f\"  HIGH sensitivity: {high_count:,}\")\n",
    "    log(f\"  MEDIUM sensitivity: {medium_count:,}\")\n",
    "    log(f\"  Total sensitive: {total_sensitive:,}\")\n",
    "    \n",
    "    # Convert only a sample to pandas for display and downstream analysis\n",
    "    log(\"Converting sample to pandas for analysis...\")\n",
    "    if total_sensitive > 100000:\n",
    "        log(f\"⚠️  Large dataset detected ({total_sensitive:,} rows), limiting to 100,000 for memory efficiency\")\n",
    "        sensitive_data = sensitive_spark.limit(100000).toPandas()\n",
    "    else:\n",
    "        sensitive_data = sensitive_spark.toPandas()\n",
    "    \n",
    "    log(f\"Loaded {len(sensitive_data):,} sensitive columns into memory\")\n",
    "    display(sensitive_data.head(20))\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error identifying sensitive columns: {str(e)}\")\n",
    "    sensitive_data = None\n",
    "    \n",
    "log_execution_time(\"Identify Sensitive Columns\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "524c3fa8-79e5-46b2-ab69-54bc3392c236",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sensitive Tables Summary"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"AGGREGATING SENSITIVE TABLES\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if sensitive_data is not None and len(sensitive_data) > 0:\n",
    "    try:\n",
    "        # Aggregate sensitive columns by table\n",
    "        log(\"Grouping sensitive columns by table...\")\n",
    "        sensitive_tables = sensitive_data.groupby(['table_catalog', 'table_schema', 'table_name', 'full_table_name']).agg({\n",
    "            'column_name': 'count',\n",
    "            'sensitivity_level': lambda x: 'HIGH' if 'HIGH' in x.values else 'MEDIUM'\n",
    "        }).reset_index()\n",
    "\n",
    "        sensitive_tables.columns = ['catalog', 'schema', 'table', 'full_table_name', 'sensitive_column_count', 'max_sensitivity']\n",
    "        sensitive_tables = sensitive_tables.sort_values('sensitive_column_count', ascending=False)\n",
    "\n",
    "        high_tables = len(sensitive_tables[sensitive_tables['max_sensitivity'] == 'HIGH'])\n",
    "        medium_tables = len(sensitive_tables[sensitive_tables['max_sensitivity'] == 'MEDIUM'])\n",
    "        \n",
    "        log(f\"\\n\uD83D\uDCCA Tables with sensitive data: {len(sensitive_tables):,}\")\n",
    "        log(f\"  HIGH sensitivity tables: {high_tables:,}\")\n",
    "        log(f\"  MEDIUM sensitivity tables: {medium_tables:,}\")\n",
    "\n",
    "        display(sensitive_tables.head(20))\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error aggregating sensitive tables: {str(e)}\")\n",
    "        sensitive_tables = None\n",
    "else:\n",
    "    log(\"⚠️  No sensitive data to aggregate\")\n",
    "    sensitive_tables = None\n",
    "    \n",
    "log_execution_time(\"Sensitive Tables Summary\", cell_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d54dd3-4f53-4f71-8f3d-689d93ee2744",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Section: Audit Trail Analysis"
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 2. Audit Trail Analysis\n",
    "\n",
    "Analyzing workspace audit logs to track:\n",
    "* Access to sensitive tables\n",
    "* Data modification events\n",
    "* Permission changes\n",
    "* Data export activities\n",
    "* Unusual access patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0889c42b-0cb4-4fe9-b46f-c6aefc32e41f",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Audit Logs"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"LOADING AUDIT LOGS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Query audit logs for the analysis period\n",
    "    log(f\"Querying system.access.audit from {start_date} to {end_date}...\")\n",
    "    audit_query = f\"\"\"\n",
    "    SELECT \n",
    "        event_time,\n",
    "        event_date,\n",
    "        workspace_id,\n",
    "        user_identity.email as user_email,\n",
    "        service_name,\n",
    "        action_name,\n",
    "        request_id,\n",
    "        request_params,\n",
    "        response.status_code as status_code,\n",
    "        response.error_message as error_message,\n",
    "        source_ip_address\n",
    "    FROM system.access.audit\n",
    "    WHERE event_date >= '{start_date}'\n",
    "        AND event_date <= '{end_date}'\n",
    "        AND action_name IS NOT NULL\n",
    "    ORDER BY event_time DESC\n",
    "    \"\"\"\n",
    "\n",
    "    audit_logs_df = spark.sql(audit_query)\n",
    "    total_events = audit_logs_df.count()\n",
    "\n",
    "    log(f\"✓ Total audit events: {total_events:,}\")\n",
    "\n",
    "    # Cache for performance\n",
    "    audit_logs_df = safe_cache(audit_logs_df, \"audit_logs_df\")\n",
    "\n",
    "    # Show sample\n",
    "    display(audit_logs_df.limit(10))\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error loading audit logs: {str(e)}\")\n",
    "    audit_logs_df = None\n",
    "    total_events = 0\n",
    "    \n",
    "log_execution_time(\"Load Audit Logs\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb1b8952-f0af-4d26-8c1d-62058593cd7b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Categorize Audit Events"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"CATEGORIZING AUDIT EVENTS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if validate_dataframe_exists(\"audit_logs_df\", audit_logs_df):\n",
    "    try:\n",
    "        # For large datasets, use Spark UDF instead of pandas\n",
    "        from pyspark.sql.functions import udf\n",
    "        from pyspark.sql.types import StringType\n",
    "        \n",
    "        # Create UDF for categorization\n",
    "        categorize_udf = udf(categorize_action, StringType())\n",
    "        \n",
    "        log(\"Categorizing events by action type using Spark...\")\n",
    "        audit_logs_categorized = audit_logs_df.withColumn('event_category', categorize_udf(F.col('action_name')))\n",
    "        \n",
    "        # Aggregate summary using Spark\n",
    "        event_summary_spark = audit_logs_categorized.groupBy('event_category').agg(\n",
    "            F.count('request_id').alias('event_count'),\n",
    "            F.countDistinct('user_email').alias('unique_users')\n",
    "        ).orderBy(F.desc('event_count'))\n",
    "        \n",
    "        # Convert only the summary to pandas (small dataset)\n",
    "        event_summary = event_summary_spark.toPandas()\n",
    "        \n",
    "        log(\"\\n\uD83D\uDCCA Audit Event Summary by Category:\")\n",
    "        for _, row in event_summary.iterrows():\n",
    "            log(f\"  {row['event_category']}: {row['event_count']:,} events ({row['unique_users']} users)\")\n",
    "        \n",
    "        display(event_summary)\n",
    "        \n",
    "        # Store the categorized Spark DataFrame for downstream use\n",
    "        audit_logs_categorized.createOrReplaceTempView(\"audit_logs_categorized\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error categorizing audit events: {str(e)}\")\n",
    "        audit_logs_categorized = None\n",
    "        event_summary = None\n",
    "else:\n",
    "    log(\"⚠️  No audit logs to categorize\")\n",
    "    audit_logs_categorized = None\n",
    "    event_summary = None\n",
    "    \n",
    "log_execution_time(\"Categorize Audit Events\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4292a0c-2837-4d4c-9bd0-13ff1cad2c0c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Filter Service Principals"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"FILTERING SERVICE PRINCIPALS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if 'audit_logs_categorized' in locals() and audit_logs_categorized is not None:\n",
    "    try:\n",
    "        # Get counts before filtering\n",
    "        total_before = audit_logs_categorized.count()\n",
    "        users_before = audit_logs_categorized.select('user_email').distinct().count()\n",
    "        \n",
    "        if FILTER_SERVICE_PRINCIPALS:\n",
    "            log(\"Applying service principal filters...\")\n",
    "            \n",
    "            # Apply filtering\n",
    "            audit_logs_users_only = filter_service_principals_spark(audit_logs_categorized, 'user_email')\n",
    "            \n",
    "            # Get counts after filtering\n",
    "            total_after = audit_logs_users_only.count()\n",
    "            users_after = audit_logs_users_only.select('user_email').distinct().count()\n",
    "            \n",
    "            events_filtered = total_before - total_after\n",
    "            users_filtered = users_before - users_after\n",
    "            \n",
    "            log(f\"\\n\uD83D\uDCCA Filtering Results:\")\n",
    "            log(f\"  Events before: {total_before:,}\")\n",
    "            log(f\"  Events after: {total_after:,}\")\n",
    "            log(f\"  Events filtered: {events_filtered:,} ({events_filtered/total_before*100:.1f}%)\")\n",
    "            log(f\"  Users before: {users_before:,}\")\n",
    "            log(f\"  Users after: {users_after:,}\")\n",
    "            log(f\"  Service principals filtered: {users_filtered:,}\")\n",
    "            \n",
    "            # Create temp view for downstream analysis\n",
    "            audit_logs_users_only.createOrReplaceTempView(\"audit_logs_users_only\")\n",
    "            log(\"\\n✓ Created view: audit_logs_users_only (for user-focused analysis)\")\n",
    "            log(\"✓ Original view: audit_logs_categorized (includes all accounts)\")\n",
    "        else:\n",
    "            log(\"ℹ️  Service principal filtering is DISABLED\")\n",
    "            audit_logs_users_only = audit_logs_categorized\n",
    "            audit_logs_users_only.createOrReplaceTempView(\"audit_logs_users_only\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error filtering service principals: {str(e)}\")\n",
    "        audit_logs_users_only = audit_logs_categorized\n",
    "else:\n",
    "    log(\"⚠️  No audit logs to filter\")\n",
    "    audit_logs_users_only = None\n",
    "\n",
    "log_execution_time(\"Filter Service Principals\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac69f645-cb81-4dab-8837-d833d1a5b190",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Sensitive Data Access Tracking"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"SENSITIVE DATA ACCESS TRACKING\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if 'audit_logs_categorized' in locals() and audit_logs_categorized is not None:\n",
    "    try:\n",
    "        # Filter for data access events using Spark\n",
    "        log(\"Filtering data access events...\")\n",
    "        data_access_events_spark = audit_logs_categorized.filter(F.col('event_category') == 'DATA_ACCESS')\n",
    "        data_access_count = data_access_events_spark.count()\n",
    "        \n",
    "        log(f\"Total data access events: {data_access_count:,}\")\n",
    "\n",
    "        if data_access_count > 0:\n",
    "            # Analyze access patterns using Spark aggregations\n",
    "            log(\"Analyzing access patterns by user...\")\n",
    "            access_by_user_spark = data_access_events_spark.groupBy('user_email').agg(\n",
    "                F.count('request_id').alias('access_count'),\n",
    "                F.min('event_date').alias('first_access'),\n",
    "                F.max('event_date').alias('last_access')\n",
    "            ).orderBy(F.desc('access_count'))\n",
    "            \n",
    "            # Convert only top results to pandas for display\n",
    "            access_by_user = access_by_user_spark.limit(100).toPandas()\n",
    "            \n",
    "            log(f\"\\nUnique users with data access: {access_by_user_spark.count():,}\")\n",
    "            log(f\"Top data accessors:\")\n",
    "            display(access_by_user.head(20))\n",
    "        else:\n",
    "            log(\"No data access events found in the audit logs for this period.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error tracking sensitive data access: {str(e)}\")\n",
    "else:\n",
    "    log(\"⚠️  No categorized audit logs available\")\n",
    "\n",
    "log_execution_time(\"Sensitive Data Access Tracking\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e4c1b65-27c2-4cab-bad0-73a384068431",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Permission Changes Audit"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"PERMISSION CHANGES AUDIT\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if 'audit_logs_categorized' in locals() and audit_logs_categorized is not None:\n",
    "    try:\n",
    "        # Track permission changes using Spark\n",
    "        log(\"Filtering permission change events...\")\n",
    "        permission_changes_spark = audit_logs_categorized.filter(F.col('event_category') == 'PERMISSION_CHANGE')\n",
    "        perm_change_count = permission_changes_spark.count()\n",
    "        \n",
    "        log(f\"Total permission change events: {perm_change_count:,}\")\n",
    "\n",
    "        if perm_change_count > 0:\n",
    "            # Analyze permission changes\n",
    "            log(\"Analyzing permission changes...\")\n",
    "            perm_summary_spark = permission_changes_spark.groupBy('user_email', 'action_name').agg(\n",
    "                F.count('request_id').alias('change_count'),\n",
    "                F.min('event_date').alias('first_change'),\n",
    "                F.max('event_date').alias('last_change')\n",
    "            ).orderBy(F.desc('change_count'))\n",
    "            \n",
    "            # Convert top results to pandas\n",
    "            perm_summary = perm_summary_spark.limit(100).toPandas()\n",
    "            perm_summary.columns = ['user_email', 'action', 'change_count', 'first_change', 'last_change']\n",
    "            \n",
    "            log(\"\\nPermission Changes Summary:\")\n",
    "            display(perm_summary.head(20))\n",
    "            \n",
    "            # Recent permission changes (last 7 days)\n",
    "            from datetime import datetime, timedelta\n",
    "            recent_cutoff = (datetime.now(eastern) - timedelta(days=7)).strftime('%Y-%m-%d')\n",
    "            recent_perms_spark = permission_changes_spark.filter(F.col('event_date') >= recent_cutoff)\n",
    "            recent_count = recent_perms_spark.count()\n",
    "            \n",
    "            log(f\"\\nRecent permission changes (last 7 days): {recent_count:,}\")\n",
    "            if recent_count > 0:\n",
    "                recent_perms = recent_perms_spark.select('event_time', 'user_email', 'action_name', 'service_name').limit(20).toPandas()\n",
    "                display(recent_perms)\n",
    "        else:\n",
    "            log(\"No permission change events found in the audit logs.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error auditing permission changes: {str(e)}\")\n",
    "else:\n",
    "    log(\"⚠️  No categorized audit logs available\")\n",
    "\n",
    "log_execution_time(\"Permission Changes Audit\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b0fc23a-7325-447c-a601-c40b57f3157b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Data Export Monitoring"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"DATA EXPORT MONITORING\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if 'audit_logs_categorized' in locals() and audit_logs_categorized is not None:\n",
    "    try:\n",
    "        # Monitor data export activities using Spark\n",
    "        log(\"Filtering data export events...\")\n",
    "        export_events_spark = audit_logs_categorized.filter(F.col('event_category') == 'DATA_EXPORT')\n",
    "        export_count = export_events_spark.count()\n",
    "        \n",
    "        log(f\"Total data export events: {export_count:,}\")\n",
    "\n",
    "        if export_count > 0:\n",
    "            # Analyze exports\n",
    "            log(\"Analyzing export patterns...\")\n",
    "            export_summary_spark = export_events_spark.groupBy('user_email', 'action_name').agg(\n",
    "                F.count('request_id').alias('export_count'),\n",
    "                F.min('event_date').alias('first_export'),\n",
    "                F.max('event_date').alias('last_export')\n",
    "            ).orderBy(F.desc('export_count'))\n",
    "            \n",
    "            # Convert top results to pandas\n",
    "            export_summary = export_summary_spark.limit(100).toPandas()\n",
    "            export_summary.columns = ['user_email', 'export_action', 'export_count', 'first_export', 'last_export']\n",
    "            \n",
    "            log(\"\\nData Export Summary:\")\n",
    "            display(export_summary.head(20))\n",
    "            \n",
    "            # Flag high-volume exporters\n",
    "            high_volume_exporters = export_summary[export_summary['export_count'] > 50]\n",
    "            if len(high_volume_exporters) > 0:\n",
    "                log(f\"\\n⚠️ HIGH VOLUME EXPORTERS (>50 exports): {len(high_volume_exporters)}\")\n",
    "                display(high_volume_exporters)\n",
    "            else:\n",
    "                log(\"✅ No high-volume exporters detected\")\n",
    "        else:\n",
    "            log(\"No data export events found in the audit logs.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error monitoring data exports: {str(e)}\")\n",
    "else:\n",
    "    log(\"⚠️  No categorized audit logs available\")\n",
    "\n",
    "log_execution_time(\"Data Export Monitoring\", cell_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7663179d-2d84-4953-b826-d1e17e035fbc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Section: Data Retention Compliance"
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 3. Data Retention Compliance\n",
    "\n",
    "Monitoring table retention against organizational policies:\n",
    "* Tables exceeding maximum retention period\n",
    "* Tables below minimum retention requirements\n",
    "* Stale data identification\n",
    "* Retention policy recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "735ae99e-74bb-4669-9f2c-b9227c4d64ac",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Analyze Table Retention"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"ANALYZING TABLE RETENTION\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Query table metadata for retention analysis\n",
    "    log(\"Querying system.information_schema.tables...\")\n",
    "    retention_query = \"\"\"\n",
    "    SELECT \n",
    "        table_catalog,\n",
    "        table_schema,\n",
    "        table_name,\n",
    "        table_type,\n",
    "        CONCAT(table_catalog, '.', table_schema, '.', table_name) as full_table_name,\n",
    "        created,\n",
    "        last_altered,\n",
    "        DATEDIFF(CURRENT_DATE(), DATE(last_altered)) as days_since_modified,\n",
    "        DATEDIFF(CURRENT_DATE(), DATE(created)) as days_since_created,\n",
    "        comment\n",
    "    FROM system.information_schema.tables\n",
    "    WHERE table_catalog NOT IN ('system', '__databricks_internal')\n",
    "        AND table_type IN ('MANAGED', 'EXTERNAL')\n",
    "    ORDER BY days_since_modified DESC\n",
    "    \"\"\"\n",
    "\n",
    "    retention_df = spark.sql(retention_query)\n",
    "    total_tables = retention_df.count()\n",
    "\n",
    "    log(f\"✓ Total tables analyzed: {total_tables:,}\")\n",
    "\n",
    "    # Cache for performance\n",
    "    retention_df = safe_cache(retention_df, \"retention_df\")\n",
    "\n",
    "    # Convert to pandas for analysis\n",
    "    log(\"Converting to pandas for retention analysis...\")\n",
    "    retention_pd = retention_df.toPandas()\n",
    "\n",
    "    # Apply retention status classification\n",
    "    log(\"Classifying retention status...\")\n",
    "    retention_pd['retention_status'] = retention_pd['days_since_modified'].apply(calculate_retention_status)\n",
    "\n",
    "    display(retention_pd.head(20))\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error analyzing table retention: {str(e)}\")\n",
    "    retention_pd = None\n",
    "    total_tables = 0\n",
    "    \n",
    "log_execution_time(\"Analyze Table Retention\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f77f9d2c-376e-43c3-8bd8-d65991364d32",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Retention Compliance Summary"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"RETENTION COMPLIANCE SUMMARY\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if retention_pd is not None and len(retention_pd) > 0:\n",
    "    try:\n",
    "        # Summarize retention compliance\n",
    "        retention_summary = retention_pd.groupby('retention_status').agg({\n",
    "            'full_table_name': 'count'\n",
    "        }).reset_index()\n",
    "        retention_summary.columns = ['retention_status', 'table_count']\n",
    "        retention_summary = retention_summary.sort_values('table_count', ascending=False)\n",
    "\n",
    "        log(\"\\n\uD83D\uDCCA Data Retention Compliance Summary:\")\n",
    "        display(retention_summary)\n",
    "\n",
    "        # Calculate percentages\n",
    "        retention_summary['percentage'] = (retention_summary['table_count'] / total_tables * 100).round(2)\n",
    "        log(\"\\nRetention Status Distribution:\")\n",
    "        for _, row in retention_summary.iterrows():\n",
    "            status_icon = \"✅\" if row['retention_status'] in ['COMPLIANT', 'WITHIN_POLICY'] else \"⚠️\"\n",
    "            log(f\"  {status_icon} {row['retention_status']}: {row['table_count']:,} tables ({row['percentage']}%)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error summarizing retention compliance: {str(e)}\")\n",
    "        retention_summary = None\n",
    "else:\n",
    "    log(\"⚠️  No retention data to summarize\")\n",
    "    retention_summary = None\n",
    "    \n",
    "log_execution_time(\"Retention Compliance Summary\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28aeab72-5d5c-4011-9d11-63b7c265c780",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Tables Exceeding Retention Policy"
    }
   },
   "outputs": [],
   "source": [
    "# Identify tables exceeding maximum retention period (compliance risk)\n",
    "exceeds_policy = retention_pd[retention_pd['retention_status'] == 'EXCEEDS_POLICY'].copy()\n",
    "exceeds_policy = exceeds_policy.sort_values('days_since_modified', ascending=False)\n",
    "\n",
    "print(f\"⚠️ Tables exceeding retention policy ({max_retention_days} days): {len(exceeds_policy):,}\")\n",
    "\n",
    "if len(exceeds_policy) > 0:\n",
    "    print(f\"\\nOldest tables (by last modification):\")\n",
    "    display(exceeds_policy[['full_table_name', 'table_type', 'days_since_modified', 'days_since_created', 'last_altered']].head(30))\n",
    "    \n",
    "    # Check if any sensitive tables exceed retention\n",
    "    if 'sensitive_tables' in locals():\n",
    "        sensitive_exceeds = exceeds_policy[exceeds_policy['full_table_name'].isin(sensitive_tables['full_table_name'])]\n",
    "        if len(sensitive_exceeds) > 0:\n",
    "            print(f\"\\n⚠️⚠️ CRITICAL: {len(sensitive_exceeds)} SENSITIVE tables exceed retention policy!\")\n",
    "            display(sensitive_exceeds[['full_table_name', 'days_since_modified', 'last_altered']].head(20))\n",
    "else:\n",
    "    print(\"✅ No tables exceed the retention policy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4c25488-bca0-4214-8a63-a1d9e45d47fd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Stale Data Identification"
    }
   },
   "outputs": [],
   "source": [
    "# Identify stale tables (not modified in 180+ days)\n",
    "stale_threshold = 180\n",
    "stale_tables = retention_pd[retention_pd['days_since_modified'] >= stale_threshold].copy()\n",
    "stale_tables = stale_tables.sort_values('days_since_modified', ascending=False)\n",
    "\n",
    "print(f\"Stale tables (not modified in {stale_threshold}+ days): {len(stale_tables):,}\")\n",
    "\n",
    "if len(stale_tables) > 0:\n",
    "    # Categorize by staleness\n",
    "    stale_tables['staleness_category'] = pd.cut(\n",
    "        stale_tables['days_since_modified'],\n",
    "        bins=[180, 365, 730, 1825, float('inf')],\n",
    "        labels=['6mo-1yr', '1-2yrs', '2-5yrs', '5+yrs']\n",
    "    )\n",
    "    \n",
    "    staleness_summary = stale_tables.groupby('staleness_category').size().reset_index(name='count')\n",
    "    print(\"\\nStaleness Distribution:\")\n",
    "    display(staleness_summary)\n",
    "    \n",
    "    print(\"\\nTop 30 stale tables:\")\n",
    "    display(stale_tables[['full_table_name', 'table_type', 'days_since_modified', 'last_altered', 'staleness_category']].head(30))\n",
    "else:\n",
    "    print(\"✅ No stale tables found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b4456f4-7ef3-4cd3-88c6-11225a02b8bc",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Section: Regulatory Compliance Tracking"
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 4. Regulatory Compliance Tracking\n",
    "\n",
    "Compliance checks for major regulations:\n",
    "* **GDPR** - Right to erasure, data portability, consent tracking\n",
    "* **CCPA** - Consumer data rights, opt-out tracking\n",
    "* **SOX** - Financial data access controls, audit trails\n",
    "* **HIPAA** - Healthcare data protection (if applicable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c18f306-8caa-4d6e-b3b3-657f282f7c37",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "GDPR Compliance Check"
    }
   },
   "outputs": [],
   "source": [
    "# GDPR Compliance: Check for tables with EU personal data\n",
    "print(\"=== GDPR Compliance Analysis ===\")\n",
    "print(\"\\nKey Requirements:\")\n",
    "print(\"  • Right to erasure (data deletion capability)\")\n",
    "print(\"  • Data portability (export capability)\")\n",
    "print(\"  • Consent tracking\")\n",
    "print(\"  • Data retention limits\")\n",
    "\n",
    "# Identify tables likely containing EU personal data\n",
    "if 'sensitive_tables' in locals() and len(sensitive_tables) > 0:\n",
    "    gdpr_relevant_tables = sensitive_tables[sensitive_tables['max_sensitivity'] == 'HIGH'].copy()\n",
    "    print(f\"\\nTables with HIGH sensitivity (potential GDPR scope): {len(gdpr_relevant_tables):,}\")\n",
    "    \n",
    "    # Cross-reference with retention compliance\n",
    "    if 'retention_pd' in locals():\n",
    "        gdpr_retention = gdpr_relevant_tables.merge(\n",
    "            retention_pd[['full_table_name', 'days_since_modified', 'retention_status']],\n",
    "            on='full_table_name',\n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Flag GDPR violations (sensitive data exceeding retention)\n",
    "        gdpr_violations = gdpr_retention[gdpr_retention['retention_status'] == 'EXCEEDS_POLICY']\n",
    "        \n",
    "        if len(gdpr_violations) > 0:\n",
    "            print(f\"\\n⚠️ GDPR RISK: {len(gdpr_violations)} sensitive tables exceed retention policy\")\n",
    "            display(gdpr_violations[['full_table_name', 'sensitive_column_count', 'days_since_modified']].head(20))\n",
    "        else:\n",
    "            print(\"\\n✅ No GDPR retention violations detected\")\n",
    "    \n",
    "    # Check for deletion tracking (look for deleted_at, is_deleted columns)\n",
    "    if 'sensitive_data' in locals():\n",
    "        deletion_columns = sensitive_data[\n",
    "            sensitive_data['column_name'].str.lower().str.contains('delet|remov|erasure', na=False)\n",
    "        ]\n",
    "        tables_with_deletion = deletion_columns['full_table_name'].nunique()\n",
    "        print(f\"\\nTables with deletion tracking columns: {tables_with_deletion:,}\")\n",
    "        if tables_with_deletion > 0:\n",
    "            print(\"✅ Deletion capability appears to be implemented\")\n",
    "        else:\n",
    "            print(\"⚠️ Consider implementing soft-delete columns for GDPR compliance\")\n",
    "else:\n",
    "    print(\"\\nNo sensitive tables identified for GDPR analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94cf8d98-0213-4e42-9ab5-e03beac6dd82",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "CCPA Compliance Check"
    }
   },
   "outputs": [],
   "source": [
    "# CCPA Compliance: California Consumer Privacy Act\n",
    "print(\"=== CCPA Compliance Analysis ===\")\n",
    "print(\"\\nKey Requirements:\")\n",
    "print(\"  • Right to know (data access)\")\n",
    "print(\"  • Right to delete\")\n",
    "print(\"  • Right to opt-out of sale\")\n",
    "print(\"  • Non-discrimination\")\n",
    "\n",
    "# Check for opt-out tracking\n",
    "if 'sensitive_data' in locals():\n",
    "    opt_out_columns = sensitive_data[\n",
    "        sensitive_data['column_name'].str.lower().str.contains('opt.?out|consent|preference', na=False, regex=True)\n",
    "    ]\n",
    "    tables_with_optout = opt_out_columns['full_table_name'].nunique()\n",
    "    \n",
    "    print(f\"\\nTables with opt-out/consent tracking: {tables_with_optout:,}\")\n",
    "    if tables_with_optout > 0:\n",
    "        print(\"✅ Opt-out tracking appears to be implemented\")\n",
    "        display(opt_out_columns[['full_table_name', 'column_name', 'data_type']].head(20))\n",
    "    else:\n",
    "        print(\"⚠️ Consider implementing opt-out tracking for CCPA compliance\")\n",
    "\n",
    "# Check for California-specific data\n",
    "if 'sensitive_data' in locals():\n",
    "    ca_columns = sensitive_data[\n",
    "        sensitive_data['column_name'].str.lower().str.contains('california|ca_resident|state', na=False)\n",
    "    ]\n",
    "    if len(ca_columns) > 0:\n",
    "        print(f\"\\nTables with California-related columns: {ca_columns['full_table_name'].nunique():,}\")\n",
    "        print(\"⚠️ These tables may require CCPA compliance measures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2443a5de-12b6-4b42-8e69-32ca6e23e888",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SOX Compliance Check"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"SOX COMPLIANCE CHECK\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # SOX Compliance: Sarbanes-Oxley Act (financial data controls)\n",
    "    log(\"=== SOX Compliance Analysis ===\")\n",
    "    log(\"\\nKey Requirements:\")\n",
    "    log(\"  • Financial data access controls\")\n",
    "    log(\"  • Audit trail completeness\")\n",
    "    log(\"  • Segregation of duties\")\n",
    "    log(\"  • Change management tracking\")\n",
    "\n",
    "    # Identify financial data tables\n",
    "    if 'sensitive_data' in locals() and sensitive_data is not None:\n",
    "        financial_keywords = ['revenue', 'invoice', 'payment', 'transaction', 'financial', \n",
    "                             'accounting', 'ledger', 'balance', 'cost', 'price', 'amount']\n",
    "        \n",
    "        financial_columns = sensitive_data[\n",
    "            sensitive_data['column_name'].str.lower().str.contains('|'.join(financial_keywords), na=False)\n",
    "        ]\n",
    "        financial_tables = financial_columns['full_table_name'].unique()\n",
    "        \n",
    "        log(f\"\\nTables with financial data: {len(financial_tables):,}\")\n",
    "        \n",
    "        if len(financial_tables) > 0:\n",
    "            log(\"\\nFinancial tables requiring SOX controls:\")\n",
    "            financial_summary = financial_columns.groupby('full_table_name').agg({\n",
    "                'column_name': 'count'\n",
    "            }).reset_index()\n",
    "            financial_summary.columns = ['full_table_name', 'financial_column_count']\n",
    "            financial_summary = financial_summary.sort_values('financial_column_count', ascending=False)\n",
    "            display(financial_summary.head(20))\n",
    "            \n",
    "            # Check audit trail coverage for financial tables\n",
    "            if 'audit_logs_categorized' in locals() and audit_logs_categorized is not None:\n",
    "                audit_count = audit_logs_categorized.count()\n",
    "                log(f\"\\n✅ Audit logging is active (SOX requirement met)\")\n",
    "                log(f\"   Total audit events in period: {audit_count:,}\")\n",
    "            else:\n",
    "                log(\"\\n⚠️ Limited audit data available - ensure comprehensive logging for SOX\")\n",
    "        else:\n",
    "            log(\"\\nNo financial data tables identified.\")\n",
    "    else:\n",
    "        log(\"\\nNo sensitive data available for SOX analysis.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error in SOX compliance check: {str(e)}\")\n",
    "\n",
    "log_execution_time(\"SOX Compliance Check\", cell_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cfe77de-922d-47ef-9979-687f8970efbe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Section: Anomaly Detection"
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 5. Access Pattern Anomaly Detection\n",
    "\n",
    "Identifying unusual access patterns that may indicate:\n",
    "* Unauthorized access attempts\n",
    "* Data exfiltration\n",
    "* Insider threats\n",
    "* Compromised accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a21f7a-6806-4722-bca1-31f4088dc5c4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Detect Unusual Access Patterns"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"DETECTING UNUSUAL ACCESS PATTERNS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Anomaly detection: Unusual access patterns (USER ACCOUNTS ONLY)\n",
    "    log(\"=== Access Pattern Anomaly Detection (Users Only) ===\")\n",
    "\n",
    "    if 'audit_logs_users_only' in locals() and audit_logs_users_only is not None:\n",
    "        # 1. After-hours access (outside 6 AM - 8 PM) using Spark\n",
    "        log(\"\\nAnalyzing after-hours access patterns...\")\n",
    "        audit_with_hour = audit_logs_users_only.withColumn('event_hour', F.hour(F.col('event_time')))\n",
    "        after_hours_spark = audit_with_hour.filter((F.col('event_hour') < 6) | (F.col('event_hour') > 20))\n",
    "        \n",
    "        after_hours_count = after_hours_spark.count()\n",
    "        total_count = audit_logs_users_only.count()\n",
    "        after_hours_pct = (after_hours_count / total_count * 100) if total_count > 0 else 0\n",
    "        \n",
    "        log(f\"\\nAfter-hours access events: {after_hours_count:,} ({after_hours_pct:.1f}%)\")\n",
    "        \n",
    "        if after_hours_count > 0:\n",
    "            after_hours_users_spark = after_hours_spark.groupBy('user_email').agg(\n",
    "                F.count('request_id').alias('after_hours_count')\n",
    "            ).orderBy(F.desc('after_hours_count'))\n",
    "            \n",
    "            after_hours_users = after_hours_users_spark.limit(20).toPandas()\n",
    "            log(\"\\nTop after-hours users:\")\n",
    "            display(after_hours_users)\n",
    "        \n",
    "        # 2. High-frequency access (potential automated scraping)\n",
    "        log(\"\\nAnalyzing high-frequency access patterns...\")\n",
    "        user_activity_spark = audit_logs_users_only.groupBy('user_email').agg(\n",
    "            F.count('request_id').alias('total_events'),\n",
    "            F.countDistinct('event_date').alias('active_days')\n",
    "        )\n",
    "        user_activity_spark = user_activity_spark.withColumn(\n",
    "            'events_per_day', \n",
    "            F.round(F.col('total_events') / F.col('active_days'), 1)\n",
    "        )\n",
    "        \n",
    "        high_frequency_spark = user_activity_spark.filter(F.col('events_per_day') > 1000)\n",
    "        high_freq_count = high_frequency_spark.count()\n",
    "        \n",
    "        if high_freq_count > 0:\n",
    "            log(f\"\\n⚠️ High-frequency users (>1000 events/day): {high_freq_count}\")\n",
    "            high_frequency = high_frequency_spark.orderBy(F.desc('events_per_day')).limit(20).toPandas()\n",
    "            display(high_frequency)\n",
    "        else:\n",
    "            log(\"\\n✅ No unusually high-frequency access detected\")\n",
    "        \n",
    "        # 3. Failed access attempts\n",
    "        log(\"\\nAnalyzing failed access attempts...\")\n",
    "        failed_attempts_spark = audit_logs_users_only.filter(\n",
    "            F.col('status_code').isNotNull() & (F.col('status_code') >= 400)\n",
    "        )\n",
    "        failed_count = failed_attempts_spark.count()\n",
    "        \n",
    "        if failed_count > 0:\n",
    "            failed_pct = (failed_count / total_count * 100) if total_count > 0 else 0\n",
    "            log(f\"\\nFailed access attempts: {failed_count:,} ({failed_pct:.1f}%)\")\n",
    "            \n",
    "            failed_by_user_spark = failed_attempts_spark.groupBy('user_email').agg(\n",
    "                F.count('request_id').alias('failed_count')\n",
    "            ).orderBy(F.desc('failed_count'))\n",
    "            \n",
    "            failed_by_user = failed_by_user_spark.limit(20).toPandas()\n",
    "            log(\"\\nUsers with most failed attempts:\")\n",
    "            display(failed_by_user)\n",
    "        else:\n",
    "            log(\"\\n✅ No failed access attempts detected\")\n",
    "    else:\n",
    "        log(\"\\nInsufficient audit data for anomaly detection.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error detecting unusual access patterns: {str(e)}\")\n",
    "\n",
    "log_execution_time(\"Detect Unusual Access Patterns\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0da3f6d7-d7ee-45ec-bca1-bfbb5ba339db",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Geographic Access Analysis"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"GEOGRAPHIC ACCESS ANALYSIS\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Analyze access by source IP (geographic anomalies) - USER ACCOUNTS ONLY\n",
    "    log(\"=== Geographic Access Analysis (Users Only) ===\")\n",
    "\n",
    "    if 'audit_logs_users_only' in locals() and audit_logs_users_only is not None:\n",
    "        log(\"\\nAnalyzing source IP patterns...\")\n",
    "        ip_summary_spark = audit_logs_users_only.filter(F.col('source_ip_address').isNotNull()).groupBy('source_ip_address').agg(\n",
    "            F.count('request_id').alias('event_count'),\n",
    "            F.countDistinct('user_email').alias('unique_users')\n",
    "        ).orderBy(F.desc('event_count'))\n",
    "        \n",
    "        unique_ips = ip_summary_spark.count()\n",
    "        log(f\"Unique source IPs: {unique_ips:,}\")\n",
    "        \n",
    "        # Convert top IPs to pandas for display\n",
    "        ip_summary = ip_summary_spark.limit(20).toPandas()\n",
    "        ip_summary.columns = ['source_ip', 'event_count', 'unique_users']\n",
    "        \n",
    "        log(\"\\nTop source IPs by activity:\")\n",
    "        display(ip_summary)\n",
    "        \n",
    "        # Flag IPs with multiple users (potential proxy/VPN)\n",
    "        multi_user_ips_spark = ip_summary_spark.filter(F.col('unique_users') > 5)\n",
    "        multi_user_count = multi_user_ips_spark.count()\n",
    "        \n",
    "        if multi_user_count > 0:\n",
    "            log(f\"\\nIPs with multiple users (>5): {multi_user_count:,}\")\n",
    "            log(\"(May indicate corporate proxy, VPN, or shared infrastructure)\")\n",
    "            multi_user_ips = multi_user_ips_spark.limit(10).toPandas()\n",
    "            multi_user_ips.columns = ['source_ip', 'event_count', 'unique_users']\n",
    "            display(multi_user_ips)\n",
    "        else:\n",
    "            log(\"\\n✅ No unusual multi-user IP patterns detected\")\n",
    "    else:\n",
    "        log(\"\\nSource IP information not available in audit logs.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error in geographic access analysis: {str(e)}\")\n",
    "\n",
    "log_execution_time(\"Geographic Access Analysis\", cell_start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f66f1c82-54d1-44f2-84f5-ffd29e941c9b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Section: Compliance Reporting"
    }
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 6. Compliance Reporting & Visualizations\n",
    "\n",
    "Executive dashboards and detailed reports for compliance stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb387fba-9738-430f-b107-676e91218ebd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compliance Score Calculation"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"CALCULATING COMPLIANCE SCORE\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    compliance_metrics = {}\n",
    "\n",
    "    # 1. Retention compliance (weight: 30%)\n",
    "    if 'retention_summary' in locals() and retention_summary is not None:\n",
    "        compliant_tables = retention_summary[\n",
    "            retention_summary['retention_status'].isin(['COMPLIANT', 'WITHIN_POLICY'])\n",
    "        ]['table_count'].sum()\n",
    "        retention_score = (compliant_tables / total_tables * 100) if total_tables > 0 else 0\n",
    "        compliance_metrics['Retention Compliance'] = retention_score\n",
    "    else:\n",
    "        compliance_metrics['Retention Compliance'] = 0\n",
    "\n",
    "    # 2. Sensitive data protection (weight: 25%)\n",
    "    if 'sensitive_tables' in locals() and sensitive_tables is not None and len(sensitive_tables) > 0:\n",
    "        # Check if sensitive tables have proper controls (simplified check)\n",
    "        sensitive_score = 75  # Baseline score if sensitive data is identified\n",
    "        compliance_metrics['Sensitive Data Protection'] = sensitive_score\n",
    "    else:\n",
    "        compliance_metrics['Sensitive Data Protection'] = 100  # No sensitive data = compliant\n",
    "\n",
    "    # 3. Audit trail coverage (weight: 25%)\n",
    "    if 'audit_logs_pd' in locals() and audit_logs_pd is not None and len(audit_logs_pd) > 0:\n",
    "        audit_score = min(100, (len(audit_logs_pd) / (days_back * 100)) * 100)  # Expect ~100 events/day\n",
    "        compliance_metrics['Audit Trail Coverage'] = audit_score\n",
    "    else:\n",
    "        compliance_metrics['Audit Trail Coverage'] = 0\n",
    "\n",
    "    # 4. Access control compliance (weight: 20%)\n",
    "    if 'permission_changes' in locals() and permission_changes is not None:\n",
    "        # Penalize excessive permission changes\n",
    "        perm_change_rate = len(permission_changes) / days_back\n",
    "        access_score = max(0, 100 - (perm_change_rate * 2))  # Deduct 2 points per change/day\n",
    "        compliance_metrics['Access Control'] = access_score\n",
    "    else:\n",
    "        compliance_metrics['Access Control'] = 100\n",
    "\n",
    "    # Calculate weighted overall score\n",
    "    weights = {\n",
    "        'Retention Compliance': 0.30,\n",
    "        'Sensitive Data Protection': 0.25,\n",
    "        'Audit Trail Coverage': 0.25,\n",
    "        'Access Control': 0.20\n",
    "    }\n",
    "\n",
    "    overall_score = sum(compliance_metrics[k] * weights[k] for k in compliance_metrics.keys())\n",
    "\n",
    "    log(\"\\n\uD83D\uDCCA Compliance Metrics:\")\n",
    "    for metric, score in compliance_metrics.items():\n",
    "        status = \"✅\" if score >= 80 else \"⚠️\" if score >= 60 else \"❌\"\n",
    "        log(f\"  {status} {metric}: {score:.1f}%\")\n",
    "\n",
    "    log(f\"\\n{'='*60}\")\n",
    "    log(f\"Overall Compliance Score: {overall_score:.1f}%\")\n",
    "    log(f\"{'='*60}\")\n",
    "\n",
    "    if overall_score >= 80:\n",
    "        log(\"✅ GOOD - Compliance posture is strong\")\n",
    "    elif overall_score >= 60:\n",
    "        log(\"⚠️ FAIR - Some compliance gaps need attention\")\n",
    "    else:\n",
    "        log(\"❌ POOR - Significant compliance issues require immediate action\")\n",
    "        \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error calculating compliance score: {str(e)}\")\n",
    "    overall_score = 0\n",
    "    compliance_metrics = {}\n",
    "    \n",
    "log_execution_time(\"Compliance Score Calculation\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3eb7fb3e-75fb-4855-9406-9d53649da7a0",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Compliance Visualizations"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "if not is_job_mode and ENABLE_VISUALIZATIONS:\n",
    "    log(\"\\n\" + \"=\"*60)\n",
    "    log(\"GENERATING VISUALIZATIONS\")\n",
    "    log(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        # Create compliance visualizations\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "\n",
    "        sns.set_style('whitegrid')\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "        # 1. Compliance Score Breakdown\n",
    "        if 'compliance_metrics' in locals() and compliance_metrics:\n",
    "            ax1 = axes[0, 0]\n",
    "            metrics_df = pd.DataFrame(list(compliance_metrics.items()), columns=['Metric', 'Score'])\n",
    "            colors = ['green' if s >= 80 else 'orange' if s >= 60 else 'red' for s in metrics_df['Score']]\n",
    "            ax1.barh(metrics_df['Metric'], metrics_df['Score'], color=colors, alpha=0.7)\n",
    "            ax1.set_xlabel('Score (%)', fontsize=12)\n",
    "            ax1.set_title('Compliance Metrics Breakdown', fontsize=14, fontweight='bold')\n",
    "            ax1.set_xlim(0, 100)\n",
    "            for i, v in enumerate(metrics_df['Score']):\n",
    "                ax1.text(v + 2, i, f'{v:.1f}%', va='center', fontsize=10)\n",
    "\n",
    "        # 2. Retention Status Distribution\n",
    "        if 'retention_summary' in locals() and retention_summary is not None:\n",
    "            ax2 = axes[0, 1]\n",
    "            colors_retention = {'COMPLIANT': 'green', 'WITHIN_POLICY': 'lightgreen', \n",
    "                               'EXCEEDS_POLICY': 'red', 'UNKNOWN': 'gray'}\n",
    "            retention_colors = [colors_retention.get(status, 'blue') for status in retention_summary['retention_status']]\n",
    "            ax2.pie(retention_summary['table_count'], labels=retention_summary['retention_status'], \n",
    "                    autopct='%1.1f%%', colors=retention_colors, startangle=90)\n",
    "            ax2.set_title('Data Retention Status', fontsize=14, fontweight='bold')\n",
    "\n",
    "        # 3. Audit Event Categories\n",
    "        if 'event_summary' in locals() and event_summary is not None:\n",
    "            ax3 = axes[1, 0]\n",
    "            top_events = event_summary.head(8)\n",
    "            ax3.bar(range(len(top_events)), top_events['event_count'], color='steelblue', alpha=0.7)\n",
    "            ax3.set_xticks(range(len(top_events)))\n",
    "            ax3.set_xticklabels(top_events['event_category'], rotation=45, ha='right')\n",
    "            ax3.set_ylabel('Event Count', fontsize=12)\n",
    "            ax3.set_title('Audit Events by Category', fontsize=14, fontweight='bold')\n",
    "            ax3.ticklabel_format(style='plain', axis='y')\n",
    "\n",
    "        # 4. Sensitive Data Distribution\n",
    "        if 'sensitive_data' in locals() and sensitive_data is not None and len(sensitive_data) > 0:\n",
    "            ax4 = axes[1, 1]\n",
    "            sensitivity_counts = sensitive_data['sensitivity_level'].value_counts()\n",
    "            colors_sens = {'HIGH': 'red', 'MEDIUM': 'orange', 'LOW': 'yellow'}\n",
    "            sens_colors = [colors_sens.get(level, 'blue') for level in sensitivity_counts.index]\n",
    "            ax4.pie(sensitivity_counts.values, labels=sensitivity_counts.index, \n",
    "                    autopct='%1.1f%%', colors=sens_colors, startangle=90)\n",
    "            ax4.set_title('Sensitive Column Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        log(\"✓ Compliance visualizations generated successfully\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error generating visualizations: {str(e)}\")\n",
    "else:\n",
    "    log(\"ℹ️ Visualizations disabled (job mode or ENABLE_VISUALIZATIONS=False)\")\n",
    "    \n",
    "log_execution_time(\"Compliance Visualizations\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afa942c7-ef61-4b5b-a283-7962ab0631e1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Executive Summary Report"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*70)\n",
    "log(\" \"*15 + \"COMPLIANCE & AUDIT TRAIL REPORT\")\n",
    "log(\" \"*20 + f\"Period: {start_date} to {end_date}\")\n",
    "log(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    if 'overall_score' in locals():\n",
    "        log(f\"\\n\uD83D\uDCCA OVERALL COMPLIANCE SCORE: {overall_score:.1f}%\\n\")\n",
    "    else:\n",
    "        log(f\"\\n\uD83D\uDCCA OVERALL COMPLIANCE SCORE: N/A\\n\")\n",
    "\n",
    "    log(\"\uD83D\uDCCB KEY FINDINGS:\\n\")\n",
    "\n",
    "    # Finding 1: Sensitive Data\n",
    "    if 'sensitive_tables' in locals() and sensitive_tables is not None:\n",
    "        log(f\"1. SENSITIVE DATA INVENTORY\")\n",
    "        log(f\"   • {len(sensitive_tables):,} tables contain sensitive/PII data\")\n",
    "        if 'sensitive_data' in locals() and sensitive_data is not None:\n",
    "            high_count = len(sensitive_data[sensitive_data['sensitivity_level']=='HIGH'])\n",
    "            medium_count = len(sensitive_data[sensitive_data['sensitivity_level']=='MEDIUM'])\n",
    "            log(f\"   • {high_count:,} HIGH sensitivity columns identified\")\n",
    "            log(f\"   • {medium_count:,} MEDIUM sensitivity columns identified\")\n",
    "        log(\"\")\n",
    "\n",
    "    # Finding 2: Audit Activity\n",
    "    if 'total_events' in locals() and total_events > 0:\n",
    "        log(f\"2. AUDIT TRAIL ACTIVITY\")\n",
    "        log(f\"   • {total_events:,} total audit events recorded\")\n",
    "        if 'audit_logs_categorized' in locals() and audit_logs_categorized is not None:\n",
    "            unique_users = audit_logs_categorized.select('user_email').distinct().count()\n",
    "            log(f\"   • {unique_users:,} unique users active\")\n",
    "        if 'event_summary' in locals() and event_summary is not None:\n",
    "            for _, row in event_summary.iterrows():\n",
    "                log(f\"   • {row['event_count']:,} {row['event_category']} events\")\n",
    "        log(\"\")\n",
    "\n",
    "    # Finding 3: Retention Compliance\n",
    "    if 'retention_summary' in locals() and retention_summary is not None:\n",
    "        log(f\"3. DATA RETENTION COMPLIANCE\")\n",
    "        log(f\"   • {total_tables:,} total tables analyzed\")\n",
    "        for _, row in retention_summary.iterrows():\n",
    "            status_icon = \"✅\" if row['retention_status'] in ['COMPLIANT', 'WITHIN_POLICY'] else \"⚠️\"\n",
    "            log(f\"   {status_icon} {row['retention_status']}: {row['table_count']:,} tables\")\n",
    "        log(\"\")\n",
    "\n",
    "    # Finding 4: Anomalies\n",
    "    log(f\"4. SECURITY ANOMALIES\")\n",
    "    anomalies_found = False\n",
    "    \n",
    "    # Check for after-hours access\n",
    "    if 'after_hours_count' in locals() and after_hours_count > 0:\n",
    "        log(f\"   ⚠️ {after_hours_count:,} after-hours access events detected\")\n",
    "        anomalies_found = True\n",
    "    \n",
    "    # Check for high-frequency users\n",
    "    if 'high_freq_count' in locals() and high_freq_count > 0:\n",
    "        log(f\"   ⚠️ {high_freq_count} users with high-frequency access (>1000 events/day)\")\n",
    "        anomalies_found = True\n",
    "    \n",
    "    # Check for failed attempts\n",
    "    if 'failed_count' in locals() and failed_count > 0:\n",
    "        log(f\"   ⚠️ {failed_count:,} failed access attempts\")\n",
    "        anomalies_found = True\n",
    "    \n",
    "    if not anomalies_found:\n",
    "        log(f\"   ✅ No significant anomalies detected\")\n",
    "    log(\"\")\n",
    "\n",
    "    log(\"⚙️ RECOMMENDATIONS:\\n\")\n",
    "\n",
    "    recommendations = []\n",
    "\n",
    "    if 'exceeds_policy' in locals() and exceeds_policy is not None and len(exceeds_policy) > 0:\n",
    "        recommendations.append(f\"1. Review and archive/delete {len(exceeds_policy):,} tables exceeding retention policy\")\n",
    "\n",
    "    if 'sensitive_tables' in locals() and sensitive_tables is not None and len(sensitive_tables) > 0:\n",
    "        recommendations.append(f\"2. Implement encryption and access controls for {len(sensitive_tables):,} sensitive tables\")\n",
    "\n",
    "    if 'high_freq_count' in locals() and high_freq_count > 0:\n",
    "        recommendations.append(f\"3. Investigate {high_freq_count} users with unusually high access frequency\")\n",
    "\n",
    "    if 'after_hours_count' in locals() and after_hours_count > 0:\n",
    "        recommendations.append(f\"4. Review after-hours access patterns ({after_hours_count:,} events)\")\n",
    "\n",
    "    if 'overall_score' in locals() and overall_score < 80:\n",
    "        recommendations.append(f\"5. Develop remediation plan to improve compliance score to 80%+\")\n",
    "\n",
    "    if len(recommendations) == 0:\n",
    "        recommendations.append(\"✅ No critical recommendations - maintain current compliance posture\")\n",
    "\n",
    "    for rec in recommendations:\n",
    "        log(f\"   {rec}\")\n",
    "\n",
    "    log(\"\\n\" + \"=\"*70)\n",
    "    log(f\"Report generated: {datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    log(\"=\"*70)\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"❌ Error generating executive summary: {str(e)}\")\n",
    "\n",
    "log_execution_time(\"Executive Summary Report\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59e63c36-e490-4ef9-bc6a-ca8bdf34c239",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Export Compliance Report"
    }
   },
   "outputs": [],
   "source": [
    "cell_start_time = time.time()\n",
    "\n",
    "log(\"\\n\" + \"=\"*60)\n",
    "log(\"EXPORTING COMPLIANCE REPORT\")\n",
    "log(\"=\"*60)\n",
    "\n",
    "if ENABLE_DELTA_EXPORT or ENABLE_JSON_EXPORT or ENABLE_EXCEL_EXPORT:\n",
    "    try:\n",
    "        report_timestamp = datetime.now(eastern)\n",
    "\n",
    "        # 1. Prepare sensitive tables inventory\n",
    "        if 'sensitive_tables' in locals() and sensitive_tables is not None and len(sensitive_tables) > 0:\n",
    "            sensitive_export = sensitive_tables.copy()\n",
    "            sensitive_export['report_date'] = report_timestamp\n",
    "            sensitive_export['analysis_period_start'] = start_date\n",
    "            sensitive_export['analysis_period_end'] = end_date\n",
    "            log(f\"  Sensitive tables inventory: {len(sensitive_export):,} records\")\n",
    "        else:\n",
    "            sensitive_export = None\n",
    "\n",
    "        # 2. Prepare retention compliance status\n",
    "        if 'retention_pd' in locals() and retention_pd is not None:\n",
    "            retention_export = retention_pd.copy()\n",
    "            retention_export['report_date'] = report_timestamp\n",
    "            retention_export['analysis_period_start'] = start_date\n",
    "            retention_export['analysis_period_end'] = end_date\n",
    "            log(f\"  Retention compliance data: {len(retention_export):,} records\")\n",
    "        else:\n",
    "            retention_export = None\n",
    "\n",
    "        # 3. Prepare compliance metrics\n",
    "        if 'compliance_metrics' in locals() and compliance_metrics:\n",
    "            metrics_export = pd.DataFrame([\n",
    "                {\n",
    "                    'report_date': report_timestamp,\n",
    "                    'analysis_period_start': start_date,\n",
    "                    'analysis_period_end': end_date,\n",
    "                    'overall_compliance_score': overall_score if 'overall_score' in locals() else 0,\n",
    "                    'retention_compliance_score': compliance_metrics.get('Retention Compliance', 0),\n",
    "                    'sensitive_data_protection_score': compliance_metrics.get('Sensitive Data Protection', 0),\n",
    "                    'audit_trail_coverage_score': compliance_metrics.get('Audit Trail Coverage', 0),\n",
    "                    'access_control_score': compliance_metrics.get('Access Control', 0),\n",
    "                    'total_tables_analyzed': total_tables if 'total_tables' in locals() else 0,\n",
    "                    'sensitive_tables_count': len(sensitive_tables) if 'sensitive_tables' in locals() and sensitive_tables is not None else 0,\n",
    "                    'audit_events_count': len(audit_logs_pd) if 'audit_logs_pd' in locals() and audit_logs_pd is not None else 0,\n",
    "                    'retention_violations_count': len(exceeds_policy) if 'exceeds_policy' in locals() and exceeds_policy is not None else 0\n",
    "                }\n",
    "            ])\n",
    "            log(f\"  Compliance metrics summary: {len(metrics_export):,} record\")\n",
    "        else:\n",
    "            metrics_export = None\n",
    "\n",
    "        # Export to Delta tables\n",
    "        if ENABLE_DELTA_EXPORT:\n",
    "            log(\"\\n\uD83D\uDCBE Exporting to Delta tables...\")\n",
    "            \n",
    "            if sensitive_export is not None:\n",
    "                sensitive_df = spark.createDataFrame(sensitive_export)\n",
    "                table_name = f\"{DELTA_CATALOG}.{DELTA_SCHEMA}.sensitive_tables_inventory\"\n",
    "                sensitive_df.write.mode('append').saveAsTable(table_name)\n",
    "                log(f\"  ✓ Exported to {table_name}\")\n",
    "            \n",
    "            if retention_export is not None:\n",
    "                retention_df = spark.createDataFrame(retention_export)\n",
    "                table_name = f\"{DELTA_CATALOG}.{DELTA_SCHEMA}.retention_compliance\"\n",
    "                retention_df.write.mode('append').saveAsTable(table_name)\n",
    "                log(f\"  ✓ Exported to {table_name}\")\n",
    "            \n",
    "            if metrics_export is not None:\n",
    "                metrics_df = spark.createDataFrame(metrics_export)\n",
    "                table_name = f\"{DELTA_CATALOG}.{DELTA_SCHEMA}.compliance_metrics\"\n",
    "                metrics_df.write.mode('append').saveAsTable(table_name)\n",
    "                log(f\"  ✓ Exported to {table_name}\")\n",
    "\n",
    "        # Export to Excel\n",
    "        if ENABLE_EXCEL_EXPORT:\n",
    "            log(\"\\n\uD83D\uDCCA Exporting to Excel...\")\n",
    "            excel_path = f\"{EXPORT_BASE_PATH}/compliance_report_{datetime.now(eastern).strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "            \n",
    "            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "                if metrics_export is not None:\n",
    "                    metrics_export.to_excel(writer, sheet_name='Compliance Metrics', index=False)\n",
    "                if sensitive_export is not None:\n",
    "                    sensitive_export.to_excel(writer, sheet_name='Sensitive Tables', index=False)\n",
    "                if retention_export is not None:\n",
    "                    retention_export.head(1000).to_excel(writer, sheet_name='Retention Compliance', index=False)\n",
    "            \n",
    "            log(f\"  ✓ Exported to {excel_path}\")\n",
    "\n",
    "        # Export to JSON\n",
    "        if ENABLE_JSON_EXPORT:\n",
    "            log(\"\\n\uD83D\uDCDD Exporting to JSON...\")\n",
    "            json_path = f\"{EXPORT_BASE_PATH}/compliance_report_{datetime.now(eastern).strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            \n",
    "            export_data = {\n",
    "                'report_metadata': {\n",
    "                    'report_date': report_timestamp.isoformat(),\n",
    "                    'analysis_period_start': start_date,\n",
    "                    'analysis_period_end': end_date\n",
    "                },\n",
    "                'compliance_metrics': compliance_metrics if 'compliance_metrics' in locals() else {},\n",
    "                'overall_score': overall_score if 'overall_score' in locals() else 0\n",
    "            }\n",
    "            \n",
    "            import json\n",
    "            with open(json_path, 'w') as f:\n",
    "                json.dump(export_data, f, indent=2, default=str)\n",
    "            \n",
    "            log(f\"  ✓ Exported to {json_path}\")\n",
    "\n",
    "        log(\"\\n✅ Compliance report export completed successfully\")\n",
    "        \n",
    "        # Display metrics summary\n",
    "        if metrics_export is not None:\n",
    "            display(metrics_export)\n",
    "            \n",
    "    except Exception as e:\n",
    "        log(f\"❌ Error exporting compliance report: {str(e)}\")\n",
    "else:\n",
    "    log(\"ℹ️ Export disabled (ENABLE_DELTA_EXPORT, ENABLE_JSON_EXPORT, and ENABLE_EXCEL_EXPORT are all False)\")\n",
    "    log(\"\\n\uD83D\uDCA1 To enable exports, set one or more of these flags to True in the configuration cell\")\n",
    "    \n",
    "log_execution_time(\"Export Compliance Report\", cell_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6155330-e2be-4deb-b542-9857146a6847",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Execution Summary"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXECUTION SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "log(\"\\n\" + \"=\"*70)\n",
    "log(\" \"*20 + \"EXECUTION SUMMARY\")\n",
    "log(\"=\"*70)\n",
    "\n",
    "try:\n",
    "    # Calculate total execution time (if notebook_start_time was set)\n",
    "    if 'cell_start_time' in dir():\n",
    "        total_time = time.time() - cell_start_time\n",
    "        log(f\"\\n⏱️  Total execution time: {total_time:.2f} seconds ({total_time/60:.1f} minutes)\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    log(\"\\n\uD83D\uDCCA Key Metrics:\")\n",
    "    \n",
    "    if 'total_columns' in locals():\n",
    "        log(f\"  • Total columns analyzed: {total_columns:,}\")\n",
    "    \n",
    "    if 'sensitive_data' in locals() and sensitive_data is not None:\n",
    "        log(f\"  • Sensitive columns identified: {len(sensitive_data):,}\")\n",
    "    \n",
    "    if 'sensitive_tables' in locals() and sensitive_tables is not None:\n",
    "        log(f\"  • Sensitive tables identified: {len(sensitive_tables):,}\")\n",
    "    \n",
    "    if 'total_tables' in locals():\n",
    "        log(f\"  • Total tables analyzed: {total_tables:,}\")\n",
    "    \n",
    "    if 'total_events' in locals():\n",
    "        log(f\"  • Audit events analyzed: {total_events:,}\")\n",
    "    \n",
    "    if 'overall_score' in locals():\n",
    "        log(f\"  • Overall compliance score: {overall_score:.1f}%\")\n",
    "    \n",
    "    # Export status\n",
    "    log(\"\\n\uD83D\uDCBE Export Status:\")\n",
    "    log(f\"  • Excel export: {'ENABLED' if ENABLE_EXCEL_EXPORT else 'DISABLED'}\")\n",
    "    log(f\"  • Delta export: {'ENABLED' if ENABLE_DELTA_EXPORT else 'DISABLED'}\")\n",
    "    log(f\"  • JSON export: {'ENABLED' if ENABLE_JSON_EXPORT else 'DISABLED'}\")\n",
    "    \n",
    "    log(\"\\n✅ Compliance & Audit Trail Monitor completed successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    log(f\"\\n⚠️  Error generating execution summary: {str(e)}\")\n",
    "\n",
    "log(\"=\"*70)\n",
    "log(f\"Report generated: {datetime.now(eastern).strftime('%Y-%m-%d %H:%M:%S %Z')}\")\n",
    "log(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "806cc270-076e-4534-ba06-35c855e21a5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Compliance & Audit Trail Monitor",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}